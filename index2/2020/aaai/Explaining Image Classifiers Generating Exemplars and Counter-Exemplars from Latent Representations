We present an approach to explain the decisions of black box image classiﬁers through synthetic exemplar and counter-exemplar learnt in the latent feature space. Our explanation method exploits the latent representations learned through an adversarial autoencoder for generating a synthetic neighborhood of the image for which an explanation is required. A decision tree is trained on a set of images represented in the latent space, and its decision rules are used to generate exemplar images showing how the original image can be mod-iﬁed to stay within its class. Counterfactual rules are used to generate counter-exemplars showing how the original image can “morph” into another class. The explanation also compre-hends a saliency map highlighting the areas that contribute to its classiﬁcation, and areas that push it into another class. A wide and deep experimental evaluation proves that the proposed method outperforms existing explainers in terms of ﬁ-delity, relevance, coherence, and stability, besides providing the most useful and interpretable explanations.