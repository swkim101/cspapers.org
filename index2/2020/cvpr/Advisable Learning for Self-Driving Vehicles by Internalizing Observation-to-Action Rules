Our driving model is based on the work by NVIDIA [1] and Codevilla et al. [2] where they successfully deployed a ConvNet to drive in real-world scenarios. Our model generally outperforms prior work in control prediction. However, to further evaluate the model, we migrate our model from the offline training to a simulated environment, called CARLA [3]. We observe that the use of semantic segmentation as the internal representation of visual scenes is helpful in transferring between real-world and simulated setting. We first train our model on the BDD-X dataset [4] and evaluate in the CARLA simulator (version: 0.9.6). Note, that we use a PID controller to perform lateral and longitudinal control in the simulator from our control commands output. We also use the Robot Operating System (ROS) for the message passing of segmentation, detection, control nodes. We consider the following four typical driving scenarios: (a) Stopping at red traffic lights, (b) Stopping at red traffic lights in a heavy rain, (c) Stopping at a stop road marking, and (d) Stopping for a jaywalker, see Figure 1 (a)–(d). We then provide the driving model with the following advice: “the light is red”, for scenarios (a) and (b) above, “there is a stop sign”, for scenario (c), and “there is a pedestrian crossing”, for scenario (d).