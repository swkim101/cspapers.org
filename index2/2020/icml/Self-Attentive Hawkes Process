Capturing the occurrence dynamics is crucial to predicting which type of events will happen next and when . A common method to do this is through Hawkes processes. To enhance their capacity, recurrent neural networks (RNNs) have been incorporated due to RNNs’ successes in processing sequential data such as languages. Recent evidence suggests that self-attention is more competent than RNNs in dealing with languages. However, we are unaware of the effectiveness of self-attention in the context of Hawkes processes. This study aims to ﬁll the gap by designing a self-attentive Hawkes process (SAHP). SAHP employs self-attention to summarise the inﬂuence of history events and compute the probability of the next event. One deﬁcit of the conventional self-attention, when applied to event sequences, is that its positional encoding only considers the order of a sequence ignoring the time intervals between events. To overcome this deﬁcit, we modify its encoding by translating time intervals into phase shifts of sinusoidal functions. Experiments on goodness-of-ﬁt and prediction tasks show the improved capability of SAHP. Furthermore, SAHP is more interpretable than RNN-based counter-parts because the learnt attention weights reveal contributions of one event type to the happening of another type. To the best of our knowledge, this is the ﬁrst work that studies the effectiveness of self-attention in Hawkes processes.