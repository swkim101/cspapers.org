Encoder Module. We first extract the visual features {vi}i=1 of the given video using a pretrained feature extractor (e.g. 3D-ConvNet [11]). We then apply a Bi-GRU network [3] to learn contextual features {fi}i=1. Next, we define T moment proposals. Each proposal is defined by the boundaries (s,e) and the proposal feature is given by ht = MaxPooling({fi}i=s). For language queries, we first extract the embedding for each word token by a pre-trained Glove embedding [8] and employ another Bi-GRU network to learn word features {sn}n=1.