The objective in statistical Optimal Transport (OT) is to consistently estimate the optimal transport plan/map solely using samples from the given source and target marginal distributions. This work takes the novel approach of posing statistical OT as that of learning the transport plan's kernel mean embedding from sample based estimates of marginal embeddings. A key result is that, under mild conditions, the sample complexity of the resulting estimator for the optimal transport plan as well as that for the Barycentric-projection based optimal transport map are dimension-free. Moreover, the implicit smoothing in the kernel embeddings not only improves the quality of finite sample estimation but also enables out-of-sample estimation. Also, complementary to existing $\phi$-divergence (entropy) based regularization techniques, our estimator employs a maximum mean discrepancy (MMD) based regularization to avoid over-fitting the samples. We present an appropriate representer theorem that leads to a kernelized convex formulation, which can then be potentially used to perform OT even in non-standard domains. Empirical results illustrate the efficacy of the proposed approach.