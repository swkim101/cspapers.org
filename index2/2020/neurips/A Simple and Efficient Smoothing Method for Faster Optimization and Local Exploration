This work proposes a novel smoothing method, called Bend, Mix and Release (BMR) , that extends two well-known smooth approximations of the convex optimization literature: randomized smoothing and the Moreau envelope. The BMR smoothing method allows to trade-off between the computational simplicity of randomized smoothing (RS) and the approximation efﬁciency of the Moreau envelope (ME). More speciﬁcally, we show that BMR achieves up to a √ d multiplicative improvement compared to the approximation error of RS, where d is the dimension of the search space, while being less computation intensive than the ME. For non-convex objectives, BMR also has the desirable property to widen local minima, allowing optimization methods to reach small cracks and crevices of extremely irregular and non-convex functions, while being well-suited to a distributed setting. This novel smoothing method is then used to improve ﬁrst-order non-smooth optimization (both convex and non-convex) by allowing for a local exploration of the search space. More speciﬁcally, our analysis sheds light on the similarities be-tween evolution strategies and BMR, creating a link between exploration strategies of zeroth-order methods and the regularity of ﬁrst-order optimization problems. Finally, we evidence the impact of BMR through synthetic experiments.