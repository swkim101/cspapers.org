Layer input channels ks stride conv1 image C′ 3/64 3 1 conv2 conv1 64/128 3 1 conv3 conv2 128/256 3 1 conv4 conv3 256/128 3 1 conv5 conv4 128/64 3 1 conv6 conv5 64/3 3 1 sigmoid conv6 N/A N/A N/A We adopt a simplified U-Net adopted in Cycle-GAN [6]. Specifically, we remove the two most inner convolutions and upconvolutions. The detailed H architectures for the DDH and UDH are shown in Table 1 and Table 2, respectively, where the conv layer is followed by a BatchNorm layer and ReLU layer. In contrast to the final Sigmoid layer adopted in DDH, we adopt a Tanh layer multiplied by a scale factor which is set to 10/255 by referencing the engineering choice in universal adversarial perturbations [3, 4, 1]. Note that different from [3, 4, 1], Se is minimized in the loss even with this constraint. With such a scale factor, some pixel intensities in C ′ in UDH might still be out of the range [0, 1]. However, empirically we find that the