Extracting valuable information from sensor data concerning group sizes and human-centered interactions will play a crucial role in balancing building functions according to user needs and preferences. Based on a newly constructed university building in Cincinnati, Ohio, USA, this paper combines data-driven methods and contextual knowledge about spatial design features using 3D stereo vision cameras to implement the detection algorithms on three different real-world case settings. By using human-proximity distances and the dynamic change in the position of an occupant over time (x- and y- coordinates) in an indoor crowded environment, this research successfully applies annotated data to detect and classify group sizes and human interaction using Machine Learning (ML) techniques. Ground-truth labels were collected manually to validate the performances, which was tedious and manually expensive. We have applied popular ML algorithms for detecting the group sizes and classifying the human interactions, with average F1-scores ranging from 79 to 87% for single, paired and group occupancy. A significant challenge is the dependency on prior knowledge about physical conditions in order to detect occupant seating and orientation in crowded environments. Here, additional features such as directions and space layouts would help mitigating this issue. The results however can contribute to new dimensions of research associated with the automatic generation of metadata from data-driven systems to make informed decisions about space design, space-use relationships and management.