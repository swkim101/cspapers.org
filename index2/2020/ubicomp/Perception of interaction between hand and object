Action knowledge graphs can play a central role in smart cities, smart homes, robot planning, and so on. This is since both of the subject and the object of the actions can add more meaningful information for the higher-level application than the action alone as a predicate. We built a system that generates the action knowledge graphs from video using deep learning. Especially, we propose an algorithm which perceives the interaction between hand and object by measuring the proximity between them with considering the direction of fingers. We showed that this approach achieves the performance of 83% in accuracy using the Stair Lab data.