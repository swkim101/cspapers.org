In this paper we report on our experiences during a first-year course on digital logic design using the Verilog hardware description language. As part of the course the students were given a series of take-home assignments, which were then marked using an automated assessment system developed by the authors. During the course the instructor was made aware that a set of solutions had been circulated to the students, and was asked to assess the impact that this had on the assessment regime. In order to answer this question, we examined and implemented a number of approaches to calculating similarity between Verilog programs, and we present the results of that study in this paper. An important feature of this work was ensuring that the measurements used were well-understood, properly calibrated and defensible. We report on the results of this study, applied to a class of 115 students who completed up to 11 projects each.