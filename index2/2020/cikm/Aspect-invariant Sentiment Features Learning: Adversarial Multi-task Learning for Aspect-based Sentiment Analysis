Neural models with attention mechanisms have achieved remarkable performance in Aspect-based Sentiment Analysis (ABSA). In most previous studies, the information about aspects in sentences is considered important for the ABSA task and therefore various attention mechanisms have been explored to leverage interactions between aspects and context. However, some sentiment expressions carry the same polarity regardless of the aspects they are associated with. In such cases, it is not necessary to incorporate aspect information for ABSA. In fact, in our experiments, we find that blindly leveraging interactions between aspects and context as features may introduce noise when analyzing those aspect-invariant sentiment expressions, especially when facing with limited aspect-related annotated data. Hence, in this paper, we propose an Adversarial Multi-task Learning Framework to identify the aspect-invariant/dependent sentiment expressions automatically without requiring extra annotations. In addition, we use a gating mechanism to control the contribution of representations derived from aspect-invariant and aspect-dependent hidden states when generating the final contextual sentiment representations for the given aspect. This essentially allows the exploitation of aspect-invariant sentiment features for better ABSA results. Experimental results on two benchmark datasets show that extending existing neural models using our proposed framework achieves superior performance. In addition, the aspect-invariant data extracted by our framework can be considered as pivot features for better transfer learning of the ABSA models on unseen aspects.