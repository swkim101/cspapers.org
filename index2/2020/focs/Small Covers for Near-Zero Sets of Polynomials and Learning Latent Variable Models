Let <tex>$V$</tex> be any vector space of multivariate degree-<tex>$d$</tex> homogeneous polynomials with co-dimension at most <tex>$k$</tex>, and <tex>$S$</tex> be the set of points where all polynomials in <tex>$V$</tex> nearly vanish. We establish a qualitatively optimal upper bound on the size of <tex>$\epsilon$</tex>-covers for <tex>$S$</tex>, in the <tex>$\ell_{2}$</tex>-norm. Roughly speaking, we show that there exists an <tex>$\epsilon$</tex>-cover for <tex>$S$</tex> of cardinality <tex>$M=(k/\epsilon)^{O_{d}(k^{1/d})}$</tex>. Our result is constructive yielding an algorithm to compute such an <tex>$\epsilon$</tex>-cover that runs in time <tex>$\text{poly}(M)$</tex>. Building on our structural result, we obtain significantly improved learning algorithms for several fundamental high-dimensional probabilistic models with hidden variables. These include density and parameter estimation for <tex>$k$</tex>-mixtures of spherical Gaussians (with known common covariance), PAC learning one-hidden-layer ReLU networks with <tex>$k$</tex> hidden units (under the Gaussian distribution), density and parameter estimation for <tex>$k$</tex>-mixtures of linear regressions (with Gaussian covariates), and parameter estimation for <tex>$k$</tex>-mixtures of hyperplanes. Our algorithms run in time quasi-polynomial in the parameter <tex>$k$</tex>. Previous algorithms for these problems had running times exponential in <tex>$k^{\Omega(1)}$</tex>. At a high-level our algorithms for all these learning problems work as follows: By computing the low-degree moments of the hidden parameters, we are able to find a vector space of polynomials that nearly vanish on the unknown parameters. Our structural result allows us to compute a quasi-polynomial sized cover for the set of hidden parameters, which we exploit in our learning algorithms.