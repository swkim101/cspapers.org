We pre-train the gated layout rectifier (GLRec) in a selfsupervised manner as illustrated in the paper. The model is pre-trained using SGD with initial learning rate 5 ˆ 10 ́2 to train the models for the first 60 epochs. Then we linearly decay the learning rate to 5 ˆ 10 ́5 in the next 100 epochs. The batch size is set to 128. For the backbone and the corresponding region locator (CRLoc) training, we use the Adam optimizer for model optimization. We first train the backbone network for 120 epochs with initial learning rate as 5 ˆ 10 ́4. The learning rate is decayed by a factor of 0.1 for every 40 epochs. The batch size is 64, constructed from 16 identities. After that, the GLRec, the backbone network and the CRLoc are jointly trained with 40 epochs in the assistance of Adam optimizer. The initial learning rate is also set to 5 ˆ 10 ́4. Our models are implemented on PyTorch.