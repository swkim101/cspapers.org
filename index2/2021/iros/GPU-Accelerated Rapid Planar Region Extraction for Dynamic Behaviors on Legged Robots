Legged robots require fast and accurate representation of their surrounding terrain to achieve behaviors such as running, push recovery, continuous walking, backflips, while also utilizing on-board computational resources efficiently. The desired tasks can be achieved efficiently by representing the environment using planar regions. However, existing methods for planar region extraction are either too slow or require significant compute time on the Central Processing Unit (CPU). In this work we exploit key properties of depth images and Graphical Processing Unit (GPU) to estimate planar regions around the robot at very high frame rates of 150-200 Hz. The proposed algorithm uses a set of fully customizable and interchangeable set of kernel layers on the GPU to process the depth map in parallel and generate a locally connected graph structure, which is later separated into planar components using a basic depth-first search. We test the proposed algorithm on the Atlas robot while performing different walking behaviors on oriented cinder blocks, as well as in simulation with simulated sensor and robot. The algorithm is open-sourced for research on legged robots and other fields.