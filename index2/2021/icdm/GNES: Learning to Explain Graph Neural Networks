In recent years, graph neural networks (GNNs) and the research on their explainability are experiencing rapid developments and achieving significant progress. Many methods are proposed to explain the predictions of GNNs, focusing on “how to generate explanations” However, research questions like “whether the GNN explanations are inaccurate”, “what if the explanations are inaccurate”, and “how to adjust the model to generate more accurate explanations” have not been well explored. To address the above questions, this paper proposes a GNN Explanation Supervision (GNES) 1 framework to adaptively learn how to explain GNNs more correctly. Specifically, our framework jointly optimizes both model prediction and model explanation by enforcing both whole graph regularization and weak supervision on model explanations. For the graph regularization, we propose a unified explanation formulation for both node-level and edge-level explanations by enforcing the consistency between them. The node- and edge-level explanation techniques we propose are also generic and rigorously demonstrated to cover several existing major explainers as special cases. Extensive experiments on five real-world datasets across two application domains demonstrate the effectiveness of the proposed model on improving the reasonability of the explanation while still keep or even improve the backbone GNNs model performance.1Code available at: https://github.com/YuyangGao/GNES.