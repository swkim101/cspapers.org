Dialogue Relation Extraction (DRE) is a new kind of relation extraction task from multi-turn dialogues. Different from the previous tasks, speaker specific relations are implicitly mixed together in both a local utterance window and a speaker context. To tackle both local and speaker dependency challenges, we explicitly construct a unified mention co-occurrence graph within a local utterance window or all utterances of a speaker from different entities. For each dialogue, a position enhanced graph attention network over this graph is proposed to obtain position aware mention representations in terms of both contexts. A gate function is utilized to help obtain a discriminative representation enough for each relation from original and position aware mention representations. For each entity pair in this dialogue, a pairwise attention mechanism is deployed to aggregate those discriminative mention representations as pair representation, which is fed into a standard multi-label classifier for relation label prediction. Experimental results on two benchmarks show the performance improvement of the proposed method is at least 1.6% and 3.2% compared with SOTA.