We study the problem of designing online algorithms given advice about the input. While prior work had focused on deterministic advice, we only assume distributional access to the instances of interest, and the goal is to learn a competitive algorithm given access to i.i.d. samples. We aim to be competitive against an adversary with prior knowledge of the distribution, while also performing well against worst-case inputs. We focus on the classical online problems of ski-rental and prophet-inequalities, and provide sample complexity bounds for the underlying learning tasks. First, we point out that for general distributions it is information-theoretically impossible to beat the worst-case competitive-ratio with any ﬁnite sample size. As our main contribution, we establish strong positive results for well-behaved distributions. Speciﬁcally, for the broad class of log-concave distributions, we show that poly(1 /(cid:15) ) samples sufﬁce to obtain (1 + (cid:15) ) - competitive ratio. Finally, we show that this sample upper bound is close to best possible, even for very simple classes of distributions.