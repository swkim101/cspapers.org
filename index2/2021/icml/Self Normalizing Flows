Efﬁcient gradient computation of the Jacobian determinant term is a core problem in many machine learning settings, and especially so in the normalizing ﬂow framework. Most proposed ﬂow models therefore either restrict to a function class with easy evaluation of the Jacobian determinant, or an efﬁcient estimator thereof. However, these restrictions limit the performance of such density models, frequently requiring signiﬁcant depth to reach desired performance levels. In this work, we propose Self Normalizing Flows , a ﬂexible framework for training normalizing ﬂows by re-placing expensive terms in the gradient by learned approximate inverses at each layer. This reduces the computational complexity of each layer’s exact update from O ( D 3 ) to O ( D 2 ) , allowing for the training of ﬂow architectures which were otherwise computationally infeasible, while also pro-viding efﬁcient sampling. We show experimen-tally that such models are remarkably stable and optimize to similar data likelihood values as their exact gradient counterparts, while training more quickly and surpassing the performance of func-tionally constrained counterparts.