We investigate the distributions of Conjugate Kernel (CK) and Neural Tangent Kernel (NTK) for ReLU networks with random initialization. We derive the precise distributions and moments of the diagonal elements of these kernels. For a feed-forward network, these values converge in law to a log-normal distribution when the network depth d and width n simultaneously tend to inﬁnity and the variance of log diagonal elements is proportional to d/n . For the residual network, in the limit that number of branches m increases to in-ﬁnity and the width n remains ﬁxed, the diagonal elements of Conjugate Kernel converge in law to a log-normal distribution where the variance of log value is proportional to 1 /n , and the diagonal elements of NTK converge in law to a log-normal distributed variable times the conjugate kernel of one feedforward network. Our new theoretical analysis results suggest that residual network remains trainable in the limit of inﬁnite branches and ﬁxed network width. The numerical experiments are conducted and all results validate the soundness of our theoretical analysis.