Human beings acquire the ability of image classi-ﬁcation through visual concept learning, in which the process of concept formation involves inter-twined searches of common properties and concept descriptions. However, in most image classi-ﬁcation algorithms using deep convolutional neural network (ConvNet), the representation space is constructed under the premise that concept descriptions are ﬁxed as one-hot codes, which limits the mining of properties and the ability of identifying unseen samples. Inspired by this, we propose a learning strategy of visual concept formation (LSOVCF) based on the ConvNet, in which the two intertwined parts of concept formation, i.e. feature extraction and concept description, are learned together. First, LSOVCF takes sample response in the last layer of Con-vNet to induct concept description being assumed as Gaussian distribution, which is part of the training process. Second, the exploration and experience loss is designed for optimization, which adopts experience cache pool to speed up convergence. Experiments show that LSOVCF improves the ability of identifying unseen samples on ci-far10, STL10, ﬂower17 and ImageNet based on several backbones, from the classic VGG to the SOTA Ghostnet. The code is available at https: //github.com/elvintanhust/LSOVCF .