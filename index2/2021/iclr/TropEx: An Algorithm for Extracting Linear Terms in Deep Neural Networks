Deep neural networks with rectiﬁed linear (ReLU) activations are piecewise linear functions, where hyperplanes partition the input space into an astronomically high number of linear regions. Previous work focused on counting linear regions to measure the network’s expressive power and on analyzing geometric properties of the hyperplane conﬁgurations. In contrast, we aim to understand the impact of the linear terms on network performance, by examining the information encoded in their coefﬁcients. To this end, we derive TropEx, a non-trivial tropical algebra-inspired algorithm to systematically extract linear terms based on data. Applied to convolutional and fully-connected networks, our algorithm uncovers signiﬁcant differences in how the different networks utilize linear regions for generalization. This underlines the importance of systematic linear term exploration, to better understand generalization in neural networks trained with complex data sets.