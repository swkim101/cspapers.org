Capturing accurate uncertainty quantiﬁcation of the predictions from deep neural networks is important in many real-world decision-making applications. A reliable predictor is expected to be accurate when it is conﬁdent about its predictions and indicate high uncertainty when it is likely to be inaccurate. However, modern neural networks have been found to be poorly calibrated, primarily in the direction of overconﬁdence. In recent years, there is a surge of research on model calibration by leveraging implicit or explicit regularization techniques during training, which achieve well calibration performance by avoiding overconﬁdent outputs. In our study, we empirically found that despite the predictions obtained from these regularized models are better calibrated , they suffer from not being as calibratable , namely, it is harder to further calibrate these predictions with post-hoc calibration methods like temperature scaling and histogram binning. We conduct a series of empirical studies showing that overconﬁdence may not hurt ﬁnal calibration performance if post-hoc calibration is allowed, rather, the penalty of conﬁdent outputs will compress the room of potential improvement in post-hoc calibration phase. Our experimental ﬁndings point out a new direction to improve calibration of DNNs by considering main training and post-hoc calibration as a uniﬁed framework.