Recent interest in learning large variational Bayesian Neural Networks (BNNs) has been partly hampered by poor predictive performance caused by underﬁtting, and their performance is known to be very sensitive to the prior over weights. Current practice often ﬁxes the prior parameters to standard values or tunes them using heuristics or cross-validation. In this paper, we treat prior parameters in a distributional way by extending the model and collapsing the variational bound with respect to their posteriors. This leads to novel and tighter Evidence Lower Bounds (ELBOs) for performing variational inference (VI) in BNNs. Our experiments show that the new bounds signiﬁcantly improve the performance of Gaussian mean-ﬁeld VI applied to BNNs on a variety of data sets, demonstrating that mean-ﬁeld VI works well even in deep models. We also ﬁnd that the tighter ELBOs can be good optimization targets for learning the hyperparameters of hierarchical priors.