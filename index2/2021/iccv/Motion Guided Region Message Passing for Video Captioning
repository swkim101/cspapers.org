As shown in Table 1, we supplement more results to the Table 5 in the main paper: #4 and #5: We apply ATGD to more model variants. Comparing #4 and #1, #5 and #0, we can conclude that ATGD is a generally effective caption decoder, and can be applied on top of different video feature encoders to obtain better captioning performance than two-layer LSTMs (which is used in most methods). #obj: We use detected object features in the baseline model, but the features are spatially mean-pooled as in #1. As can be seen, the performances of #obj is worse than #0 (grid) and #1 (region). We conjecture the reason is that the object features can ignore some background regions because of extracting information only from object bounding box areas, while grid features (#0) contains all the information of a feature map and region features (#1) can capture the background information by the soft attention maps. This information loss can be compensated by the MGCMP, that is why the performances of our model with object and region features are close in the Table 4 of the main paper.