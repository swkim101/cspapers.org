Recently, there has been significant interest in Temporal Graph Neural Networks (TGNN) because of their capability to learn from time-evolving graph-related data. However, similar to Graph Neural Network (GNN), explaining the predictions of TGNN is non-trivial due to its black-box and complex nature. A major approach for this problem in GNNs is by analyzing the model’s responses to some perturbations of the model’s inputs, called perturbation-based explanation methods. These methods are convenient and flexible as they do not require access to the model’s internals. However, a question arises: Does the lack of internal access limit these methods from uncovering crucial information about the predictions? Motivated by the question, this study explores the limitations of some popular classes of perturbation-based explanation methods. By constructing specific instances of TGNNs, we show (i) Node-perturbation is not reliable for identifying the paths that carry out the prediction, (ii) Edgeperturbation cannot reliably determine all the nodes contributing to the prediction and (iii) perturbing both nodes and edges does not consistently help identify the graph components responsible for the temporal aggregation in TGNNs. Our experimental results further demonstrate situations for failures of explanations can occur frequently in both synthetic and real-world scenarios. Thus, they emphasize the importance of perturbation choices and the internal information of the explained model in determining faithful explanations of the model’s predictions.