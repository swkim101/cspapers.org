AI alignment refers to models acting towards human-intended goals, preferences, or ethical principles. In this paper, we focus on the models’ visual perception alignment with humans, further referred to as AI-human visual alignment . Specifically, we propose a new dataset for measuring AI-human visual alignment in terms of image classiﬁcation. In order to evaluate AI-human visual alignment , a dataset should encompass samples with various scenarios and have gold human perception labels. Our dataset consists of three groups of samples, namely Must-Act ( i.e. , Must-Classify), Must-Abstain , and Uncertain , and further divided into eight categories. All samples have a gold human perception label; even Un-certain ( e . g ., severely blurry) sample labels were obtained via crowd-sourcing. The validity of our dataset is veriﬁed by sampling theory, statistical theories related to survey design, and experts in the related ﬁelds. Using our dataset, we analyze the visual alignment and reliability of ﬁve popular visual perception models and eight abstention methods. Our code and data is available at https://github.com/jiyounglee-0523/VisAlign .