The Suessex-Huawei Locomotion-Transportation (SHL) Recognition Challenge presents a large real-world dataset derived from multimodal smartphone sensors, with the aim of accurately distinguishing between eight different states of locomotion and transportation. Based on the end-to-end principle, our team (Yummy MacMuffin) proposed AttenDenseNet, which integrates channel and spatial attention as well as convolutional layers for feature extraction and implements DenseNet as our classification model. In our experiments, we viewed the data on different axes as multi-channel input, splitting them into 5-second windows and then performing the classification task with each window as a unit. Data leakage was avoided by dividing the training data by timestamps and the generalization ability was enhanced by increasing the sample size. In the end, we achieved an average F1 score of 0.5442 on the validation set.