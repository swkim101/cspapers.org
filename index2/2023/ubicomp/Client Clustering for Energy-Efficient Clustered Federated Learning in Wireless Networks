Clustered Federated Learning (FL) is a novel approach for handling data heterogeneity in FL and training personalized ML models for clients. However, existing research overlooks network constraints and objectives when implementing clustered FL in a wireless network. Since clients experience varying energy costs when connected to different servers, the cluster formation greatly impacts system energy efficiency. To address this, we present an energy-efficient client clustering problem that optimizes FL performance while minimizing energy costs. We introduce a new metric, the effective number of clients, to predict cluster FL performance based on size and data heterogeneity, guiding the clustering optimization. To find optimal client clusters, we propose an energy-efficient clustering algorithm using parallel Gibbs Sampling. Simulations and data experiments demonstrate that our algorithm achieves tunable trade-offs between data similarity (thus FL performance) and energy cost.