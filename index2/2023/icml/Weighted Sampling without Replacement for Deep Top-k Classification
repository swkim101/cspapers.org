The top-k classification accuracy is a crucial metric in machine learning and is often used to evaluate the performance of deep neural networks. These networks are typically trained using the cross-entropy loss, which optimizes for top-1 classification and is considered optimal in the case of infinite data. However, in real-world scenarios, data is often noisy and limited, leading to the need for more robust losses. In this paper, we propose using the Weighted Sampling Without Replacement (WSWR) method as a learning objective for top-k loss. While traditional methods for evaluating WSWR-based top-k loss are computationally impractical, we show a novel connection between WSWR and Reinforcement Learning (RL) and apply well-established RL algorithms to estimate gradients. We compared our method with recently proposed top-k losses in various regimes of noise and data size for the prevalent use case of k = 5 . Our experimental results reveal that our method consistently outperforms all other methods on the top-k metric for noisy datasets, has more robustness on extreme testing scenarios, and achieves competitive results on training with limited data.