Programming courses and instructors provide a wide variety of materials to help students learn how a computer will execute code. However, students may not engage with these resources effectively, or at all. This paper presents an initial framework for designing formative assessments, coined Code Diagram Queries (CDQs). The goal of CDQs is to act as a forcing function to engage students with formative diagrammatic material that will challenge and correct their current mental models for the notional machine of the language they are learning. Optional resources are often disregarded unless there are formative activities that drive students to interact with the materials. CDQs are questions that include or reference diagrams and are designed to drive student interaction with diagrammatic notional machine representations in a low-risk formative environment. This developmental environment allows students to explore new computer science concepts and expand on their understanding of them. I hypothesize that CDQs will help increase student engagement by leveraging diagrammatic materials and through this, help students accurately visualize and comprehend code execution. I will measure engagement with class materials and student confidence levels following their interactions with CDQs. This framework is meant to aid in the effective design of CDQs so that a student's comprehension of code execution may be understood more deeply and then corrected as needed. The focus of this research is to provide new insights into the importance of guiding students through the construction of their mental models.