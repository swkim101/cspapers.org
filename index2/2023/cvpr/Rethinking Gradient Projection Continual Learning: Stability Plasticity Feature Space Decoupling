Continual learning aims to incrementally learn novel classes over time, while not forgetting the learned knowledge. Recent studies have found that learning would not forget if the updated gradient is orthogonal to the feature space. However, previous approaches require the gradient to be fully orthogonal to the whole feature space, leading to poor plasticity, as the feasible gradient direction becomes narrow when the tasks continually come, i.e., feature space is unlimitedly expanded. In this paper, we propose a space decoupling (SD) algorithm to decouple the feature space into a pair of complementary subspaces, i.e., the stability space $\mathcal{I}$ and the plasticity space $\mathcal{R}. \mathcal{I}$ is established by conducting space intersection between the historic and current feature space, and thus $\mathcal{I}$ contains more task-shared bases. $\mathcal{R}$ is constructed by seeking the orthogonal complementary subspace of $T$ and thus $\mathcal{R}$ mainly contains task-specific bases. By putting distinguishing constraints on $\mathcal{R}$ and $\mathcal{I}$, our method achieves a better balance between stability and plasticity. Extensive experiments are conducted by applying SD to gradient projection baselines, and show SD is model-agnostic and achieves SOTA results on publicly available datasets.