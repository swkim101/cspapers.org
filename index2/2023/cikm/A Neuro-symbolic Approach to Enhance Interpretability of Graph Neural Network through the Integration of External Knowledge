Graph Neural Networks (GNNs) have shown remarkable performance in tackling complex tasks. However, interpreting the decision-making process of GNNs remains a challenge. To address the challenge, we explore representing the behaviour of a GNN in a representation space that is more transparent such as a knowledge graph, in a way that captures the behaviour of a GNN as a graph. Our initial experiments on the node classification task can represent the trained graph convolutional neural network (GCN) behaviour with some semantics uncovered by state-of-the-art approaches. This research offers a promising direction for enhancing GNN interpretability and understanding by providing structured, human-understandable representations and incorporating external knowledge for more accurate predictions.