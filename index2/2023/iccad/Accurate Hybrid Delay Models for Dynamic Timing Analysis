To facilitate the analysis of timing relations between individual transitions in a signal trace, dynamic digital timing analysis offers a less accurate but much faster alternative to analog simulations of digital circuits. It primarily requires gate delay models, which account for the fact that the input-to-output delay of a particular input transition also depends on the temporal distance to the previous output transitions. In the case of multi-input gates, this delay also experiences variations caused by multi-input switching (MIS) effects, i.e., transitions at different inputs that occur in close temporal proximity. In this paper, we advocate the development of hybrid delay models for CMOS gates obtained by replacing transistors with time-variant resistors. We exemplify our approach by applying it to a NOR gate (and, hence, to the dual NAND gate) and a Muller C gate. We analytically solve the resulting first-order differential equations with non-constant coefficients and derive analytic expressions for the resulting MIS gate delays. The resulting formulas not only pave the way to a sound model parametrization procedure but are also instrumental in implementing a fast and efficient digital timing simulation. By comparison with analog simulation data, we show that our models faithfully represent all relevant MIS effects. Using an implementation in the Involution Tool, we also demonstrate that our models surpass all alternative digital delay models known to us in terms of accuracy, with comparably short running times.