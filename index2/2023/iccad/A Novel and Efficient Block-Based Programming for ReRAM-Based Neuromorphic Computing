ReRAM-based accelerators have emerged as promising accelerators for deep neural networks (DNNs). How-ever, programming every ReRAM cell to its corresponding conductance before inference can be time-consuming and energy-intensive using existing one-by-one/row-by-row programming mechanisms. Although a two-phase multi-row programming scheme has been proposed to enhance programming efficiency, there are situations where multiple rows cannot be programmed together and only row-by-row programming can be employed. Therefore, this paper proposes a new block-based programming architecture for ReRAM crossbars that enables precise control of wordline and bitline transistors. In addition, a block-based programming framework, including the approximation phase and the fine-tuning phase, along with a multi-line programming algorithm and a programming-aware model retraining are proposed to reduce programming cycles and energy consumption. Experimental results demonstrate that our proposed method can reduce programming cycles and energy consumption by 46%-49 % and 63 % -64 %, respectively, compared to the state of the art. Additionally, the area and power overhead are negligible.