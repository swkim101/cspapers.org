Recent advancement in emerging brain-inspired computing has pointed out a promising path to Machine Learning (ML) algorithms with high efficiency. Particularly, research in the field of HyperDimensional Computing (HDC) brings orders of magnitude speedup to both ML model training and inference compared to their deep learning counterparts. However, current HDC-based ML algorithms generally lack uncertainty estimation, despite having shown good results in various practical applications and outstanding energy efficiency. On the other hand, existing solutions such as the Bayesian Neural Networks (BNN) are generally much slower than regular neural networks and lead to high energy consumption. In this paper, we propose a hyperdimensional Bayesian framework called DiceHD, which enables uncertainty estimation for the HDC-based regression algorithm. The core of our framework is a specially designed HDC encoder that maps input features to the high dimensional space with an extra layer of randomness, i.e., a small number of dimensions are randomly dropped for each input. Our key insight is that by using this encoder, DiceHD implements Bayesian inference while maintaining the efficiency advantage of HDC. We verify our framework with both toy regression tasks and real-world datasets. We compare our DiceHD to several widely-used BNN baselines in terms of performance and efficiency. The results on CPU show that DiceHD provides comparable uncertainty estimations while achieving significant speedup compared to the BNN baseline. We also deploy DiceHD on two FPGA platforms with different acceleration capabilities, showing that DiceHD provides up to 84× (3740×) better energy efficiency for training (inference).