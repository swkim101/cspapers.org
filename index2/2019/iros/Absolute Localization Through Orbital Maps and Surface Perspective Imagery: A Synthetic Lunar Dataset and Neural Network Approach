We present a neural network approach and publicly available dataset for developing and benchmarking algorithms for localization on the Moon. Accurate localization is essential for navigation, path planning, and science objectives. On Earth, localization can be achieved using a satellite navigation system; such a system is, however, unavailable for other planetary bodies. Current absolute and relative localization methods for rovers on planetary surfaces are laborious manual tasks or accumulate significant location errors over time resulting in incorrect location estimates, respectively. Recently, Earth-based approaches have explored localization by matching surface-perspective imagery to satellite imagery using computer vision and machine learning. Those focusing on natural environments have shown such image matching to be feasible for localization, though it remains challenging. The purpose of this project was to produce a working proof-of-concept model to match surface-perspective imagery to satellite images for planetary roving applications. This would improve absolute localization accuracy (relative to current methods) by utilizing machine learning algorithms to offer greater automation and faster calculations, and produce a benchmark dataset for future studies. To achieve these, the following objectives were met: (1) configuration and modification of an existing synthetic planetary environment using Unreal Engine 4 to generate surface and satellite perspective images, (2) reprojection of 2.4 + million surface perspective images to 600,000 + satellite perspective images, and (3) matching of said reprojected images to satellite images via a Siamese convolutional neural network, designed and trained for this task.