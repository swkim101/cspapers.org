This project aims to implement, utilize, and improve upon the BERT model to perform sentiment analysis and other downstream tasks. In the first part of our project, we fully implement the original BERT model and test it on sentiment analysis â€“ in the second part, we fine-tune and extend the model for optimal performance on paraphrase detection and semantic textual similarity. To do so, we implement Projected Attention Layers (PALs) Stickland and Murray (2019), adapters Houlsby et al. (2019), and prefix tuning Li and Liang (2021) to achieve optimal performance over multi-tasks while being efficient. We also experiment with changes to the BERT model architecture by implementing Sentence-BERT Reimers and Gurevych (2019) and modifying the downstream classifier head architecture. We experiment with different model architectures, adaptation modules, samplers, adding additional training data, and hyper-parameter configurations. We find Sentence-BERT learns more semantically meaningful sentence embeddings and has better performance on the paraphrase and similarity tasks. PAL, prefix, and adapter improve average performance by about 8% when pretraining, and have comparable performance to full fine-tuning with less than 10% of trainable parameters.