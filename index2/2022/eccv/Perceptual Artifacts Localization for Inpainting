In this work, all of our images are uniformly sampled from Places2 dataset [17]. We generate two types of masks for our experiments, which are masks on the background region and masks covering a complete object. We discuss the details of how to generate these masks in below. Masks on the Background. Since current inpainting models still can not understand the object-level prior and thus can not properly fill the object region, we do not want to sample masks that cover partial objects, which none of the current methods can properly deal with. To this end, we first use Mask R-CNN [3] to find all object masks, and then avoid sampled holes to partially overlap with these object regions. We use both free-form masks [12] and instance masks in our experiments, where the hole size ratio over the entire image ranges from 0.08 to 0.3. The instance masks are collected from multiple segmentation datasets, such as COCO [6] and Pascal VOC [2]. Some examples of these masks are shown in Fig. 1.