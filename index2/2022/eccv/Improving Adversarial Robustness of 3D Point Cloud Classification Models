1 Mutual Information Maximization Theorem 1 Let f be a function that maps a point cloud to the feature space, and Q be the distribution of clean point clouds. S is sampled from Q. Q(S, ε) is the distribution of noisy point clouds, in which each element Sk is perturbed from S with an additional noise ε, and the difference of numbers of points between S and Sk is smaller than a constant k, i.e., −k ≤ |Sk| − |S| ≤ k. Then for every S ∼ Q and Sk ∼ Q(S, ε), the mutual information I(Sk, f(S)) has a lower bound, which is negatively correlated with the k-measurement Mk(f, S, Sk). Proof. According to the definition of mutual information, we have the following equation: I(Sk, f(S)) = H(f(S))−H(f(S)|Sk) =