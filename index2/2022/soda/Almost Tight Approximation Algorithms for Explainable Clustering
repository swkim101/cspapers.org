Recently, due to an increasing interest for transparency in artificial intelligence, several methods of explainable machine learning have been developed with the simultaneous goal of accuracy and interpretability by humans. In this paper, we study a recent framework of explainable clustering first suggested by Dasgupta et al. [8]. Specifically, we focus on the k-means and k-medians problems and provide nearly tight upper and lower bounds. First, we provide an O(log k log log k)-approximation algorithm for explainable k-medians, improving on the best known algorithm of O(k) [8] and nearly matching the known Ω(log k) lower bound [8]. In addition, in low-dimensional spaces d ≪ log k, we show that our algorithm also provides an O(d log d)-approximate solution for explainable k-medians. This improves over the best known bound of O(d log k) for low dimensions [14], and is a constant for constant dimensional spaces. To complement this, we show a nearly matching Ω(d) lower bound. Next, we study the k-means problem in this context and provide an O(k log k)-approximation algorithm for explainable k-means, improving over the O(k) bound of Dasgupta et al. and the O(dk log k) bound of [14]. To complement this we provide an almost tight Ω(k) lower bound, improving over the Ω(log k) lower bound of Dasgupta et al. All our algorithms run in near linear time in the number of points and the dimension.