Training visuomotor robot controllers from scratch on a new robot typically requires generating large amounts of robot-speciﬁc data. Could we leverage data previously collected on another robot to reduce or even completely remove this need for robot-speciﬁc data? We propose a “robot-aware” solution paradigm that exploits readily available robot “self-knowledge” such as proprioception, kinematics, and camera calibration to achieve this. First, we learn modular dynamics models that pair a transferable, robot-agnostic world dynamics module with a robot-speciﬁc, analytical robot dynamics module. Next, we set up visual planning costs that draw a distinction between the robot self and the world. Our experiments on tabletop manipulation tasks in simulation and on real robots demonstrate that these plug-in improvements dramatically boost the transferability of visuomotor controllers, even permitting zero-shot transfer onto new robots for the very ﬁrst time. Project website: https://hueds.github.io/rac/