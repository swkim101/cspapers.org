Our collected data can provide sufficient synchronized camera views for high quality 4D reconstruction of challenging dynamic objects and view-dependent effects in a natural daily indoor environment, which did not exist in public 4D datasets. Our captured data demonstrates a variety of challenges for video synthesis, including objects of high specularity, translucency and transparency. It also contains scene changes and motions with changing topology (poured liquid), self-cast moving shadows, volumetric effects (fire flame), and an entangled moving object with strong view-dependent effects (the torch gun and the pan), various lighting conditions (daytime, night, spotlight from the side), multiple people moving around in open living room space with outdoor scenes seen through transparent windows with relatively dark indoor illumination. We visualize one snapshot of the sequence in Fig. 1. Unless otherwise stated, we use keyframes that are 30 frames apart. In total, we trained our methods on a 60 second video sequence (flame salmon) in 6 chunks with each 10 seconds in length, five other 10 seconds cooking videos captured at different time with different motion and lighting, and one 25 seconds video in indoor videos in 5 chunks. We also trained a few additional videos of outdoor scenes in chunks of 5 seconds with denser keyframes, which are 10 frames apart. In the end, we employ a subset of 18 camera views for training, and 1 view for quantitative evaluation for all datasets except one sequence observing multiple people moving, which only uses 14 cameras views for training. We calculate a continuous interpolated spiral trajectory based on the training camera views, which we employ for qualitative novel view evaluation.