In GNCD, base and novel sets contain different training targets — for base set we have accurate and consistent ground-truth labels, while for novel set the training targets (generated pseudo labels) are less accurate and may change during training. We aim to build a model able to handle both base and novel classes, which is challenging due to (1) the ambiguity between base and novel classes, and (2) the lack of supervision on novel data. To address (1), we propose to model base and novel data using two groups of compositional experts with complementary specialties from both batch-wise and class-wise perspectives. For (2), we propose a Local Aggregation strategy to refine the pseudo labels by encouraging local consistency among novel data. Our idea of the compositional structure comes from the observations towards GNCD output space, which can be clearly characterized with a batch-class view (cf. Fig. 1b in our paper). From this view the GNCD output space exhibits a compositional nature in both batch-wise and classwise perspectives. We propose to model this compositional nature using a group of batch-wise experts (Fig. 1c) and another group of class-wise experts (Fig. 1d). Each group of experts is capable of characterizing the whole dataset, yet presents different specialties — with batch-wise experts capturing separability between base and novel sets, and class-wise experts modeling discriminability within each set of classes. Considering the challenges of GNCD, we keep both groups of experts to make the most of their learning abilities by allowing complementary collaborations. We further propose to address the aforementioned inconsistent training targets using local aggregation for novel samples, which complement the global-to-local pseudolabeling with local-to-local regularization. Extensive experiments have validated the effectiveness of the above ideas, which compose our proposed ComEx and contribute to its superior performance in GNCD.