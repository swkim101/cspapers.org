Our goal is to populate digital environments, in which digital humans have diverse body shapes, move perpetu-ally, and have plausible body-scene contact. The core challenge is to generate realistic, controllable, and infinitely long motions for diverse 3D bodies. To this end, we propose generative motion primitives via body surface markers, or GAMMA in short. In our solution, we decompose the long-term motion into a time sequence of motion primitives. We exploit body surface markers and conditional variational autoencoder to model each motion primitive, and generate long-term motion by implementing the gen-erative model recursively. To control the motion to reach a goal, we apply a policy network to explore the genera-tive model's latent space and use a tree-based search to preserve the motion quality during testing. Experiments show that our method can produce more realistic and controllable motion than state-of-the-art data-driven methods. With conventional path-finding algorithms, the generated human bodies can realistically move long distances for a long period of time in the scene. Code is released for re-search purposes at: https://yz-cnsdqz.github.io/eigenmotion/GAMMA/