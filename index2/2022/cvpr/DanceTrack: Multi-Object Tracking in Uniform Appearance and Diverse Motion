A typical pipeline for multi-object tracking (MOT) is to use a detector for object localization, and following re-identification (re-ID)for object association. This pipeline is partially motivated by recent progress in both object detection and re- ID, and partially motivated by biases in existing tracking datasets, where most objects tend to have distin-guishing appearance and re-ID models are sufficient for es-tablishing associations. In response to such bias, we would like to re-emphasize that methods for multi-object tracking should also work when object appearance is not sufficiently discriminative. To this end, we propose a large-scale dataset for multi-human tracking, where humans have sim-ilar appearance, diverse motion and extreme articulation. As the dataset contains mostly group dancing videos, we name it “DanceTrack”. We expect DanceTrack to provide a better platform to develop more MOT algorithms that rely less on visual discrimination and depend more on motion analysis. We benchmark several state-of-the-art trackers on our dataset and observe a significant performance drop on DanceTrack when compared against existing benchmarks. The dataset, project code and competition is released at: https://github.com/DanceTrack.