This paper proposes a theoretical analysis of a Lasso-based classiﬁcation algorithm. Leveraging on a realistic regime where the dimension of the data p and their number n are of the same order of magnitude, the theoretical classiﬁcation error is derived as a function of the data statistics. As a result, insights into the functioning of the Lasso in classiﬁcation and its differences with competing algorithms are highlighted. Our work is based on an original novel analysis of the Iterative Soft-Thresholding Algorithm (ISTA), which may be of independent interest beyond the particular problem studied here and may be adapted to similar iterative schemes. A theoretical optimization of the model’s hyperparameters is also provided, which allows for the data- and time-consuming cross-validation to be avoided. Finally, several applications on synthetic and real data are provided to validate the theoretical study and justify its impact in the design and understanding of algorithms of practical interest. Abstract The appendix contains the main technical arguments omitted in the core of the article due to space limitation, and is organized as follows. Section A recalls the optimization problem of Lasso as well as the main goal of the theoretical analysis and the assumptions on the data. Section B derives the asymptotic classiﬁcation error of the Lasso-based classiﬁcation. To this end, Section B.1 proves the Gaussian distribution of the classiﬁcation score under concentrated random vector assumptions. Section B.2 details the strategy of the derivation of the ﬁrst and second order moment of the classiﬁcation score as well various lemmas related to Gaussian statistics of the soft threshold functions needed for Theorem 1 of the main paper. Sections B.2.2 and B.2.3 then provide the overall derivation of the mean and variance, respectively, of the score of decision for the case of generic covariance matrix. The identity covariance matrix is retrieved as a special case. Section C complements the experimental part of the main article by providing additional experiments and additional insights to the experiments derived in the main paper. Section D explains how to use the code provided as supplementary ﬁles to reproduce the results of the paper.