Caches are pervasively used in content delivery networks (CDNs) to serve requests close to users and thus reduce content access latency.  However, designing latency-optimal caches are challenging in the presence of delayed hits, which occur in high-throughput systems when multiple requests for the same content occur before the content is fetched from the remote server.  In this paper, we propose a novel timer-based mechanism that provably optimizes the mean caching latency, providing a theoretical basis for the understanding and design of latency-aware (LA) caching that is fundamental to content delivery in latency-sensitive systems.  Our timer-based model is able to derive a simple ranking function which quickly informs us the priority of a content for our goal to minimize latency.   Based on that we propose a lightweight latency-aware caching algorithm named LA-Cache.  We have implemented a prototype within Apache Traffic Server, a popular CDN server. The latency achieved by our implementations agrees closely with theoretical predictions of our model.  Our experimental results using production traces show that LA-Cache consistently reduces latencies by 5%-15% compared to state-of-the-art methods depending on the backend RTTs.