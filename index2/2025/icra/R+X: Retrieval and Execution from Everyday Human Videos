We present $\mathbf{R}+\mathbf{X}$, a framework which enables robots to learn skills from long, unlabelled, first-person videos of humans performing everyday tasks. Given a language command from a human, $\mathbf{R}+\mathbf{X}$ first retrieves short video clips containing relevant behaviour, and then executes the skill by conditioning an in-context imitation learning method (KAT) on this behaviour. By leveraging a Vision Language Model (VLM) for retrieval, $\mathbf{R}+\mathbf{X}$ does not require any manual annotation of the videos, and by leveraging in-context learning for execution, robots can perform commanded skills immediately, without requiring a period of training on the retrieved videos. Experiments studying a range of everyday household tasks show that $\mathbf{R}+\mathbf{X}$ succeeds at translating unlabelled human videos into robust robot skills, and that $\mathbf{R}+\mathbf{X}$ outperforms several recent alternative methods. Appendix and videos are available at https://www.robot-learning.uk/r-plus-x.