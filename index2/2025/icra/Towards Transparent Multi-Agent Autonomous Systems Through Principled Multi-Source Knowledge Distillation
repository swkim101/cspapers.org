Many real-world robotic applications can be formulated as Multi-Agent Path-Finding (MAPF) problems and approximated using Multi-Agent Reinforcement Learning (MARL) algorithms. However, the opaque nature of the blackbox neural network models employed by MARL algorithms has impeded their widespread adoption due to concerns over interpretability, debugging, and user trust. To address these limitations, we propose an interpretable MAPF framework that emulates a group of $n$ path-finding agents optimized through reinforcement learning (RL) using behavior trees (BTs), where $n$ is the number of agents in path-finding scenarios. Expert behavior datasets consisting of state-action trajectories from MARL algorithms are generated, and a knowledge distillation approach is employed to reduce the size of the datasets and extract implicit rules. Additionally, a principled rules factorization technique based on Boolean algebra theory is utilized to prune the behavior rules and create more compact BTs representations. The proposed framework is evaluated on randomly generated MAPF scenarios and demonstrates superior performance compared to conventional BTs generation methods. This paper advances the field of interpretable AI by enabling the extraction of understandable decision-making processes from complex reinforcement learning models in multiagent systems.