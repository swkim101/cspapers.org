This paper proposed M3DSS, a multi-platform, multi-sensor, and multi-scenario dataset for Simultaneous Localization and Mapping (SLAM) systems. Fifty-five sequences were collected from multiple platforms, including a handheld equipment, an unmanned ground vehicle, a quadruped robot, a car, and an unmanned aerial vehicle. Sensors used in M3DSS included two pairs of stereo event cameras with resolutions of $640\times 480$ and $346\times 260$, one infrared camera, four RGB cameras, two visual-inertial sensors, four mechanical and one solid-state LiDARs, three inertial measurement units, two global navigation satellite and inertial navigation systems with real-time kinematic signals. 21 various sensors were used on 5 different platforms under various challenging scenarios, including extreme illumination, aggressive motion, low-texture, high-speed driving scenarios, etc. To the best of our knowledge, M3DSS offered the richest event-based sensory information for SLAM up to date. We comprehensively evaluated state-of-the-art SLAM approaches and identified their limitations on M3DSS. Details could be found at https://neufs-ma.github.io/M3DSS.