Exoskeleton controllers have recently employed machine learning (ML) techniques to provide appropriate assistance throughout the terrains of the real world. One successful approach has been to learn a mapping between an exoskeleton wearer's kinematic measurements and a gait state vector that encodes how the wearer is currently walking (i.e. gait phase, speed), and then dynamically update the assistance based on the gait state. However, these methods require paired datasets of input kinematics to output gait states, which usually involves manual, time-consuming labeling of data from participants wearing specific exoskeletons and thus limits the scalability of these ML methods. A prior solution to this challenge—leveraging large pre-labeled datasets of normative human walking—introduces another problem, in that networks trained on these datasets learn only normative locomotion patterns, and thus may deteriorate when the data are changed by wearing the exoskeleton itself. In this context, we present an unsupervised-learning-based approach to both bypass the requirement of labeled data for gait state prediction and address the difficulty of domain adaptation from normative to exoskeleton-assisted walking. We validate our method in a set of walking simulations that featured exoskeleton data from 14 participants. This model showed significant improvements in state estimation relative to a model trained solely on pre-labeled normative walking, while also not requiring ground truth labels. This work presents a foundation that demonstrates labeled, device-specific data may not be required for predicting walking behavior in real time.