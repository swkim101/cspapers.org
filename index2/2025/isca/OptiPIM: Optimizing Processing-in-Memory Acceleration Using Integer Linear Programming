Processing-in-memory (PIM) accelerators provide superior performance and energy efficiency to conventional architectures by minimizing off-chip data movement and exploiting extensive internal memory bandwidth for computation. However, efficient PIM acceleration requires careful software-hardware mapping that transforms application algorithms into PIM operations and data layout. Unfortunately, existing PIM accelerators adopt manually tuned heuristics or exhaustive search to determine the mappings on PIM accelerators, leading to under-optimized performance and/or long optimization time. In this work, we propose OptiPIM, a novel optimization framework based on Integer Linear Programming (ILP) to efficiently generate the optimal mapping for data-intensive applications on PIM accelerators. The proposed framework adopts a PIM-friendly mapping representation with accurate cost modeling and a concise description of the entire design space, allowing us to formulate an efficient and effective ILP problem and optimize the mapping on PIM architectures. We implement OptiPIM in the open-source MLIR framework, enabling OptiPIM to generate optimized mappings for PyTorch workloads on PIM accelerators. We evaluate widely used machine learning workloads on two state-of-the-art PIM accelerators. Our experiments show that OptiPIM can generate optimal mappings within 4 minutes. Mappings generated by OptiPIM are at least 1.9 Ã— faster than those generated by heuristics.