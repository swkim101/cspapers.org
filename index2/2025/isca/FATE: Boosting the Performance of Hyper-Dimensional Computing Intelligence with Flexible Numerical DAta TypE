Hyper-Dimensional Computing (HDC) is a promising brain-inspired learning framework designed for efficient, hardware-friendly computation. By utilizing highly parallel operations, HDC encodes raw data into a hyper-dimensional space, facilitating efficient training and inference processes. However, the high precision required for representing high-dimensional vectors presents challenges for implementing HDC on resource-constrained edge devices, mainly due to the significant computational cost of performing multiplication for cosine similarity calculations. On the other hand, binary HDC offers lower costs but sacrifices accuracy. This paper addresses these challenges by focusing on data quantization, a hardware-efficient compression technique that incorporates sparsity to effectively balance cost and accuracy on embedded FPGA. Unlike existing methods that employ the same quantization scheme for all dimensions, we propose a novel solution that applies different numerical data types to different dimensions of data representations. This approach is motivated by two factors: (1) the varying importance among dimensions and (2) the potential for better utilization of heterogeneous FPGA hardware resources. To achieve this, we propose FATE, an algorithm/architecture co-designed solution that utilizes low-precision data types for less important dimensions, enabling the replacement of multiplication operations with logic calculations. This approach leverages FPGA Look-Up Table (LUT) resources for efficient implementation. The key insight of FATE is that dimensions with high importance require high-precision data types, while the low-importance dimensions do not, allowing them to be sacrificed for low hardware overheads. Furthermore, to maximize resource utilization, we design a workload-aware mixed quantization scheme that offers flexible compression based on these differences in dimensional importance. The proposed quantization framework is seamlessly integrated into the existing FPGA implementations, leveraging the heterogeneous resources available (LUTs and DSPs) for low-precision and high-precision dimensions, respectively. We evaluate FATE across multiple application domains. With optimized data type ratios on the Kintex-7 FPGA device, our design results in up to \( 50\% \) speedup and \( 53.79\% \) energy saving, while maintaining accuracy compared to solely exploiting DSPs for all dimension computations.