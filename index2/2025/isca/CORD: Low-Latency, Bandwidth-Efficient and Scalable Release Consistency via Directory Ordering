Increasingly, multi-processing unit (PU) systems (e.g., CPU-GPU, multi-CPU, multi-GPU, etc.) are embracing cache-coherent shared memory to facilitate inter-PU communication. The coherence protocols in these systems support write-through accesses that place the data directly at the LLC to enable efficient producer-consumer communications pervasive in AI/ML workloads. Moreover, release consistency has emerged as the standard memory model in such systems due to its programming simplicity and ability to support high performance. In todayâ€™s multi-PU systems, the source processor that issues the writes also orders them to enforce release consistency, even for write-through accesses. Unfortunately, such source ordering of write-through operations results in unnecessary communications between the source processor and the LLC directory, incurring significant performance, interconnect traffic, and energy overheads for multi-PU applications. To eliminate such communication, we present cord 1, a novel cache coherence protocol that orders write-through accesses directly at the cache directory. cord employs several novel mechanisms to minimize the metadata required for ordering traffic while efficiently scaling to multiple directories. Evaluations atop the gem5 simulator show that compared to source ordering, cord improves application performance by \( 24\% \) and reduces traffic by \( 13\% \) on average while incurring \( \lt 1\% \) storage, area, and power overheads. Compared to hand-optimized message-passing implementations, cord observes a mere \( 3\% \) performance overhead and \( 6\% \) more traffic on average with a significantly simpler programming model.