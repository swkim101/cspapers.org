Machine Learning as a Service (MLaaS) platforms facilitate collaborative machine learning, but trust issues necessitate privacy-preserving methods. As for tree ensembles, prior Fully Homomorphic Encryption (FHE)-based approaches suffer from slow inference speed and high memory usage. This work proposes the VESTA system that leverages compile-time precomputation and parallelizable model partitioning to enhance performance. VESTA achieves a 2.1x speedup and reduces memory consumption by 59.4% compared to state-of-the-art solutions.