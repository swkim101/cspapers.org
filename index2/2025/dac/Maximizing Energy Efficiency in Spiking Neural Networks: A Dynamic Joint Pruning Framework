Spiking Neural Networks (SNNs) face increasing challenges for efficient deployment as architectures grow in complexity, necessitating network pruning to improve energy and computational efficiency. Existing pruning methods primarily focus on a single form of sparsity, overlooking the importance of joint pruning, which is critical for minimizing synaptic operations (SOPs) and enhancing energy efficiency. This paper presents a novel dynamic joint pruning framework that leverages both spatiotemporal spike sparsity and weight sparsity to minimize SOPs in SNN inference. Based on a comprehensive analysis of the SOPs model, we introduce an integrated solution that combines a multi-stage masking mechanism for fine-grained neuron firing threshold control, a temporal attention batch normalization (TABN) module with learnable time scaling factors, and a dynamic sparse strategy that adjusts importance coefficients based on real-time computational impact. Experimental results on CIFAR-10, CIFAR-100, and ImageNet validate the effectiveness of proposed framework. Our method achieves up to $126.38 \times$ compression ratio of SOPs on CIFAR-10 with minimal accuracy loss, establishing a new state-of-the-art in energy-efficient SNN pruning.