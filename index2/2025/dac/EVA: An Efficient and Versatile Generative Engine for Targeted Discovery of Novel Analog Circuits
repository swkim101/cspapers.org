Analog circuit design has traditionally depended on manual expertise, slowing the discovery of novel topologies essential for advanced technologies like AI, $5 \mathrm{G} / 6 \mathrm{G}$, and quantum computing. While AI-driven methods have accelerated hardware design workflows, most of them focus on topology synthesis, often reusing known structures to achieve specific goals. The challenge of discovering entirely new, high-performance topologies remains largely underexplored due to its abstract nature. In this work, we introduce EVA, an efficient and versatile generative engine for discovering novel analog circuit topologies. EVA employs a bottom-up generation framework, using a decoder-only transformer to sequentially predict device pin connections and create diverse circuits from scratch. Pretraining on unlabeled circuit topologies builds foundational knowledge about circuit connectivity, achieving baseline discovery efficiency by generating valid circuits and reducing performance-labeled samples needed in fine-tuning. For targeted discovery of highperformance designs, EVA leverages two fine-tuning strate-gies-proximal policy optimization (PPO) and direct preference optimization (DPO)-to further enhance discovery efficiency for relevant, high-performing topologies. Experimental results across various circuit types highlight EVAâ€™s strengths in validity, novelty, versatility, and both training sample and discovery efficiency.