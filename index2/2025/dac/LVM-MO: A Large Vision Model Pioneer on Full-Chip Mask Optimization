Moving toward the post-Moore era, full-chip mask optimization (MO) has become a pivotal step for semiconductor designers and manufacturers in extending current resolution enhancement techniques. The majority of recent research efforts have focused on clip-level restoration, employing a divide-and-conquer approach to mitigate the impacts of optical proximity and process bias across entire chips. Nevertheless, when confronted with industrial full-chip mask optimization challenges, these works exhibit limited correction capabilities, struggle with generalization, and are time-inefficient. In this paper, we propose a novel full-chip mask optimization paradigm based on a massive lithography data-driven large vision model. Our approach features a foundation layout feature extractor, which is aware of the mutual influence of polygons in long-range pattern perception as well as optical physics and chemical characteristics of lithography, matters. Compared with state-of-the-art (SOTA) works, our work demonstrates significant advantages in terms of resolution fidelity, correction speed, and the ability to handle full-chip scale layouts.