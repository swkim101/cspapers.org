Point-based point cloud neural networks (PNNs) have shown remarkable accuracy in various applications. However, the performance of PNNs is usually limited on resource-constrained edge devices. This paper presents Point-CIM, an efficient Compute-in-Memory (CIM) based accelerator for PNNs, with software and hardware co-optimization. A reconfigurable CIM unit, which exploits the inherent zero-bit-skipping capability of CIM array, is designed to efficiently process Multiply-Accumulate (MAC) operations on decomposed feature data with high bit sparsity. A Voxel-Morton-sorted Partitioning (VMP) method combined with Channel-wise Minimum (CM) base point selection is proposed to improve the decomposition bit sparsity further and reduce the hardware implementation overhead. Additionally, a detailed Bit-level Truncation Quantization (BTQ) method is proposed to directly compress the bit-width of the offset without incurring any additional hardware overhead. Based on this decomposition method, a Pre-decomposition (PD) data movement strategy is employed to reduce data transfers of intermediate features between the on-chip buffer and CIM array. Extensive evaluation experiments on multiple datasets show that Point-CIM achieves an average speedup of $1.69 \times$ to $9.63 \times$, and $3.11 \times$ to $17.32 \times$ improvement in energy efficiency, compared to state-of-the-art accelerators and general-purpose processors.