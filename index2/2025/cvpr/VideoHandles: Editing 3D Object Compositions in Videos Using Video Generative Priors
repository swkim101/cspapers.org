Generative methods for image and video editing leverage generative models as priors to perform edits despite incomplete information, such as changing the composition of 3D objects depicted in a single image. Recent methods have shown promising composition editing results for images. However, in the video domain, editing methods have focused on editing objects’ appearance and motion, or camera motion. As a result, methods for editing object composition in videos remain largely unexplored. We propose VideoHandles as a method for editing 3D object compositions in videos of static scenes with camera motion. Our approach enables the editing of an object’s 3D position across all frames of a video in a temporally consistent manner. This is achieved by lifting intermediate features of a generative model to a 3D reconstruction that is shared between all frames, editing the reconstruction, and projecting the features on the edited reconstruction back to each frame. To the best of our knowledge, this is the first generative approach to edit object compositions in videos. Our approach is simple and training-free, while outperforming state-of-the-art image editing baselines. Our project page is https://videohandles.github.io.