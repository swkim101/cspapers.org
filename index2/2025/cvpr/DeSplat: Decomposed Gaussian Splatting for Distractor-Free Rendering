Gaussian splatting enables fast novel view synthesis in static 3D environments. However, reconstructing real-world environments remains challenging as distractors or occluders break the multi-view consistency assumption required for accurate 3D reconstruction. Most existing methods rely on external semantic information from pre-trained models, introducing additional computational overhead as pre-processing steps or during optimization. In this work, we propose a novel method, DeSplat, that directly separates distractors and static scene elements purely based on volume rendering of Gaussian primitives. We initialize Gaussians within each camera view for reconstructing the view-specific dis-tractors to separately model the static 3D scene and dis-tractors in the alpha compositing stages. DeSplat yields an explicit scene separation of static elements and distrac-tors, achieving comparable results to prior distractor-free approaches without sacrificing rendering speed. We demonstrate DeSplatâ€™s effectiveness on three benchmark data sets for distractor-free novel view synthesis. See the project website at https://aaltoml.github.io/desplat/.