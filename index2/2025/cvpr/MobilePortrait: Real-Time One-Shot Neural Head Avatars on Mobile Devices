Existing neural head avatars methods have achieved significant progress in the image quality and motion range of portrait animation. However, these methods prioritize effectiveness over computational overhead. This paper presents MobilePortrait, a lightweight one-shot neural head avatars method that reduces learning complexity by integrating external knowledge into both the motion modeling and image synthesis, enabling real-time inference on mobile devices. Specifically, We introduce a mixed keypoint representation of explicit and implicit keypoints for precise motion modeling and equip it with precomputed visual features to enhance both facial and background synthesis. With these designs, our model can match state-of-the-art performance using UNet with low FLOPs as the backbone, requiring less than 1/10 the computational demand. It has been validated to reach speeds of over 50 FPS on mobile devices and support both video and audio-driven inputs.