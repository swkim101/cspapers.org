Accurate, real-time machine perception is a key enabler of emerging mobile applications such as augmented reality and autonomous driving. However, running complex vision models within the tight latency budgets of resource-limited platforms remains challenging. We address two root causes: (i) the growing computational demands of state-of-the-art vision models and (ii) the variability of compute resource availability in on-device AI deployments. In this extended abstract, we introduce two adaptive perception systems that leverage AI-system co-design. Deployed on commercial devices and evaluated on representative perception workloads, our systems demonstrate high-performance perception under practical latency and resource constraints.