Transformer-based speech and language models deliver high-quality transcription and context-aware responses but require significant resources, complicating on-device deployment. Our work aims to build efficient mobile systems for real-time, accurate on-device model processing through system- and runtime-level innovations, thus eliminating cloud dependency and enhancing privacy. We address three main challenges: inefficient model execution pipelines causing temporal load imbalance, serving systems that do not fully exploit the heterogeneous compute and memory hierarchies of modern devices, and the overlooked potential of legacy devices as collective computational assets. To overcome these issues, we introduce optimizations such as pilot inference, hush word padding, and beam pruning, along with CPU/GPU pipelining for the first two challenges. These approach improves performance, reduces latency, and enables privacy-preserving speech understanding directly on personal devices as the native interface of the mobile devices. Additionally, we plan to develop scalable systems that orchestrate legacy phones to support more demanding models and applications, ultimately forming in-house clusters to democratize access to state-of-the-art models and promote a sustainable future.