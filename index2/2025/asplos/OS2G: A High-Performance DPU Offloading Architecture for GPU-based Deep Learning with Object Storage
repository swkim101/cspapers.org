Object storage is increasingly attractive for deep learning (DL) applications due to its cost-effectiveness and high scalability. However, it exacerbates CPU burdens in DL clusters due to intensive object storage processing and multiple data movements. Data processing unit (DPU) offloading is a promising solution, but naively offloading the existing object storage client leads to severe performance degradation. Besides, only offloading the object storage client still involves redundant data movements, as data must first transfer from the DPU to the host and then from the host to the GPU, which continues to consume valuable host resources. In this paper, we propose OS2G, a high-performance offloading architecture designed to free up valuable CPU resources while providing high-performance storage services for DL applications. The key idea of OS2G is to offload the object storage client to a DPU and enable direct data transfer between the DPU and GPU. Specifically, we design a high-performance OS2G Client running on the DPU, utilizing asynchrony, pre-reading, and concurrency strategies to provide high-performance object storage services. Additionally, we propose the GPUDirect DPU (GDD) technique for OS2G to optimize the data path, allowing direct data transfer between the DPU-accelerated storage system and the GPU computing system, fully bypassing the host. Results demonstrate that compared to S3FS and S3Connector, OS2G reduces the execution time of the ResNet18 model by 34.3% and 50.4%, and also decreases CPU consumption by 61.9% and 57.7%, respectively.