This paper presents the first edge computing framework for RTI systems combining intermediate sensor fusion with model quantization. Traditional RTI approaches [1] rely on centralized architectures (450 ms latency), while our key innovations enable 73% faster edge processing (120 ms) through hybrid ResNet architecture compressed via 8-bit quantization and intermediate fusion of RTI attenuation maps and RGB features. Our work introduces an on-device processing pipeline implemented on low-power IoT devices---including Jetson Nano, Raspberry Pi, and ESP32---that enables real-time inference without cloud-based computation. In our proposed system, radio tomography data is integrated with RGB camera input using an intermediate fusion strategy. This approach addresses RTI's inherent low spatial resolution and the "blob effect" by combining complementary features extracted from both modalities [2]. Optimized deep learning models---enhanced through model quantization, TensorRT acceleration, and lightweight inference techniques---are employed to efficiently meet the constraints of embedded hardware while maintaining high detection accuracy. Experiments show 94.2% accuracy vs 92.5% in cloud systems with 67% lower energy use 5.5W vs 15W. First implementation of realtime RGB-RTI fusion on edge devices, outperforming SOTA edge-RTI by 18% in latency. These findings underscore the potential for deploying privacy-preserving, low-power, and real-time human detection systems in smart buildings, security applications, and IoT-based occupancy monitoring.