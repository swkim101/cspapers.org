Existing autonomous vehicles have not utilized the cloud computing for execution of their deep learning-based driving tasks due to the long vehicle-to-cloud communication latency. Meanwhile, the vehicles are in general equipped with the resource-constrained edge computing devices which may be unable to execute the compute-intensive deep learning models in real time. The increasing data transmission speed of the commercial mobile networks sheds light upon the feasibility of using the cloud computing for autonomous driving. Our city-scale real-world measurements show that the vehicles can partially use the cloud computing via the fifth generation (5G) mobile network with the low data transmission latency. In this paper, we present the design and implementation of ECSeg, an edge-cloud switched image segmentation system that dynamically switches between the edge and cloud for executing the deep learning-based semantic segmentation models to understand the vehicle's visual scenes in real time. The switching decision-making is challenging due to the intricate interdependencies among various factors including the dynamic wireless channel condition, vehicle's movement and visual scene change. To this end, we employ deep reinforcement learning to learn an optimal switching policy. Extensive evaluation based on both real-world experiments and trace-driven simulations demonstrates that ECSeg achieves superior image segmentation accuracy for autonomous vehicles, compared with four baseline approaches.