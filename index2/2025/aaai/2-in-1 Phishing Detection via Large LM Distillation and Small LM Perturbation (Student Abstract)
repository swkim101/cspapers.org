Phishing emails are an escalating threat, underscoring the need for precise detection methods. While large language models (LLMs) have gained attention for their potential in this area, their reliance on extensive data for fine-tuning poses practical challenges. This paper introduces DualLM for phishing detection with minimal data, which distills the reasoning ability from a large LM to enhance a small target LM and integrates trainable perturbations to improve the small LM's inference capabilities. Experiments demonstrate that DualLM can benefit from dual LMs, which reduces training parameters and data required, while maintaining high performance in phishing email detection with limited data.