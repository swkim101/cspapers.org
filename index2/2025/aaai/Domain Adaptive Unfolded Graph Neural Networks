Over the last decade, graph neural networks (GNNs) have made significant progress in numerous graph machine learning tasks.
In real-world applications, where domain shifts occur and labels are often unavailable for a new target domain, graph domain adaptation (GDA) approaches have been proposed to facilitate knowledge transfer from the source domain to the target domain.
Previous efforts in tackling distribution shifts across domains have mainly focused on aligning the node embedding distributions generated by the GNNs in the source and target domains.
However, as the core part of GDA approaches, the impact of the underlying GNN architecture has received limited attention.
In this work, we explore this orthogonal direction, i.e., how to facilitate GDA with architectural enhancement.
In particular, we consider a class of GNNs that are designed explicitly based on optimization problems, namely unfolded GNNs (UGNNs), whose training process can be represented as bi-level optimization.
Empirical and theoretical analyses demonstrate that when transferring from the source domain to the target domain, the lower-level objective value generated by the UGNNs significantly increases, resulting in an increase in the upper-level objective as well.
Motivated by this observation, we propose a simple yet effective strategy called cascaded propagation (CP), which is guaranteed to decrease the lower-level objective value.
The CP strategy is widely applicable to general UGNNs, and we evaluate its efficacy with three representative UGNN architectures.
Extensive experiments on five real-world datasets demonstrate that the UGNNs integrated with CP outperform state-of-the-art GDA baselines.