The integration of social information into recommender systems (RSs) has gained significant popularity for enhancing recommendation performance and user experience. However, this practice introduces substantial privacy risks, particularly concerning the leakage of sensitive social relationships. While prior research has primarily focused on user-level and interaction-level privacy risks, the social relation-level privacy risks remain largely unexplored. To fill this gap, we investigate social privacy risks through membership inference attacks (MIA) and propose a Social relation-level MIA (SMIA) framework. Two key challenges arise: (1) the adversary can only access the recommended item IDs, which provide indirect and limited information about social relationships, and (2) extracting socially relevant preferences from recommendation results is inherently difficult. To tackle the first challenge, we leverage shadow models to transform sparse item IDs into dense features, enabling adversaries to effectively utilize recommendation outputs. For the second challenge, SMIA employs a dual-branch learning approach that disentangles social and behavioral preferences. Therefore, we can extract socially relevant signals from the disentangled preferences.Extensive experiments on real-world datasets demonstrate that both social and general RSs are highly vulnerable to such attacks, highlighting the urgent need for robust privacy protection mechanisms. To defend against these attacks, we introduce a Socially Adversarial Learning (SAL) defense mechanism that selectively obscures sensitive social information in user representations during training, effectively reducing privacy leakage. We further evaluate the effectiveness of our defense and discuss future directions for developing privacy-preserving mechanisms in social RSs.