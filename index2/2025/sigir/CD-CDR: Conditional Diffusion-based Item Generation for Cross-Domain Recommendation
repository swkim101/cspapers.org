Cross-domain recommendation (CDR) has emerged as a promising direction for expanding the applicability of recommendation systems. Recent advances in CDR have demonstrated the effectiveness of the unified distribution paradigm, which leverages shared distributions to transfer knowledge across domains and employs domain-specific adapters for targeted recommendations. While this well-designed paradigm shows promising performance, existing methods require extra supervision signals (e.g. contrastive learning on domain-masked embeddings) to maintain unified distributions across domains, leading to an inherent trade-off between unified objectives and domain-specific preference modeling. To address these limitations, we propose CD-CDR (Conditional Diffusion-CDR), a novel approach that leverages a shared conditional diffusion model to learn unified item distributions and facilitate knowledge transfer across domains. The key insight is to utilize the powerful generative capabilities of diffusion models to learn a shared distribution while naturally incorporating domain-specific characteristics through conditional generation. This design enables CD-CDR to replace traditional adapters with generation conditions as an integral part of the distribution model, thereby eliminating extra supervision signals and fundamentally resolving the trade-off between unified and domain-specific objectives. Extensive experiments on six domain pairs from two real-world datasets demonstrate that CD-CDR significantly outperforms existing methods for both normal and cold-start settings. To the best of our knowledge, this is the first work to explore the unified distribution paradigm in CDR using conditional diffusion models.