Large language models (LLMs) can be effective at retrieval but are generally too expensive to use as first-stage rankers. As a consequence, several approaches to their use as rerankers of less expensive first-stage retrieval results have been suggested. Zhuang et al. recommend setwise approaches. In particular, they use heapsort to efficiently return top-ranked documents by obtaining a partial order from each LLM call. Utilizing rerankers requires setting parameters such as %window size, number of input documents, number of documents to be reranked, evaluation depth, number of tokens per document, and selection of LLM, all of which contribute to latency and effectiveness. In this work, we reproduce the batching and reranking of Zhuang et al. with a larger comparison window size. Furthermore, we determine that the document truncation used in the original implementation is suboptimal. By providing more context to the LLM, we show that the reranker is more effective than originally reported.