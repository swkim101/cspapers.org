Large language models (LLMs) cannot effectively collaborate with humans who provide imperfect information at the initial stage of the dialogue, unless they learn to proactively ask questions. Our core idea is to enable LLMs to decide whether to take the action of ''ask'' or ''tell'' at each turn by self-reasoning, with the belief of the decisions enhanced by retrieving knowledge related to the user input. Thus, we propose the ask and retrieve knowledge framework (Ark), where LLMs think through what to retrieve, when to stop retrieving, and then take actions accordingly. Ark is used to produce the action paths for model training. To mitigate the collapse of models trained on synthetic data, we propose a progressive training strategy: self-reason learning by supervised fine-tuning on produced paths and knowledge alignment through direct preference optimization on doctor response. To evaluate the information gain brought by the ask action, we design a method to calculate the ask utility value (AUV) based on the expected value of perfect information (EVPI) theory. Although MedArk is trained using synthetic data from GPT-4o-mini, it highly outperforms GPT-4o and other medical LLMs in six aspects: helpfulness, hallucination, action selection, BERTScore, AUV, and asking correctness. MedArk also achieves SOTA results in the perfect information scenario, i.e., medical examinations. We release our code, data and models at https://github.com/Bolin97/MedArk.