Large Language Models (LLMs) have shown remarkable proficiency in Natural Language Generation (NLG) across various tasks. However, they often require additional resources beyond their internal knowledge to respond reliably to user queries. Determining the optimal methods, content, and timing for introducing new knowledge remains a critical challenge without clear solutions. Our main objective is to enhance knowledge injection in LLMs to generate trustworthy responses. Therefore, this research is centered around two main research questions. (RQ1) What are the most effective and efficient choices of knowledge injection for question-answering over less popular knowledge? A key challenge in knowledge injection is determining how to introduce new knowledge into an LLM while balancing both effectiveness and efficiency. RAG and FT with synthetic data have emerged as two distinct paradigms, yet there has been no comprehensive comparison that highlights both their strengths and limitations. To address this, we conduct an extensive evaluation of RAG and FT for handling less popular factual knowledge, assuming limited textual descriptions are available for a given domain and application. Through this analysis, we find that RAG substantially outperforms FT in this setup. Our second research question is: (RQ2 ) How can we quantify the uncertainty of LLMs during response generation and leverage it to improve the reliability of their outputs? By knowing LLMs uncertainty, one can determine when to leverage RAG and when to rely solely on LLMs' internal knowledge. However, existing Uncertainty Estimation (UE) techniques primarily focus on scenarios where the input consists only of a user query, overlooking the complexities introduced by retrieved knowledge. We investigate UE in the context of RAG and find that the performance of current UE methods is inconsistent, often degrading when non-parametric knowledge is incorporated into the input prompt. We propose an axiomatic framework to formalize optimal behavior of UE methods. Recent advancements in active RAG aim to enhance the dynamic interaction between retrievers and generators. In this context, UE is primarily employed to determine when retrieval is necessary. Typically, uncertainty is measured based on next-token probabilities, and the interpretation of an uncertainty value is based on comparing it with accuracy. However, we raise three key questions: (RQ2.1) What is the optimal method for measuring uncertainty? We argue that relying solely on probability-based methods may not be the most effective approach. Furthermore, in conversational systems, previous turns or sessions do not directly influence next-token probabilities, highlighting the need for a new UE method. (RQ2.2) How should uncertainty values be interpreted? Current approaches, which define uncertainty through relative comparisons with other values, lack precision. Additionally, some applications correlate uncertainty with correctness, but how should uncertainty be interpreted in cases where correctness labels are unavailable? Moreover, we aim to enhance conversational interactions by leveraging an appropriate UE method. Specifically, we will investigate the research question: (RQ2.3) Can an LLM anticipate its next action, identify its information needs, and then generate an appropriate response? To explore this, we will utilize UE to help the LLM determine its next step, whether to generate a response, retrieve relevant information, ask a clarification question, or take an alternative action.