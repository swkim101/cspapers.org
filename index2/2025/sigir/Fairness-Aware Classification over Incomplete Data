The missing values widely existed in tabular data hinder the effective analysis of algorithmic fairness. Existing fairness intervention algorithms incorporate constraints or regularizers to reduce discrimination which rely on the complete information. They cannot effectively handle common tabular data with missing values in both sensitive and non-sensitive attributes without imputation. In this paper, we propose a novel Transformer-based fairness-aware prediction model FATE that mitigates the bias introduced by missing values to achieve algorithmic fairness without imputation. FATE consists of two modules, i.e., an incomplete data encoding (IDE) module and a debiased representation learning (DRL) module. IDE designs an incomplete tabular data embedding strategy and a missingness-aware Transformer block to effectively learn the observed data distribution and the missing state information. DRL converts fairness into attention parity when the sensitive attributes are completely missing. It offers a debiased attention mechanism to normalize attention weights in the attention score calculation process. We theoretically prove that, the differences in attention scores can represent the demographic disparities among sensitive groups which in FATE are bounded by a constant, substantially minimizing the group discrimination. Extensive experiments on three public real-world datasets demonstrate that, FATE, with the competitive fairness, yields more than 22% accuracy gain, compared to the state of the arts.