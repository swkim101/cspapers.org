Generative AI (GenAI) models are increasingly employed for professional search tasks, from enterprise copilots to systematic reviews. Recently introduced ''deep research'' products (OpenAI, Google), marketed as automated professional search agents, promise comprehensive query generation and synthesis from large document sets. However, this drive toward automation risks minimizing user involvement, potentially leading to misaligned results and inappropriate reliance on AI-generated outputs. Professional search, especially in domains such as biomedical research and systematic reviews, necessitates expert input, transparency, and user control, requirements often not met by current generative search tools. Interactive Information Retrieval (IIR) literature has long emphasized the value of user interactions and iterative search refinement. The rise of generative search introduces innovative methods of accessing information through natural language interactions, which show promise in addressing complex information needs. However, professional search tasks differ substantially from general consumer searches, involving transparent criteria for relevance judgments and systematic data extraction processes. Despite early evidence suggesting increased productivity among knowledge workers using GenAI tools. these systems also exhibit substantial shortcomings. Studies highlight issues such as unsupported claims, inaccurate citations, and overreliance on generated content, questioning the reliability and suitability of current generative search systems for high-stakes professional use. Given these challenges, this work explores systematically integrating structured expert feedback at multiple stages of a generative search process, specifically query formulation, relevance judgments, and information extraction. We aim to shift the focus from full automation toward increased expert control and quality assurance. Our key research questions are:RQ1: How can expert feedback be systematically integrated into GenAI-driven retrieval systems to improve result quality? RQ2: How does a feedback-driven generative search system compare to fully automated approaches in terms of retrieval effectiveness and task performance? RQ3: Does the integration of human expert feedback improve professional search outcomes compared to GenAI-simulated expert feedback? Building on our earlier experiments that assessed the performance of GenAI models in a fully automated setting for biomedical question answering and introduced an interactive system for refining generated queries, we now seek to evaluate how a structured, human-in-the-loop feedback approach might improve retrieval and task effectiveness over fully automated pipelines. By evaluating our proposed framework on biomedical Q&A datasets such as CLEF BioASQ and TREC BioGen as well as systematic review datasets, we want to highlight the value of human guidance for professional generative search.