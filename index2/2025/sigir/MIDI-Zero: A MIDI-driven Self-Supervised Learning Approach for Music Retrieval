Content-based Music Retrieval (CBMR) is a fundamental task in music information retrieval, encompassing sub-tasks including Audio Identification, Audio Matching, and Version Identification. Traditional methods typically analyze audio signals or spectrograms to extract features related to rhythm, melody, harmony, and timbre. However, with the rapid development of Music Transcription and digital music technologies, MIDI representation has emerged as a powerful alternative fo r music analysis. In this paper, we propose MIDI-Zero, a novel self-supervisedlearning framework for CBMR that operates entirely on MIDI representations. Unlike existing approaches, MIDI-Zero requires no external training data; all training data is automatically generated based on predefined task rules, eliminating the need for labeled datasets or external music collections. MIDI-Zero is designed to handle both symbolic music data and audio-based tasks by leveraging Music Transcription models. Its strong robustness ensures effectiveness even with low-quality transcriptions. Extensive experiments demonstrate that MIDI-Zero achieves competitive performance across various CBMR sub-tasks, particularly excelling in Audio Matching. Our approach simplifies the feature extraction process, bridges the gap between audio and symbolic music representations, and offers a versatile and scalable solution for music retrieval.