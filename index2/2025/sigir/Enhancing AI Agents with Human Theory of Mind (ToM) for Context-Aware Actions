In the domain of artificial intelligence, the role of agents is rapidly expanding. An agent is an autonomous entity that perceives its environment, makes informed decisions, and takes actions to complete tasks. However, enabling agents to accurately interpret and respond to human requests-particularly through understanding human Theory of Mind (ToM)-remains a complex and pivotal challenge. ToM encompasses the ability to recognize that individuals possess distinct mental states, such as intentions, beliefs, and desires, which may differ from one's own. This cognitive faculty is fundamental to human interaction, enabling empathy and understanding across diverse contexts. As AI agents are increasingly deployed across new domains, approximating ToM becomes vital for producing appropriate and context-sensitive responses. Recent studies have explored methodologies to enhance ToM capabilities in large language models (LLMs), aiming to refine their ability to infer and track human mental states. By leveraging ToM, LLMs have been shown to approximate key aspects of human-like social cognition. However, most existing research has focused primarily on evaluating ToM within LLMs, overlooking the broader challenge of enabling AI agents to actively understand and respond to human cognitive and mental states. While previous studies have developed agents focused on extracting human mental states, further research is needed to design agents capable of performing diverse tasks while integrating ToM reasoning. AI agents must not only infer mental states but also leverage them to enhance reasoning, planning, decision-making, and task execution. For example, an educational tutor agent assisting a student with a math problem must recognize whether the student seeks step-by-step guidance or conceptual clarification, and whether their broader intent is to master the material or to prepare for an exam. Without accurately inferring and leveraging such mental states, agents risk producing ineffective interactions, leading to user frustration and disengagement. Incorporating ToM information is therefore crucial for improving decision-making and enabling more contextually appropriate responses. To address this gap,ToM capabilities must be incorporated into AI agents by modeling core mental states such as Belief, Desire, and Intention (BDI). Building on BDI as a structured foundation for representing human cognition, ToM-Act is proposed as a framework to enhance agent decision-making and action-taking through ToM-informed reasoning. This motivation stems from the increasing deployment of AI agents, highlighting the need for methods that optimize their decision-making.