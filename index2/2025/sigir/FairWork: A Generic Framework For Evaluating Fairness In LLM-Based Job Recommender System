Large Language Models (LLMs) have revolutionized recommender systems by offering highly personalized and context-aware suggestions. However, their inherent biases pose significant challenges in sensitive scenarios like job recommendation, potentially compromising fairness and resulting in harmful effects on both users and platforms. While previous studies have explored fairness issues in LLM-based job recommendations, they often focus on limited dimensions. We introduce FairWork, a comprehensive fairness evaluation framework to examine LLM-based recommender system from both the user's and recruiter's perspectives, employing fairness metrics to assess how sensitive user attributes influence job recommendations. The system allows stakeholders such as recruitment platforms and job seekers to upload personalized profiles and job descriptions for fairness analysis. By integrating specific job requirements and user-driven data inputs, FairWork captures the relationship between candidate qualifications and job demands. This framework provides a robust foundation for evaluating fairness in LLM-based job recommender systems and supports future research on bias mitigation strategies. The demo is available at https://github.com/chenzhouli/FairWork.