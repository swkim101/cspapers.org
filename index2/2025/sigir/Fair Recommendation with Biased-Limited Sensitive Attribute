Ensuring fair recommendations for users with different sensitive attributes is essential for building trustworthy recommender systems. A significant challenge in achieving this in the real world is that some users are unwilling to disclose their sensitive attributes, limiting the applicability of traditional approaches. Recent efforts have attempted to address this challenge by reconstructing sensitive attributes based on the observed data. However, the observed data often does not represent an unbiased sample of the true distribution, rendering the reconstructed results unreliable. Moreover, it is difficult to select a debiasing method to achieve unbiased reconstruction, due to lacking sufficient prior knowledge about the bias. This motivates us to develop new fairness approaches. This work proposes a new method, Multiple Prior-Guided Robust Optimization (MPR), to achieve fair recommendations under biased observations of sensitive attributes, without requiring real sensitive attribute distribution. MPR begins by estimating various potential distributions of sensitive attributes using multiple randomly set priors, and then ensures fairness by minimizing the worst-case unfairness across all estimations. Thus, MPR can ensure fairness optimization under the real distribution approximately once one of the estimates approaches the true distribution. Both theoretical and empirical evidence demonstrate that MPR effectively ensures fairness in recommender systems when sensitive attribute observations are limited and biased. The code and data are released at https://github.com/jizhi-zhang/MPR.