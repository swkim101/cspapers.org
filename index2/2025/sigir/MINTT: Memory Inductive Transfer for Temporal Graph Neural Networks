Interactions between entities are often time-dependent in real-world systems such as e-commerce, social networks, streaming platforms, finance, and healthcare, and are best modeled as temporal interaction graphs. The temporal dimension plays a crucial role in modern recommendation systems, which rely on future link predictions. Temporal Graph Neural Networks (TGNN) have demonstrated state-of-the-art performance in future link prediction tasks for temporal interaction graphs. However, these models often require substantial training data unavailable in real-world settings. A potential solution to data scarcity is model pre-training on semantically related datasets. Unfortunately, transferring the TGNN model from one dataset to another is not trivial, as it contains node-specific memory modules vital for performance, resulting in them being inherently non-transferable. To overcome this limitation, we propose a novel transfer method that effectively utilizes common attributes between source and target datasets by decoupling graph nodes and corresponding attributes via bipartite encoding. This decoupling facilitates the transfer of memories and other inductive biases from source datasets to a target dataset. We evaluate the proposed transfer technique on real-world datasets and establish that it improves the performance of TGNN on the target dataset by 56% compared to the no-transfer methods and 36% over the state-of-the-art baselines in data-scarce settings.