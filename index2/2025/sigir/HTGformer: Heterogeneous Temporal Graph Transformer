In recent years, heterogeneous temporal graphs (HTGs) have attracted substantial attention in applications of information retrieval such as recommender systems and social networks. To enhance representation learning in HTGs, numerous tailored neural networks have recently been proposed. Despite these successes, existing methods adopt independent parameterization strategies to handle various data distributions in HTGs, leading to optimization challenges and speed bottlenecks. To bridge this gap, this paper proposes a novel transformer-based representation learning paradigm for HTGs called HTGformer. Specifically, assisted by two major modules, i.e., a graph embedding layer and a heterogeneous-temporal encoder, HTGformer can effectively and efficiently capture spatio-temporal heterogeneous information in HTGs for comprehensive node representations. Extensive experiments demonstrate that HTGformer achieves up to 6Ã— speed-up compared to the state-of-the-art baseline while maintaining the best forecasting accuracy.