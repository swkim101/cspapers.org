Cross-Domain Recommendation (CDR) leverages auxiliary information from extra domains to enhance performance by learning domain-invariant and domain-specific representations. However, existing methods primarily rely on ID information of users and items, resulting in the entanglement of these two representations and hindering effective knowledge transfer. Therefore, we present a novel plug-in contrastive learning for CDR (PicCDR), which utilizes textual semantics to disentangle and enhance domain-invariant and domain-specific representations via LLMs. First, PicCDR introduces the CoT prompting to generate and encode content-independent domain-invariant and domain-specific texts. Next, a contrastive domain-disentangled augmentation strategy is used to align domain-invariant and domain-specific representations in the semantic space with those of ID space via MI estimation. To further enhance representations, we present contrastive MI lower-bound and upper-bound approximations to optimize MI maximization and minimization terms. We also provide theoretical proof to reveal the superiority of our contrastive strategy. Lastly, we encapsulate PicCDR into a plug-and-play framework. This allows PicCDR to be plugged into any existing CDR model. Extensive experiments show the efficiency, robustness, and generalization of PicCDR.