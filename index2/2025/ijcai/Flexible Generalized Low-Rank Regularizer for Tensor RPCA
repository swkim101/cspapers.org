Tensor Robust Principal Component Analysis (TRPCA) has emerged as a powerful technique for low-rank tensor recovery. To achieve better recovery performance, a variety of TNN (Tensor Nuclear Norm) based low-rank regularizers have been proposed case by case, lacking a general and flexible framework. In this paper, we design a novel tensor low-rank regularization framework coined FGTNN (Flexible Generalized Tensor Nuclear Norm). Equipped with FGTNN, we develop the FGTRPCA (Flexible Generalized TRPCA) framework, which has two desirable properties. 1) Generalizability: Many existing TRPCA methods can be viewed as special cases of our framework; 2) Flexibility: Using FGTRPCA as a general platform, we derive a series of new TRPCA methods by tuning a continuous parameter to improve performance. In addition, we develop another novel smooth and low-rank regularizer coined t-FGJP and the resulting SFGTRPCA (Smooth FGTRPCA) method by leveraging the low-rankness and smoothness priors simultaneously. Experimental results on various tensor denoising and recovery tasks demonstrate the superiority of our methods.