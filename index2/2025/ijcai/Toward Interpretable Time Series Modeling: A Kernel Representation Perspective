Time series modeling is essential in finance, healthcare, and environmental science, yet nonlinear patterns, noise, and concept drift pose challenges. Although deep learning models, such as Transformer-based and recent pre-trained models, have achieved good performance across various time series tasks, they often lack interpretability, especially in co-evolving time series. This work introduces a kernel representation learning (KRL) perspective, rethinking time series modeling through kernel-induced self-representation to effectively capture temporal structures and dynamic transitions. Additionally, we establish theoretical connections between KRL and advanced deep-network models, demonstrating how kernel methods provide a principled approach to capturing complex time series behaviors.