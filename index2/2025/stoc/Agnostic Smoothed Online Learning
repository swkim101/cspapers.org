Classical results in statistical learning typically consider two extreme data-generating models: i.i.d. instances from an unknown distribution, or fully adversarial instances, often much more challenging statistically. To bridge the gap between these models, recent work introduced the smoothed framework, in which at each iteration an adversary generates an instance from a distribution constrained to have density bounded by σ−1 compared to some fixed base measure µ. This framework interpolates between the i.i.d. and adversarial cases, depending on the value of σ. For the classical online prediction problem, most prior results in smoothed online learning rely on the arguably strong assumption that the base measure µ is known to the learner, contrasting with standard settings in the PAC learning or consistency literature. We consider the general agnostic problem in which the base measure is unknown and values are arbitrary. In this direction, Block et al. (2024) showed that empirical risk minimization has sublinear regret under the well-specified assumption. We propose an algorithm R-Cover based on recursive coverings which is the first to guarantee sublinear regret for agnostic smoothed online learning without prior knowledge of µ and without the well-specified assumption. For classification, we prove that R-Cover has adaptive regret