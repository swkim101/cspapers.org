CS1 courses have rapidly adopted Large Language Model (LLM)- based assistance, promising quick and always-available support for homework help. However, it is challenging to ensure that the hints and guidance provided by these models are accurate and not based on hallucinated solutions. In this work, we study and categorize LLM behavior in cases where students believe an LLM-powered homework tutor gave an inaccurate hint. We then describe correlations between certain student behaviors (SB) and our suggested bot-behavior (BB) categories.