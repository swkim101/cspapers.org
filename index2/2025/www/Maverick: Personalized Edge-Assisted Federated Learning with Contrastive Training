In an edge-assisted federated learning (FL) system, edge servers aggregate the local models from the clients within their coverage areas to produce intermediate models for the production of the global model. This significantly reduces the communication overhead incurred during the FL process. To accelerate model convergence, FedEdge, the state-of-the-art edge-assisted FL system, trains clients' models in local federations when they wait for the global model in each training round. However, our investigation reveals that it drives the global model towards clients with excessive local training, causing model drifts that undermine model performance for other clients. To tackle this problem, this paper presents Maverick, a new edge-assisted FL system that mitigates model drifts by training personalized local models for clients through contrastive local training. It introduces a model-contrastive loss to facilitate personalized local federated training by driving clients' local models away from the global model and close to their corresponding intermediate models. In addition, Maverick includes anomalous models in contrastive local training as negative samples to accelerate the convergence of clients' local models. Extensive experiments are conducted on three widely-used models trained on three datasets to comprehensively evaluate the performance of Maverick. Compared to state-of-the-art edge-assisted FL systems, Maverick accelerates model convergence by up to 16.2x and improves model accuracy by up to 12.7%.