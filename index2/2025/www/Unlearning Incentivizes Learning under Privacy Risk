While federated learning enables intelligent services and personalized user experiences, it raises privacy concerns due to regulatory requirements and user demands for data protection. Federated unlearning offers a potential solution to these issues. However, despite increasing demand for its practical implementation driven by right-to-be-forgotten regulations, the economic implications of federated unlearning on user behavior and platform profitability remain underexplored, potentially hindering its adoption. In this paper, we formulate a set of contract design problems for both unlearning-disabled and unlearning-enabled scenarios. Challenges arise when the unlearning-enabled platform jointly designs compensation for both learning and unlearning to incentivize users' sequential decisions to balance the expected revenue and unlearning cost. We first conduct a questionnaire survey that reveals that federated unlearning increases users' willingness to participate in federated learning. We then provide a necessary condition for maximizing the surplus of an unlearning-enabled platform, enabling the point-wise decomposition for the optimal contract design problem, based on which we minimize the incentive cost and maximize the surplus for the platform. Our further analysis reveals that i) the incentive effects of unlearning grow quadratically with users' privacy sensitivity, and ii) enabling unlearning may even profit more than disabling it when the training cost increases at a faster rate than the probability of privacy leakage as effort levels rise. Our numerical results show that the platform's profitability is primarily influenced by users' privacy sensitivity. When users have a relatively high privacy sensitivity, enabling unlearning can significantly improve profitability.