Fine-tuning Large Language Models (LLMs) is one response to the critique of LLMs being biased, erasing diversity, and raising ethical concerns. The Artificial Intimacy project employs artistic methods, taking personalization of chatbots to an extreme by fine-tuning LLMs on individual social media data. We find that regular GPT-3 chatbots attempt to circumvent value-laden content through flagging prompts and producing generic non-answers with variable success. While the transactional nature of such output allowed participants to make sense of responses with less personification, fine-tuned models presented value-laden, normative, and familiar personalities, resulting in strong personification as a way of making sense of the interactions. This mimicry of emotional connection resulted in a sense of artificial intimacy creating expectations for reciprocity and consideration that the models cannot express by design. As the commercialization of interactions with chatbots continues, we discuss the ethics of such emotional manipulation and its implications for personalization of LLMs.