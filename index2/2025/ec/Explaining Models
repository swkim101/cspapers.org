We study when and how explanations of complex models can aid a decision maker (DM) whose payoff depends on a state of the world described by inputs and outputs. The DM cannot directly understand the true model linking inputs to outputs and must instead rely on an explanation from a class of simpler intelligible models. We analyze mappings from the infinite-dimensional space of possible true models to the finite-dimensional space of intelligible ones — what we call explainers — and focus on those that yield explanations which are robustly useful: that is, they allow the DM to improve their worst-case payoff across all models that are consistent with the explanation received. When a decision maker cares about their average payoff across inputs, it is possible to offer an explanation that is robustly first-best across models using the canonical OLS approach. But no explainer can offer a useful explanation that is robust across both models and inputs, or even across both models and distributions of inputs. Moreover, the robust explanation offered by OLS is not robust to sampling error. We apply these results to policy evaluation and AI regulation. For average-scenario decisions, OLS explanations can guide action when data are reliable. For tail risk—such as policies affecting the worst-off or AI failures under extreme conditions—explanations are uninformative unless paired with strong theoretical structure. In such settings, theory is needed to rule out models that explanations cannot distinguish. Direct recommendations about actions is another way to achieve optimal decisions, but only if they are aligned with the DM's preferences. A full version of this paper can be found at https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4723587.