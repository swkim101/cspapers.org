Discrete choice models are fundamental for predicting how individuals select among alternatives based on their preferences and the attributes of the options. Traditional Random Utility Models assume stochastic rationality, often failing to capture context-dependent behaviorâ€”where the presence of certain alternatives alters the perceived utility of others. In this work, we introduce a flexible framework that embeds choice utilities into a vector-valued Reproducing Kernel Hilbert Space (RKHS) over choice-set and feature pairs, and thereby captures high-order interactions among items without imposing strong structural assumptions. In the featureless setting, we design a set kernel that accommodates varying choice-set sizes via a masking mechanism; with features, we construct a product kernel that combines set structure with item-specific attributes. Estimation under a fixed kernel reduces to a finite-dimensional convex program via the Representer Theorem. To ensure scalability, we propose two approaches: (i) random-feature approximations, which map data into a randomized low-dimensional space, reducing computational costs from quadratic to linear in both the number of items and samples; and (ii) a Neural Tangent Kernel formulation, which adopts deep multi-head attention neural architectures to learn a data-adaptive kernel, enabling efficient convergence via stochastic gradient descent. We establish generalization bounds that avoid the exponential "curse of cardinality". Empirical results on synthetic and real-world datasets demonstrate that our approach consistently outperforms existing benchmarks in both predictive accuracy and generalization. The full paper is available at https://ssrn.com/abstract=5267975.