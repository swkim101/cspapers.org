In many resource allocation settings, the value derived from resources depends on their usage history, with resources that have sufficient recovery or idle periods between allocations often providing greater utility or reward. This paper studies a resource allocation problem in which resource rewards recover over time following each use. Motivated by settings such as content recommendation, service platforms, and renewable energy management, we consider a dynamic matching problem with non-stationary arrivals, where customer types and matching preferences vary over time. Each arriving customer must be immediately and irrevocably matched to a resource or lost. The reward from matching a resource is non-decreasing in the time since its previous use. The goal is to maximize the expected reward collected from all arrivals over a finite time horizon. Computing the optimal policy for this problem is computationally intractable as the number of resources grows. While the greedy policy achieves at least 1/2 of the optimal expected revenue for subadditive reward functions, it performs arbitrarily poorly for general non-decreasing reward functions. To address this, we propose an approximate policy based on a novel value function approximation that achieves a 1/3 performance guarantee for general non-decreasing reward functions. We show that for the special case of concave reward functions as well as the special case where all resources are reusable, the approximate policy's guarantee improves to 1/2. Our approximation leverages resource-specific dynamic programs that incorporate each resource's recovery level as a state, a framework that we believe to be new to the literature. A full version of this paper can be found at https://ssrn.com/abstract=5046898.