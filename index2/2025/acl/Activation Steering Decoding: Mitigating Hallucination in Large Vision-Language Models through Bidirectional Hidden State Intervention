Large Vision-Language Models (LVLMs) have 001 demonstrated impressive capabilities in multi-002 modal understanding, but they frequently suffer 003 from hallucination - generating content incon-004 sistent with visual inputs. In this work, we 005 explore a novel perspective on hallucination 006 mitigation by examining the intermediate ac-007 tivations of LVLMs during generation. Our 008 investigation reveals that hallucinated content 009 manifests as distinct, identifiable patterns in 010 the model’s hidden state space. Motivated by 011 this finding, we propose A ctivation S teering 012 D ecoding (ASD), a training-free approach that 013 mitigates hallucination through targeted inter-014 vention in the model’s intermediate activations. 015 ASD operates by first identifying directional 016 patterns of hallucination in the activation space 017 using a small calibration set, then employing 018 a contrast decoding mechanism that computes 019 the difference between positive and negative 020 steering predictions. This approach effectively 021 suppresses hallucination patterns while preserv-022 ing the model’s general capabilities. Extensive 023 experiments demonstrate that our method sig-024 nificantly reduces hallucination across multiple 025 benchmarks while maintaining performance on 026 general visual understanding tasks. Notably, 027 our approach requires no model re-training or 028 architectural modifications, making it readily 029 applicable to existing deployed models. 030