We present ğ›¿ - Stance , a large-scale dataset of stances involved in legal argumentation. ğ›¿ - Stance contains stance-annotated argument pairs, semi-automatically mined from millions of examples of U.S. judges citing precedent in context using citation signals. The dataset aims to facilitate work on the legal argument stance classification task , which involves assessing whether a case summary strengthens or weak-ens a legal argument (polarity) and to what extent (intensity). To assess the complexity of this task, we evaluate various existing NLP methods, including zero-shot prompting proprietary large language models (LLMs), and supervised fine-tuning of smaller open-weight language models (LMs) on ğ›¿ - Stance . Our findings reveal that although prompting proprietary LLMs can help predict stance polarity, supervised model fine-tuning on ğ›¿ - Stance is necessary to distinguish intensity. We further find that alternative strategies such as domain-specific pretraining and zero-shot prompting using masked LMs remain insufficient. Beyond our datasetâ€™s utility for the legal domain, we further find that fine-tuning small LMs on ğ›¿ - Stance improves their performance in other domains. Finally, we study how temporal changes in signal definition can impact model performance, highlighting the importance of careful data curation for down-stream tasks by considering the historical and sociocultural context. We publish the associated dataset 1 to foster further research on legal argument reasoning.