While deep learning offers superior performance in systems and networking, adoption is often hindered by difficulties in understanding and debugging. Explainability aims to bridge this gap by providing insight into the model's decisions. However, existing methods primarily identify the most influential input features, forcing operators to perform extensive manual analysis of low-level signals (e.g., buffer t - 1). In this paper, we introduce Agua, an explainability framework that explains a model's decisions using high-level, human understandable concepts (e.g., "volatile network conditions"). Our concept-based explainability framework lays the foundation for intelligent networked systems, enabling operators to interact with data-driven systems. To explain the controller's outputs using concept-level reasoning, Agua builds a surrogate concept-based model of the controller with two mappings: one from the controller's embeddings to a predefined concept space, and another from the concept space to the controller's output. Through comprehensive evaluations on diverse applications—adaptive bitrate streaming, congestion control, and distributed denial of service detection—we demonstrate Agua's ability to generate robust, high-fidelity (93–99%) explanations, outperforming prior methods. Finally, we demonstrate several practical use cases of Agua in networking environments—debugging unintended behaviors, identifying distribution shifts, devising concept-based strategies for efficient retraining, and augmenting environment-specific datasets.