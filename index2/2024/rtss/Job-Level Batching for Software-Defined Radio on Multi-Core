Conventional wireless communication is built upon hardware-based signal processing. This enables high performance, but is inflexible as the signal-processing algorithms are “baked in” to the hardware. Software-defined radio (SDR) is an emerging solution in which more of the signal-processing logic is implemented in software instead of hardware. This allows for adaptability to spectrum conditions (e.g., jamming or congestion), changes to protocols, and software updates that improve signal-processing logic – features that are beneficial in many consumer and military applications. However, the high sampling rate $(\text{kHz}$ to MHz or faster) of many SDR applications poses significant challenges for real-time scheduling of such workloads. To manage high sampling rates on general-purpose processors, which process samples sequentially instead of in parallel, as can be done using hardware acceleration, samples must be buffered, or “batched” together, to minimize overheads and maximize locality. To address this characteristic of high-frequency signal processing, this paper presents an extension of traditional real-time scheduling models called the marginal cost model, which reflects the fact that when batching many samples, the marginal cost of processing additional samples is often much less than the cost of processing the first sample. Empirical evaluations are presented from the open source GNU Radio SDR framework to validate the marginal cost model. Experiments are then presented that demonstrate the trade-offs between batching and worst-case latency for synthetic SDR workloads. Finally, a case study is presented to demonstrate the utility of the presented model and batching techniques in real-world signal-processing applications.