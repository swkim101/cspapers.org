This paper proposes Blenda, a dynamically-partitioned memory-cache blend architecture for giga-scale die-stacked DRAMs. Blenda architects the stacked DRAM partly as memory and partly as cache, and dynamically adjusts each part's size to workloads' demands. The memory part hosts hot data objects and serves requests to them efficiently (i.e., without metadata overheads). The cache part captures transient data and filters requests to bandwidth-limited off-chip DRAM. Blenda provides three key contributions: (i) Blenda partitions stacked DRAM's capacity in a workload-aware manner: different workloads enjoy different memory-cache configurations. (ii) Blenda is reactive: the configuration is adjusted to workloads' phases dynamically and application-transparently: no reboot or user involvement are needed. (iii) Blenda gracefully transitions among configurations: no data invalidation is required upon most reconfigurations. We simulate 15 diverse big-data workloads running on a state-of-the-art processor and show that Blenda outperforms the best-performing prior architecture by 34%. Blenda's total storage overhead is less than 100 bytes per core.