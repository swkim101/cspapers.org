In this paper, we study asymptotic behaviors of continuous-time and discrete-time gradient flows of a “lower-unbounded” convex function on a Hadamard manifold, particularly, their convergence properties to the boundary at infinity. We establish a duality theorem that the infimum of the gradient-norm of a convex function is equal to the supremum of the negative of the recession function over the boundary, provided that the infimum is positive. Furthermore, the infimum and the supremum are obtained by the limits of the gradient flows of the function. Our results feature convex-optimization ingredients of the moment-weight inequality for reductive group actions by Georgoulas, Robbin, and Salamon, and are applied to noncommutative optimization by Bürgisser et al. FOCS 2019. We show that the gradient descent of the KempfNess function for an unstable orbit converges to a 1-parameter subgroup in the Hilbert-Mumford criterion, and the associated moment-map sequence converges to the minimum-norm point of the moment polytope. We show further refinements for operator scaling-the left-right action on a matrix tuple. We characterize the gradient-flow limit of operator scaling via a vector-space generalization of the classical Dulmage-Mendelsohn decomposition of a bipartite graph.