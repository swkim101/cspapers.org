A <tex>$(1\pm\epsilon)$</tex> -sparsifier of a hypergraph <tex>$G(V, E)$</tex> is a (weighted) subgraph that preserves the value of every cut to within a <tex>$(1\pm\epsilon)$</tex> -factor. It is known that every hypergraph with <tex>$n$</tex> vertices admits a <tex>$(1 \pm \epsilon)$</tex> -sparsifier with <tex>$\tilde{O}(n/{\epsilon}^{2})$</tex> hyperedges. In this work, we explore the task of building such a sparsifier by using only linear measurements (a linear sketch) over the hyperedges of <tex>$G$</tex>, and provide nearly-matching upper and lower bounds for this task. Specifically, we show that there is a randomized linear sketch of size <tex>$\tilde{O}(nr\log(m)/\epsilon^{2})$</tex> bits which with high probability contains sufficient information to recover a <tex>$(1\pm\epsilon)$</tex> cut-sparsifier with <tex>$\tilde{O}(n/\epsilon^{2})$</tex> hyperedges for any hypergraph with at most <tex>$m$</tex> edges each of which has arity bounded by <tex>$r$</tex>. This immediately gives a dynamic streaming algorithm for hypergraph cut sparsification with an identical space complexity, improving on the previous best known bound of <tex>$\tilde{O}(nr^{2}\log^{4}({m})/\epsilon^{2})$</tex> bits of space (Guha, McGregor, and Tench, PODS 2015). We complement our algorithmic result above with a nearly-matching lower bound. We show that for every <tex>$\epsilon\in(0,1)$</tex>, one needs <tex>$\Omega(nr\log(m/n)/\log(n))$</tex> bits to construct a <tex>$(1\pm\epsilon)$</tex> -sparsifier via linear sketching, thus showing that our linear sketch achieves an optimal dependence on both <tex>$r$</tex> and <tex>$\log(m)$</tex>. The starting point for our improved algorithm is importance sampling of hyperedges based on the new notion of <tex>$k$</tex> -cut strength introduced in the recent work of Quanrud (SODA 2024). The natural algorithm based on this concept leads to <tex>$\log m$</tex> levels of sampling where errors can potentially accumulate, and this accounts for the polylog <tex>$(m)$</tex> losses in the sketch size of the natural algorithm. We develop a more intricate analysis of the accumulation in error to show most levels do not contribute to the error and actual loss is only polylog <tex>$(n)$</tex>. Combining with careful preprocessing (and analysis) this enables us to get rid of all extraneous <tex>$\log m$</tex> factors in the sketch size, but the quadratic dependence on <tex>$r$</tex> remains. This dependence originates from use of correlated <tex>$\ell_{0}$</tex> -samplers to recover a large number of low-strength edges in a hypergraph simultaneously by looking at neighborhoods of individual vertices. In graphs, this leads to discovery of <tex>$\Omega(n)$</tex> edges in a single shot, whereas in hypergraphs, this may potentially only reveal <tex>$O$</tex>(<tex>$n$</tex>/<tex>$r$</tex>) new edges, thus requiring <tex>$\Omega(r)$</tex> rounds of recovery. To remedy this we introduce a new technique of random fingerprinting of hyperedges which effectively eliminates the correlations created by large arity hyperedges, and leads to a scheme for recovering hyperedges of low strength with an optimal dependence on <tex>$r$</tex>. Putting all these ingredients together yields our linear sketching algorithm. Our lower bound is established by a reduction from the universal relation problem in the one-way communication setting.