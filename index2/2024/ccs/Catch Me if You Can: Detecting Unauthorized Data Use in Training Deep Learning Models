The rise of deep learning (DL) has led to a surging demand for training data, which incentivizes the creators of DL models to trawl through the Internet for training materials. Meanwhile, users often have limited control over whether their data (e.g., facial images) are used to train DL models without their consent, which has engendered pressing concerns. In this work, we propose a technique that can support ordinary users to detect the unauthorized use of their data in training DL models. Our work is built upon membership inference (MI) attacks, a prominent class of attacks that aim to infer whether a sample was used to train the model, and it is known that the ability to perform accurate MI on a specific sample is directly related to how well the model memorizes it. Therefore, our idea is to guide the users to inject a small amount of targeted changes to their own data, which can be strongly memorized by the model trained on them. The users can then perform MI to detect whether the suspect model exhibits strong memorization effect on their specially-marked data. Preliminary results illustrate that our technique is able to support the users to reliably trace the provenance of their data with high true positive rate and low false positive rate.