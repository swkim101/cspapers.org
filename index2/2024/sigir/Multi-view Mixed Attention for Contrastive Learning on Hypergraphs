Hypergraphs are effective in learning high-order relationships between nodes, which naturally represent group interactions as hyperedges (i.e., arbitrary-sized subsets of nodes). However, most approaches currently used for learning hypergraph representations do not consider pairwise relationships between nodes. While high-order relationships provide insight into the general connections among nodes in a group, they do not reveal the pairwise relationships between individual nodes within that group. Considering that it is unlikely for all nodes in the same group to share identical relationships, we argue that considering pairwise relationships is a critical aspect. In this paper, we propose Multi-view Mixed Attention for Contrastive Learning (MMACL) to address the aforementioned problem. MMACL proposes Mixed-Attention, which blends high-order relationships derived from the hypergraph attention network and pairwise relationships derived from the graph attention network. Then, it performs node-level contrastive learning to the graph structure with different views learned at each layer to finally obtain an expressive node representation. Our extensive experimental results on several popular datasets validate the effectiveness of the proposed MMACL for hypergraph node classification. Our code is available at: https://github.com/JongsooLee-HYU/MMACL