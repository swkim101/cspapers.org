Interpretable models are crucial in many high-stakes decision-making applications. In this paper, we focus on learning a decision rule list for binary and multi-class classification. Different from rule set learning problems, learning an optimal rule list involves not only learning a set of rules, but also their orders. In addition, many existing algorithms rely on rule pre-mining to handle large-scale high-dimensional data, which leads to suboptimal rule list model and degrades its generalization accuracy and interpretablity. In this paper, we learn a rule list from the sequence submodular perspective. We consider the rule list as a sequence and define the cover set for each rule. Then we formulate a sequence function which combines both model complexity and classification accuracy. Based on its appealing sequence submodular property, we propose a general distorted greedy insert algorithm under Minorization-Maximization (MM) framework, which gradually inserts rules with highest inserting gain to the rule list. The rule generation process is treated as a subproblem, allowing our method to learn the rule list through a unified framework which avoids rule pre-mining. We further provide a theoretical lower bound of our greedy insert algorithm in rule list learning. Experimental results show that our algorithm achieves better accuracy and interpretability than the state-of-the-art rule learning methods, and in particular it scales well on large-scale datasets, especially on high-dimensional data.