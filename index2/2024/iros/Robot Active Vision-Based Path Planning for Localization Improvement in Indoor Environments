Reliable and robust navigation of autonomous mobile robots in indoor environments faces significant challenges due to the absence of GPS, visual degradation, repetitive structures, illumination variations, and low texture. These factors adversely affect localization systems. Current robots often use a uniform navigation approach, regardless of the varying localization uncertainties within different indoor environments. In this paper, we propose a holistic, active vision-based path planning method that produces efficient trajectories, aiming to minimize localization error and enhance navigation performance. Specifically, we utilize a 3D model of an indoor environment to derive an Artificial Potential Field (APF) with its associated localizability scores that encapsulate both visual features’ richness and fiducial markers’ placement. APF is employed to direct a Kinematically Constrained Bi-directional Rapidly Exploring Random Tree (KB-RRT) planner towards the calculation of optimal paths, prioritizing high localization areas. Subsequently, we use an online weight-adaptive MPC-based approach that, apart from robust path planning and obstacle avoidance, guides the robot towards areas with the most robust visual features in order to further refine the localization error. The proposed framework has been extensively tested in both simulation and real-world experiments with a mobile robot in a visually challenging indoor environment.