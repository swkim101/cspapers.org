Effective localization is crucial for the reliable operation of autonomous delivery robots. This paper introduces ConLocGAN, a novel context-aware GAN, addressing challenges in Lidar-based localization. Our approach employs a two-step process, integrating image retrieval with Lidar-based localization. ConLocGAN extracts robust global descriptors for coarse pose estimator, which acts as a precursor for Lidar-based pose refinement. The discriminator in ConLocGAN identifies differences in images of the same scene under diverse conditions at the feature level. This information is then utilized to enhance localization-specific feature extraction by the generator in a self-supervised setting. Additionally, we present a simple data collection pipeline that is seamlessly integrated into routine robot operations. Using heatmaps for visualization, we demonstrate that our network learns robust descriptors by prioritizing static components of the scene while effectively disregarding environmental changes such as illumination and weather, as well as dynamic objects like people and vehicles. We further validate our method on the challenging CMU seasons dataset, where it outperforms state-of-the-art retrieval-based methods in coarse pose estimation.