Autonomous Vehicles (AVs) have recently attracted considerable attention due to their potential to significantly reduce road accidents and improve peopleâ€™s lives. However, they rely solely on the data collected by their mounted sensors to make predictions, which can lead to inaccurate results if a sensor becomes occluded or damaged. This issue can be addressed by employing Vehicle-to-Vehicle communication, which allows a Connected Autonomous Vehicle (CAV) to interact with other CAVs within its field of view and exchange information about their surrounding objects. Existing research on cooperative perception has primarily focused on clear weather scenarios, with limited exploration into adverse weather conditions. This paper demonstrates the necessity of Vehicle-to-Vehicle communication by showcasing its benefits in maintaining high accuracy under adverse weather conditions. A collaborative perception system is introduced and its performance in foggy weather scenarios is assessed to further improve adverse weather perception. The pipeline of the network combines state-of-the-art methods for accurate object detection. Specifically, with PointPillars as the backbone, the Spatial-wise Adaptive Feature Fusion method is used to aggregate information from different vehicles. The model is trained on the large-scale dataset OPV2V and evaluated on modified data to simulate fog. The experiments show that cooperative perception can maintain high detection accuracy even in challenging weather conditions. Finally, a comparative analysis of LiDAR detectors for cooperative perception in bad weather conditions is presented.