Powered upper limb prostheses offer a particularly interesting case of human-machine interaction, where the user and the robot are physically coupled as an open chain manipulator. The biological and mechanical degrees of freedom (DOF) must collaborate for the user to manipulate objects in the environment. Current state-of-the-art systems use machine learning models to classify electromyogram (EMG) signals into motion intent primitives, allowing users to move the prosthetic joints sequentially at a fixed velocity. This interface is intended to work for simple systems but does not extend well into higher DOF. Consequently, current commercially available systems are limited to 1 or 2 powered DOF. In this paper, we present a semi-autonomous (SA) hybrid gaze-EMG interface that allows users to command the device in task-space instead of joint-space. Target end-effector poses are selected by tracking the user’s gaze vector, and then EMG signals guide the prosthetic along a calculated trajectory towards that pose. To examine how prosthesis interface performance scales with available mechanical DOF, we had 4 subjects complete virtual pick and place tasks with SA and traditional controller interfaces, varying the available DOF in the prosthetic wrist. Our results show that with the SA interface, increased DOF leads to a significant (p≤0.05) reduction in compensatory motion of the upper arm, more effective (p≤0.01) utilization of the increased configuration space, and overall more efficient motion (p≤0.01) than traditional classification based interfaces. These findings indicate that when given SA interfaces, subjects can benefit from fully articulated prosthetic devices, which motivates more clinical research into SA systems and commercial development of higher-DOF devices.