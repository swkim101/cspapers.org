Adaptive grippers enable easy and robust grasping of diverse objects by adapting to their shapes and enclosing them. However, determining the exact state of the hand remains challenging. This is not always straightforward but is often necessary to assess grip success, quality, or the pose of the object. In this work, we present two deep learning approaches using recurrent neural networks to successfully estimate the joint states of the Robotiq 3-Finger Adaptive Gripper. The models are compared with an existing analytical approach, which does not distinguish between the fingers of the hand and calculates the three angles of their respective joints using joint limits, contact information, and motor position in a transition model. We test the differences in accuracy with our networks by not distinguishing between the fingers as the analytical approach for the first model and by looking at the entire hand in the second model. Our experiments demonstrate that the model considering the entire hand outperforms the other two approaches, is more robust against object movements and achieves an average joint position accuracy of 2.29 degrees.