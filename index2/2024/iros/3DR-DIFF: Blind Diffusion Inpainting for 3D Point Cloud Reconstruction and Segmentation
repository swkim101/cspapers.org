LiDAR-based 3D perception is a focal point in autonomous vehicle research due to its efficacy in real-world environments and falling costs. However, recent research reveals challenges with LiDAR sensing under corruptions that occur due to adverse weather conditions and sensor-level errors, known as common corruptions. In particular, the majority of these corruptions lead to sparsity or noise in LiDAR point clouds, degrading the performance of downstream perception tasks. To address this, we propose a blind inpainting method named 3DR-DIFF, utilizing diffusion networks to reconstruct and segment corrupted point clouds. 3DR-DIFF comprises two key components: a corrupted region prediction network, acting as a binary mask predictor, and a conditional diffusion network. The evaluation results demonstrate that the 3DR-DIFF is able to reconstruct the LiDAR samples with a depth error of less than 0.56 mean absolute error (MAE) and an intensity error of 0.02 MAE, along with an average segmentation performance of 0.43 mean intersection over union. Furthermore, benchmarking results highlight that 3DR-DIFF outperforms state-of-the-art methods in reconstructing LiDAR beam-missing scenarios, exhibiting an approximately 9.2% lower error for a degradation of 1 MAE.