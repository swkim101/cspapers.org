Speech super-resolution techniques offer a promising solution to enhance audio quality in wearable devices, particularly when addressing the challenges of reduced sampling rates necessitated by battery life constraints and network instability. However, existing methods either prove computationally prohibitive for mobile platforms, or lack sufficient performance. We present TraMSR, a novel hybrid model combining transformer and Mamba architectures for acoustic speech super-resolution. TraMSR achieves superior performance while significantly reducing computational demands compared to state-of-the-art methods. Our model outperforms GAN-based approaches by up to 7.3% in Perceptual Evaluation of Speech Quality (PESQ) and 1.8% in Short-Time Objective Intelligibility (STOI), with an order of magnitude smaller memory footprint and up to 465 times faster inference speed.