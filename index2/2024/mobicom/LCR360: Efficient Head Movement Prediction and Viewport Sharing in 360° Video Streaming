In the realm of 360째 video streaming, three paramount challenges arise: accurate prediction of the field of view, efficient bandwidth utilization, and seamless viewport sharing. Current solutions often neglect the latter aspect of 360째 videos, thereby necessitating ultra-low latency, high quality, and real-time rendering. The considerable size of 360째 videos further intensifies these challenges. To tackle these issues, we introduce LCR360, a deep learning-based neural network solution designed for accurate head movement prediction. Our solution is deployed on E3PO [9], an end-to-end open-source evaluation platform dedicated to 360째 video-on-demand streaming. By integrating simulated streams into the Unreal Engine, we provide an immersive experience that enables users to share viewports seamlessly. Moreover, LCR360 surpasses its competitors by ~5% in the S-metric, a measure that combines the MSE of video quality, bandwidth, and storage costs.