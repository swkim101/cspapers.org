As a new federated learning(FL) paradigm, clustered federated learning (CFL) could effectively address the issue of model training accuracy loss due to different data distribution in FL. However, the introduction of the clustering process also brings new risks. Adversaries can implement model poisoning by adding crafted perturbations with clients' model parameters, potentially resulting in overall clustering failure. To tackle this problem, we propose a client detection and parameter correction framework in this paper. Our approach aims to identify malicious clients by analyzing the difference in vector parameter density distribution between malicious and benign clients. We precisely locate malicious perturbations in the parameters and recover them, enabling the server to effectively utilize benign updates for normal clustering and training within the CFL framework. Experiment results show that our defense algorithm outperforms others, consistently improving training accuracy by an average of 30% under various kinds of attacks.