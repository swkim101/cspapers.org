Numerous algorithms have been proposed for discovering denial constraints (DCs), which are essential and effective for maintaining data consistency. However, existing methods only focus on discovering the complete set of DCs, often resulting in hundreds or even tens of thousands of discovered rules. Such a large number of DCs are impractical for users to verify and utilize. Besides, these methods overlook the intent of users, which requires the discovered DCs to be succinct, relevant, and diverse concurrently. To address these limitations, we introduce DCMiner, a deep reinforcement learning (DRL)-based framework that produces rules satisfying user preferences. Specifically, we first model the discovering process via a kCover Markov decision process to improve efficiency. Then, a graphQ model is introduced to capture the data distribution and facilitate the discovery of DCs. Lastly, we design a reward function that flexibly integrates both objective and subjective criteria to align the discovered rules with user intent, and we propose an efficient training process. Extensive experiments on both real-world and synthetic datasets show that DCMiner can discover succinct, relevant, and diverse rules.