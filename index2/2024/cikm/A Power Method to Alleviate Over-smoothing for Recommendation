In recent years, graph convolution networks (GCNs) have been widely used in recommender systems due to high-order node information propagation and aggregation mechanisms. However, existing GCN-based recommender systems drop sharply in performance as the depth of the network increases. This phenomenon is called over-smoothing, which refers to the fact that the embeddings of all nodes become more similar and indistinguishable. Previous works have rarely explored over-smoothing from characteristics of the recommendation field. Specifically, we found experimentally that too many layers can lead to such large loss values that they are difficult to decrease. After theoretical analysis, we can effectively solve the problem of difficulty in decreasing the loss value by adding only a hyperparameter, called "power". This hyperparameter can effectively control the smoothness and alleviate the over-smoothing problem. Experiments on four public datasets demonstrate that this hyperparameter can effectively improve performance.