Achieving coordination while avoiding suboptimal equilibria poses a significant challenge in decentralized multi-agent reinforcement learning (MARL) systems operating under limited global information. Conventional decentralized approaches have struggled to effectively induce cooperative behaviors between agents. We propose a novel hierarchical framework that synergistically combines large language models (LLMs) and deep reinforcement learning to address this challenge. Our proposed learning-to-share (ILTS) method decomposes the global objective into a two-level hierarchy: high-level LLM policy determines how to share rewards between neighboring agents to shape emergent collaboration, while low-level policies optimize the induced localized objectives using Q value networks. A meta-learning sophisticated dynamic reward-sharing scheme via LLMs is developed to facilitate decentralized cooperation without explicit communication. Experimental results demonstrate that ILTS outperforms prior MARL algorithms across cooperative multi-agent tasks by inducing collaborative strategies and performing intention propagation from lightweight learned signals. This hierarchical framework avoids the need for hand-engineered rewards or explicit communication while promoting scalable learning of intricate symbiotic behaviors between agents perceiving only local observations.