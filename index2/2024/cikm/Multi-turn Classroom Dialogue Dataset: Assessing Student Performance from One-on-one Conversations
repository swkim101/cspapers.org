Accurately judging student on-going performance is crucial for adaptive teaching. In this work, we focus on the task of automatically predicting students' levels of mastery of math questions from teacher-student classroom dialogue data in online one-on-one classes. As a step toward this direction, we introduce the Multi-turn Classroom Dialogue (MCD) dataset as a benchmark testing the capabilities of machine learning models in classroom conversation understanding of student performance judgment. Our dataset contains aligned multi-turn spoken language of 5000+ unique samples of solving grade-8 math questions collected from 500+ hours' worth of online one-on-one tutoring classes. In our experiments, we assess various state-of-the-art models on the MCD dataset, highlighting the importance of understanding multi-turn dialogues and handling noisy ASR transcriptions. Our findings demonstrate the dataset's utility in advancing research on automated student performance assessment. To encourage reproducible research, we make our data publicly available at https://github.com/ai4ed/MCD.