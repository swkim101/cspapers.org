The adoption of Artificial Intelligence (AI) for hiring processes is often impeded by a scarcity of comprehensive employee data. We hypothesize that the inclusion of behavioral measures elicited from applicants can enhance the predictive accuracy of AI in hiring. We study this hypothesis in the context of microfinance credit officers. We focus on cognitive and behavioral measures commonly used in economics and psychology. We first investigate the efficiency of behavioral measures to explain the productivity of the existing employees. All employees of the firm take a 90-minute survey composed of incentivized and non-incentivized measures. We then use a sample of employees with a minimum tenure of 12 months to train an algorithm to predict which employees will receive a bonus. A random forest algorithm, relying solely on firm data such as age, education level, marital status, and other factors, accurately categorizes 65% of employees in an out-of-sample test. The inclusion of non-incentivized survey measures significantly improves the performance of the random forest algorithm by five percentage points. There are two concerns about using survey measures for hiring decisions. First, applicants might alter their responses to fit what they believe is the company's ideal employee profile. Second, our training sample includes only employees who worked for the company for over a year, which might bias the algorithm. As a first step to address the issue of on-the-job selection, we provide an out-of-sample prediction for employees with less than one year of tenure at the time of the survey. We examine their propensity to receive a bonus both at the 12-month mark of their employment and one year after the survey. Our findings indicate that those predicted by the algorithm to be productive are significantly more likely to receive a bonus at both time horizons, conditional and unconditional on being employed. To further address selection into the job and potential strategic responses, we conduct a field experiment in which all job applicants were evaluated by the algorithm. We divide applicants into two groups: HR and AI treatments. In the HR treatment, local and regional managers made hiring decisions as usual, but we also recorded the algorithm's recommendation. In the AI treatment, a decision by local and regional managers was overridden if it conflicted with the algorithm's recommendation. We compare the performance of employees hired under these two treatments at the 12-month mark of their employment and in February 2024. Overall, the performance of employees hired in the AI treatment is better than in the HR treatment. However, the differences are only marginally significant. We also examine the predictive power of the algorithm. Those who were recommended for hiring by the algorithm are significantly more likely to receive a bonus at the 12-month mark and in February 2024 compared to those who were not recommended. Based on these findings, we conclude that the algorithm is robust to both selection effects and strategic manipulation of responses in our context.