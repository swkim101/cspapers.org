Standard game-theoretic analysis yields highly incomplete descriptions of behavior in complex games of complete information. A key part of the reason is that standard models do not account for the role of complexity in shaping players' strategic behavior. We investigate the implications of complexity empirically and theoretically, focusing on the game of chess---a good setting for our study because it is a rare example of an extensive-form game that is played by experienced, motivated players, and for which we have vast amounts of data. First, using a large database of chess play, we quantify the significant margin by which machine-learning predictions trained on human play outperform a Zermelo's Theorem benchmark prediction of outcomes under perfectly rational play. Comparing this prediction to the actual empirical average outcome in a given position, we find that the game-theoretic model achieves 72% completeness. This demonstrates that while classical game-theoretic forces are predictive, they leave a significant amount of variation in outcomes unexplained. To assess how much of the remaining variation is predictable, we train a machine learning algorithm using additional features of the positions beyond the minimax value. This algorithm achieves 93% completeness, indicating that positional information beyond standard game theory is crucial for predicting behavior. We then focus on the role of complexity in explaining deviations from game-theoretic predictions. We define a heuristic measure of complexity based on whether a position's minimax value can be predicted using easily observable features that are commonly noticed by human players, or requires more complicated analysis. This measure of complexity predicts deviations from minimax predictions, with actual play closer to game-theoretic predictions in simple positions than in complex ones. An important cross-sectional finding also emerges: complex positions occur more frequently in games between higher-rated players. Specifically, complex positions are 73% more common in games between top-tercile players compared to bottom-tercile players. To understand the mechanisms behind this finding, we develop a tractable model capturing key strategic incentives for (and against) complex play in chess. The model features two players who take turns moving, with an evolving position state variable that encodes whether the position is simple or complex. Players can choose between simple moves that aim to produce simple states, or complex moves that aim to produce complex states for the opponent. However, players may unwittingly make blunders, defined as moves that reduce the value of the game for them. The key tradeoff in the model is that a more complex move creates a harder situation for the opponent when it succeeds, but increases the probability of one's own blunder. Players receive a stochastic signal called an "insight" before each move, informing them of how likely they are to avoid blundering if they attempt a complex move. We show theoretically that increasing player skill has theoretically ambiguous effects on the prevalence of complex positions because there are competing forces at play. More skilled players are more likely to execute complex moves without blundering, but more skilled opponents are more likely to successfully respond and thereby punish a complex move with a complex ensuing position. We structurally estimate a version of our model, recovering estimates of players' willingness to make complex moves at different rating levels. Surprisingly, we find that strong players are more cautious than weak players given the same information. We conclude that high-rated players spend more time in complex positions not because they are more willing to take risks, but because they maintain complex positions for many moves without blundering.