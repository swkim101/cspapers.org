Reliable detection of skilled forecasters is slow and resource-intensive. The gold-standard approach relies on proper scores and requires forecasters to answer dozens of questions, which may take months or years to resolve. To accelerate skill identification, we propose the Full Accuracy Score (FAS). FAS combines the strengths of objective ground-truth proper scores with the early availability of proper proxy scores that measure the distance between individual and consensus estimates (Witkowski et al. 2017). FAS treats the two inputs as complements, using ground-truth scores on resolved questions and proxy scores on questions with yet unknown answers. The proxy component acts as a running tally of individual performance and helps to complete the evolving picture of relative skill. We discuss three reasons to expect FAS outperformance over ground-truth-only scores: 1) Wisdom-of-crowds consensus estimates tend to outperform most individuals and thus provide a useful benchmark for performance, especially when sourced from elite crowds; 2) Consensus estimates update continuously, while individual estimates update at varying rates; 3) Proxy scores may reduce biases stemming from question resolution timing, i.e., systematic differences between early and late-resolving questions. We also describe three main design decision points in implementing full-accuracy scoring: A) Whether or not to employ a separate pre-selected crowd of forecasters; B) The choice of aggregation algorithm used to calculate consensus estimates that serve as the basis for proxy scores; C) The relative weights of ground-truth vs. proxy scores. We validate FAS using publicly available data from two U.S. government-sponsored forecasting competitions. First, the ACE competition featured forecasts from human crowds. We use data from Seasons 2, 3, 4, which included 300+ forecasters working independently in each season, to make probability predictions on 114, 147, and 136 forecasting questions, respectively, in the 2012--2015 period. Second, the Hybrid Forecasting Competition employed human crowds and machine-learning algorithms. We rely only on human crowd data on 188 geopolitical forecasting questions posed in 2017, featuring 1,360 individual forecasters. Our baseline version of FAS: A) does not employ a separate, pre-selected crowd; B) relies on aggregation algorithms optimized out-of-sample for aggregate accuracy (See Atanasov et al. 2017); and C) employs equal weighting between ground-truth and proxy scores. Our results across two forecasting tournaments show that this baseline-FAS version outperforms classic ground-truth scoring alone in predicting final performance scores of individual forecasters. More specifically, FAS produces higher correlations between intermediate and end-of-season accuracy scores than the ground-truth-only Brier scores, across both forecasting competitions. Employing a simple unweighted linear opinion pool aggregation algorithm in calculating consensus estimates reduces FAS advantage. Conversely, sourcing consensus estimates from a separate crowd of elite forecasters further boosts FAS performance, and becomes essential when the objective is to identify top talent. FAS adds more value in longitudinal, multi-shot settings, where forecasters can update their forecasts in response to new developments. Regarding the relative weights of ground-truth and proxy components, equal weighting is approximately optimal, and placing any small weight on proxy scores is better than placing none. Overall, FAS accelerates the detection of relatively skilled forecasters. It is especially useful in long-term forecasting competitions featuring forecast updating and varying question resolution dates. Access to an pre-selected forecasting crowd is especially helpful for spotting new elite forecasters. Full text available at: https://bit.ly/fas_2024_preprint