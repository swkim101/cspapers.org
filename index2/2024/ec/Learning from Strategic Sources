This paper studies learning from multiple informed agents where each agent has a small piece of information about the unknown state of the world in the form of a noisy signal and sends a message to the principal, who then makes a decision that is not constrained by predetermined rules. In contrast to the existing literature, we model the conflict of interest between the principal and the agents more generally and consider the case where the preferences of the principal and the agents are misaligned in some realized states. We show that if the conflict of interest between the principal and the agents is moderate, there is a discontinuity: when the number of agents is large enough, adding even a tiny probability of misaligned states leads to complete unraveling in which the agents ignore their signals, in contrast to the almost complete revealing that is predicted by the existing literature. Furthermore, we demonstrate that no matter how small the conflict of interest between the principal and the agents is, the information contained in each agent's message must vanish as the number of agents grows large. Finally, no matter how many agents there are, the total amount of information that is transmitted is limited, and the principal always fails to fully learn the unknown state.