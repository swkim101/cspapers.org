In this paper, we present a groundbreaking largescale system technology that leverages optimization on global, node, and device levels to achieve unprecedented scalability for tensor networks. Our techniques enable accommodating largescale tensor networks with up to tens of terabytes of memory, reaching up to 2304 GPUs with a peak computing power of 561 PFLOPS. Notably, we have achieved a time-to-solution of 14.22 seconds with an energy consumption of 2.39 kWh which achieved a fidelity of 0.002. Our most remarkable result is a time-to-solution of 17.18 seconds, with energy consumption of only 0.29 kWh which achieved a XEB of 0.002 after post-processing. The experiments conducted demonstrate that our research outperforms Googleâ€™s quantum processor Sycamore in both speed and energy efficiency, which recorded 600 seconds and 4.3 kWh, respectively. The code is available at https://github.com/DeepLinkorg/OpenTenNet.