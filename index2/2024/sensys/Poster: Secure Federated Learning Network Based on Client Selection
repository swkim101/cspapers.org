Federated learning (FL) enables the training of a global model using clients' local datasets, leveraging their computing resources for efficient machine learning while preserving user privacy. This paper explores FL in wireless networks, focusing on client selection and bandwidth allocation as key factors impacting latency, covert constraint and energy consumption. We propose the per-round energy drift plus cost (PEDPC) algorithm to address this optimization problem from an online perspective. The performance of the PEDPC algorithm is validated through simulations, evaluating latency and energy consumption under both IID and non-IID data distributions.