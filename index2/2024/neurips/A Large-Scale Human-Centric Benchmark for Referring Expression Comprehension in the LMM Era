Prior research in human-centric AI has primarily addressed single-modality tasks 1 like pedestrian detection, action recognition, and pose estimation. However, the 2 emergence of large multimodal models (LMMs) such as GPT-4V has redirected 3 attention towards integrating language with visual content. Referring expression 4 comprehension (REC) represents a prime example of this multimodal approach. 5 Current human-centric REC benchmarks, typically sourced from general datasets, 6 fall short in the LMM era due to their limitations, such as insufficient testing 7 samples, overly concise referring expressions, and limited vocabulary, making them 8 inadequate for evaluating the full capabilities of modern REC models. In response, 9 we present HC-RefLoCo (Human-Centric Referring Expression Comprehension 10 with Long Context), a benchmark that includes 13,452 images, 24,129 instances, 11 and 44,738 detailed annotations, encompassing a vocabulary of 18,681 words. 12 Each annotation, meticulously reviewed for accuracy, averages 93.2 words and 13 includes topics such as appearance, human-object interaction, location, action, 14 celebrity, and OCR. HC-RefLoCo provides a wider range of instance scales and 15 diverse evaluation protocols, encompassing accuracy with various IoU criteria, 16 scale-aware evaluation, and subject-specific assessments. Our experiments, which 17 assess 24 models, highlight HC-RefLoCoâ€™s potential to advance human-centric 18 AI by challenging contemporary REC models with comprehensive and varied 19 data. Our benchmark, along with the evaluation code, are available at https: 20