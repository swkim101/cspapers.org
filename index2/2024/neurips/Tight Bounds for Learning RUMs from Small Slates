A Random Utility Model (RUM) is a classical model of user behavior defined by a distribution over R n . A user, presented with a subset of { 1 , . . . , n } , will select the item of the subset with the highest utility, according to a utility vector drawn from the specified distribution. In practical settings, the subset is often of small size, as in the “ten blue links” of web search. In this paper, we consider a learning setting with complete information on user choices from subsets of size at most k . We show that k = Θ( √ n ) is both necessary and sufficient to predict the distribution of all user choices with an arbitrarily small, constant error. Based on the upper bound, we obtain new algorithms for approximate RUM learning and variations thereof. Furthermore, we employ our lower bound for approximate RUM learning to derive lower bounds to fractional extensions of the well-studied k -deck and trace reconstruction problems.