We study the interplay of local differential privacy (LDP) and robustness with respect to Huber corruption and possibly heavy-tailed rewards in the context of multi-armed bandits (MABs). We consider two different practical settings: LDP-then-Corruption (LTC) where each userâ€™s locally private response might be further corrupted during the data collection process, and Corruption-then-LDP (CTL) where each user may be adversary or corrupted such that each LDP mechanism will be applied to the corrupted data. To start with, we present the first tight characterization of high-probability mean estimation error under both LTC and CTL settings. Leveraging this result, we then present an almost tight (up to log factor) characterization of the minimax regret in online MABs and sub-optimality in offline MABs under both LTC and CTL settings. One key message behind all three problems is that LTC is a more difficult setting and leads to a worse performance guarantee compared to the CTL setting (in the minimax sense). This interesting interplay between privacy and corruption highlights that one needs to carefully design and analyze bandit algorithms when considering both privacy and corruption rather than treating them as linear combinations. Along the way, several results are of independent interest. Our proposed minimax optimal mean estimators can find application in many other scenarios. As a by-product, we also improved the state-of-the-art regret lower bound for locally private and heavy-tailed online MABs, i.e., without Huber corruption.