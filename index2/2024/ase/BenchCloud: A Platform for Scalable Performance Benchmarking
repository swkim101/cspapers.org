Performance evaluation is a crucial method for assessing automated-reasoning tools. Evaluating automated tools requires rigorous benchmarking to accurately measure resource consumption, including time and memory, which are essential for understanding the tools’ capabilities. BenchExec, a widely used benchmarking framework, reliably measures resource usage for tools executed locally on a single node. This paper describes BenchCloud, a solution for elastic and scalable job distribution across hundreds of nodes, enabling large-scale experiments on distributed and heterogeneous computing environments. BenchCloud seamlessly integrates with BenchExec, allowing BenchExec to delegate the actual execution to BenchCloud. The system has been employed in several prominent international competitions in automated reasoning, including SMT-COMP, SV-COMP, and Test-Comp, underscoring its importance in rigorous tool evaluation across various research domains. It helps to ensure both internal and external validity of the experimental results. This paper presents an overview of BenchCloud’s architecture and highlights its primary use cases in facilitating scalable benchmarking.Demonstration video: https://youtu.be/aBfQytqPm0URunning system: https://benchcloud.sosy-lab.org/CCS Concepts• General and reference → Cross-computing tools and techniques; • Computing methodologies → Distributed computing methodologies; • Computer-systems organization → Cloud computing; Client-server architectures; • Software and its engineering → Software verification and validation.