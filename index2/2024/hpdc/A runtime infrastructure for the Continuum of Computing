Devices at the Edge of the network are experiencing a considerable increase in computational resources. At the same time, connectivity becomes more pervasive. These phenomena jointly facilitate the emergence of a new computational model, increasingly referred to as the Continuum of Computing. This model aims at including Edge resources in Cloud-like (and Cloud-inclusive) resource pooling to accommodate computations that need reduced latency, increased privacy, and general mobility. This model has the potential to enhance the power and the reach of high-performance computing (HPC) applications, making them extend up to the Edge of the network. However, managing a pool of resources that span across both Cloud and Edge nodes poses new challenges. Moving data across the network generates latency and security issues, while national policies may outright limit data mobility. This suggests moving computation towards data instead of the usual opposite. Enabling migrating computation is one of key traits of the envisioned Continuum of Computing. The vast heterogeneity in the technological stacks and the lack of uniform standards, however, hinder the deployment of applications in the Continuum. The availability of a common runtime environment across all host nodes of the Continuum is an obvious way to circumvent those problems, reviving the write-once-run-anywhere promise in that context. The ability to move computations opportunistically after user-specific performance objectives is another key trait of the Continuum model, which also is a foundation to spatial computing, a context-aware and space-aware computing paradigm. How to effectively orchestrate migrating computations so that they can deliver value added to their users is still an open question. There is a general understanding that Cloud-native orchestrators perform poorly when shifting towards the Edge, due to exceedingly restrictive (Cloud-centric) assumptions underneath their orchestration model. The matter of efficient orchestration in the Continuum is paramount in the envisioned model. To showcase the feasibility and viability of a Continuum-worthy runtime infrastructure, we singled out two emerging technologies: Rust and WebAssembly. The Rust programming language's highlight is its statically-checked memory safety. WebAssembly's highlights are solid guarantees of isolation and a portable bytecode format for applications compiled for its Instruction Set Architecture (ISA). To this project, WebAssembly components written in Rust constitute the candidate building blocks for the Continuum infrastructure, centred on memory-safe and sand-boxed execution capsules. In addition to that, this project aims to develop and deploy Continuum-worthy orchestration capabilities that leverage seamless migration. The initial results of this project suggest that applications written in Rust and executing as WebAssembly components offer greater isolation and memory safety compared to containerized applications. Moreover, this novel approach might easily support live migration, consisting of the migration of an executing application into a different hosting node, preserving the state of the computation. Live migration prevents re-execution, and, at the time of writing, is largely unsupported in modern industrially applied containerized solutions. Supporting migrating computations might benefit multiple application scenarios. For example, the ability to migrate computation instead of freezing it during a low-energy phase may be of interest to energy-harvesting systems, Similarly, urgent science and Internet of Things (IoT) applications might want to move across Cloud and Edge nodes opportunistically, seeking optimal trade-offs between heavy and low-latency types of computation.