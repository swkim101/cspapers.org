Graphic Processing Units (GPUs) have become a key component of high-end computing infrastructures due to their massively parallel architecture, which delivers large floating-point operations per cycle rates. Many scientific workloads benefit from GPUs and, in particular, numerical methods solving linear systems of equations Ax = b typically run on GPUs. Among them, the Conjugate Gradient (CG) method, which targets linear systems with Symmetric and Positive Definite (SPD) matrices, runs on GPUs using its preconditioned form. However, state-of-the-art preconditioning techniques like the Factorized Sparse Approximate Inverse (FSAI) preconditioner ignore the benefits of data coalescence and locality on GPU architectures and leave substantial performance on the table. These approaches are exclusively based on numerical criteria. This paper proposes the GPU-aware Factorized Sparse Approximate Inverse (GFSAI) preconditioner. GFSAI generates sparse patterns that enhance the numerical benefits of FSAI and improve data locality and coalescence on GPU architectures. We evaluate GFSAI considering NVIDIA V100 and AMD MI50 GPUs and a set of 47 sparse matrices. GFSAI improves the state-of-the-art by reducing the average CG iteration count by 27.48% and 31.25% on NVIDIA and AMD, respectively, which leads to average decreases in execution time of 23.83% and 26.07% in these two GPU architectures.