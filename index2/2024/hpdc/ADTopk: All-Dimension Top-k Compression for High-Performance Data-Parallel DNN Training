Data-parallel deep neural networks (DNN) training systems deployed across nodes have been widely used in various domains, while the system performance is often bottlenecked by the communication overhead among workers for synchronizing gradients. Top-k sparsification compression is the de facto approach to alleviate the communication bottleneck, which truncates the gradient to its largest k elements before sending it to other nodes. However, we observe that the traditional Top-k still has performance issues: i) the gradient at each layer of a DNN is typically represented as a tensor of multiple dimensions, and the largest k elements selected by the traditional Top-k are centered in only some of all dimensions and hence the training may miss many dimensions (we call dimension missing), which leads to low convergence performance; ii) the traditional Top-k performs the selection by globally sorting the gradient elements in each layer (we call single global sorting), which leads to a low GPU core parallelism and hence a low training throughput. In this paper, we propose an all-dimension Top-k sparsification scheme, called ADTopk, which selects the largest k elements from all dimensions of the gradient tensor in each layer, meaning that each dimension must provide some elements, so as to avoid the dimension missing. Further, ADTopk enables each dimension to perform sorting locally within the elements of the dimension, and thus all dimensions can perform multiple local sortings independently and parallelly, instead of a single global sorting for the entire gradient tensor in each layer. On top of ADTopk, we further propose an interleaving compression scheme and an efficient threshold estimation algorithm so as to enhance the performance of ADTopk. We build a sparsification compression data-parallel DNN training framework and implement a compression library containing state-of-the-art sparsification algorithms. Experiments on a local cluster and Alibaba Cloud show that compared with state-of-the-art sparsification compression methods, ADTopk improves the training throughput by 12.28%-293.49% with approximately the same convergence accuracy as the non-compression baseline.