This paper introduces a lightweight framework for quantifying uncertainty in deep learning models deployed at the edge, addressing the challenge of making reliable predictions under computational constraints. By integrating conformal inference and evidential learning into a novel approach called Conformalized Evidential Quantile Regression (CEQR), it offers a practical solution for models to assess and communicate their confidence in predictions. The method efficiently distinguishes between aleatoric and epistemic uncertainties, ensuring statistical robustness and real-time applicability on resource-limited devices. This work paves the way for safer, more reliable AI applications in critical areas by enabling models to recognize when they don't know.