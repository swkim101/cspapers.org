Composed image retrieval is designed to more accurately retrieve target images that align with user intentions by using a combination of reference images and descriptive modification texts. However, existing methods primarily focus on designing complex feature fusion networks while neglecting the prevalent issues of noise and inconsistent sample quality in training data, leading to insufficient cross-modal semantic alignment and sample relevance modeling. To address this, we propose an innovative Tripartite Alignment Network (TAN) that introduces a momentum distillation mechanism, leveraging the historical knowledge of a teacher network as additional super-vision to guide the optimization of the student network. During the feature encoder fine-tuning stage, we design response-based knowledge distillation and feature-based knowledge distillation techniques, explicitly strengthening modal alignment through composed-target contrastive learning and implicitly promoting modal fusion via composed-target matching learning. In the combiner training stage, we incorporate a lightweight combiner network and employ a cross-entropy-based matching loss function, encouraging high matching scores for relevant image-text pairs and low scores for irrelevant pairs. Extensive experiments on the FashionIQ and Shoes datasets demonstrate that TAN exhibits superior performance compared to existing state-of-the-art methods, with notable improvements in R@10 of +14.03% and +13.09%, respectively. These results affirm the effectiveness of momentum distillation in multimodal learning. Access the source code at https://github.com/Maserhe/TAN.