Traditional machine learning relies on centralized data, which poses risks to privacy and strains networks when collecting data from multiple centers. Federated Learning, introduced to address these challenges, trains models on various clients, transfers them to a central server to aggregate their knowledge in a global model, and shares the global model with clients to transfer knowledge among them. In real-world problems, data are not distributed independently and identically among different data sources. The non-IID nature of data makes the learning process more challenging, especially when the problem is non-convex. In such cases, the global model may become trapped in local optima. In this paper, we have introduced an approach that employs techniques from metaheuristic optimization algorithms to help the learning process escape from local optima. Relying on the concepts of exploration and exploitation, the optimization process is directed toward promising areas. Experiments conducted on several non-IID datasets show that our algorithm outperforms others in a wide variety of cases.