Recently, various contrastive learning techniques have been developed to categorize time series data and have exhibited promising performance for real-world applications. A general paradigm is to utilize appropriate data augmentation methods and construct feasible positive samples such that the encoder can yield robust and discriminative representations by mapping similar data points closer together in the feature space while pushing dissimilar data points farther apart. Despite its efficacy, the fine-grained relative similarity (e.g., rank) information of positive samples is not fully exploited, especially when labeled samples are limited. To this end, we present Rank Supervised Contrastive Learning (RankSCL) to perform time series classification. Different from conventional contrastive learning frameworks, RankSCL augments raw data in a targeted manner in the embedding space and selects more informative positive and negative pairs for the targeted sample. Moreover, a novel rank loss is developed to assign higher weights to more confident positive pairs and lower weights to less confident positive pairs, enabling the encoder to extract the same class's fine-grained information and produce a clear boundary among different classes. Thoroughly empirical studies on 128 UCR and 30 UEA datasets demonstrate that the proposed RankSCL can achieve state-of-the-art performance compared to existing baseline methods. Code is available at: https://github.com/UConn-DSIS/Rank-Supervised-Contrastive-Learning-for-Time-Series-Classification.