In the evolving landscape of sequential recommendation systems, the application of Large Language Models (LLMs) is increasingly prominent. However, current attempts typically utilize general-purpose LLMs, which present a mismatch in capability and a large semantic gap relative to the specialized needs of recommendation tasks. To tackle these issues, we introduce RecCoder, an innovative model that reformulates sequential recommendation as a code completion task. This approach leverages the superior reasoning capability of code LLMs as a backbone, aligning well with the requirements of recommendation systems. To bridge the semantic gap, RecCoder creates extra tokens for each item and employs item content to initialize token embeddings. Furthermore, we have developed a suite of Semantic Adaptation Fine-tuning tasks, tailored to enhance the model's acquisition of both content and collaborative semantic information, thus aligning the model's intrinsic capabilities with the unique demands of recommendation tasks. Through extensive testing on three public datasets, RecCoder has shown remarkable improvements over existing models in terms of recommendation accuracy and efficiency. This success highlights the substantial yet previously underexplored potential of code LLMs in improving recommendation accuracy and efficiency, suggesting a promising new direction for future research in this area. The implementation code is accessible at https://github.com/AllminerLab/Code-for-RecCoder-master.