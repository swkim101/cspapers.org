Generating explanations for deep neural networks (DNNs) can make them more trustworthy in real-world applications. For a text classification task, existing methods visualize the contributions of words or word interactions layer by layer in a traversal manner, to assist users in understanding the decision-making of models. However, all these methods only focus on the explanation performance while ignoring inefficiencies in explaining due to the traversal manner. This means that the explanation is not available to users in a timely manner, so users may no longer use it due to the big time cost. To overcome this problem, we propose HETSG, an interaction-based method for explaining text classification models quickly and faithfully, by a simple and effective two-step building strategy. Such a strategy captures the important interaction by first determining the important position and then confirming the direction of interaction, without iterating over all word interactions. We also provide a novel metric to accurately evaluate the performance of each interpretation method. The proposed method is compared with baseline methods (baselines) on six text classification datasets to explain three natural language processing (NLP) models. Experimental results show that our method outperforms all baselines with higher efficiency and it is also competitive in performance.