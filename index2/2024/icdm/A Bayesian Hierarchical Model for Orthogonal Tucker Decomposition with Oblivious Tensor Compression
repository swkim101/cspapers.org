Low-rank representations such as the Tucker decomposition underlie many frequentist methods for tensor analysis. Bayesian analogues, in contrast, have received less attention. Notably missing in the literature is a Bayesian Tucker decomposition with orthogonal factor matrices-a standard interpretability restriction in frequentist settings. We propose a Bayesian hierarchical model for the orthogonal Tucker decomposition, which we implement via conditionally conjugate Gibbs sampler. To reduce the complexity of tensor operations in MCMC estimation, we incorporate a mechanism that uses Johnson-Lindenstrauss embeddings to compress data. Our theoretical analysis bounds change in the full-conditional posterior distributions of tensor components due to compression (with respect to Hellinger distance). We further establish posterior consistency for the decomposition's factor matrices in settings where these parameters are shared across tensor observations. Empirical results show that, for large tensor datasets, moderate compression can significantly reduce draw time (by about 50%) with only a moderate increase (up to about 15%) in median reconstruction error. Compression in the proposed model additionally enables analyses of tensor data that are too large to be held wholly in memory, thus making large-scale analyses tractable on even moderate computing resources.