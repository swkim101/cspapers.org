—Recent advancements have enabled human-robot collaboration through physical assistance and verbal guidance. However, limitations persist in coordinating robots’ physical motions and speech in response to real-time changes in human behavior during collaborative contact tasks. We first derive principles from analyzing physical therapists’ movements and speech during patient exercises. These principles are translated into control objectives to: 1) guide users through trajectories, 2) control motion and speech pace to align completion times with varying user cooperation, and 3) dynamically paraphrase speech along the trajectory. We then propose a Language-Grounded Motion Controller that synchronizes motion and speech, modulating both based on user cooperation. Experiments with 12 users show the Language-Grounded Motion Controller successfully aligns motion and speech compared to baselines. This provides a framework for fluent human-robot collaboration. The implementation of the controller is available as open source code to enable extension, verification, and reuse of the methods 1 .