Object detection in radar imagery with neural networks shows great potential for improving autonomous driving. However, obtaining annotated datasets from real radar images, crucial for training these networks, is challenging, especially in scenarios with long-range detection and ad-verse weather and lighting conditions where radar performance excels. To address this challenge, we present Rad-SimReal, an innovative physical radar simulation capable of generating synthetic radar images with accompanying annotations for various radar types and environmental conditions, all without the need for real data collection. Re-markably, our findings demonstrate that training object de-tection models on RadSimReal data and subsequently eval-uating them on real-world data produce performance lev-els comparable to models trained and tested on real data from the same dataset, and even achieves better performance when testing across different real datasets. Rad-SimReal offers advantages over other physical radar simulations that it does not necessitate knowledge of the radar design details, which are often not disclosed by radar sup-pliers, and has faster run-time. This innovative tool has the potential to advance the development of computer vision al-gorithms for radar-based autonomous driving applications. Our GitHub: https://yuvalhg.github.io/RadSimReal.