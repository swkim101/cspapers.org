There is a steady increase in the size of data stored and processed as part of data science applications, leading to bottlenecks and ineﬃciencies at various layers of the stack. One way of reducing such bottlenecks and increasing energy eﬃciency is by tailoring the underlying distributed storage solution to the application domain, using resources more eﬃciently. We explore this idea in the context of a popular column-oriented storage format used in big data workloads, namely Apache Parquet. Our prototype uses an FPGA-based storage node that offers high bandwidth data deduplication and a companion software library that exposes an API for Parquet ﬁle access. This way the storage node remains general purpose and could be shared by applications from diﬀerent domains, while, at the same time, beneﬁting from deduplication well suited to Apache Parquet ﬁles and from selective reads of columns in the ﬁle. In this demonstration we show, on the one hand, that by relying on the FPGA’s dataﬂow processing model, it is possible to implement in-line deduplication without increasing latencies signiﬁcantly or reducing throughput. On the other hand, we highlight the beneﬁts of implementing the application-speciﬁc aspects in a software library instead of FPGA circuits and how this enables, for instance, regular data science frameworks running in Python to access the data on the storage node and to oﬄoad ﬁltering operations.