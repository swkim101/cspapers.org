In this paper, we propose a cost-aware cascading bandits model, a new variant of multi-armed bandits with cascading feedback, by considering the random cost of pulling arms. In each step, the learning agent chooses an <italic>ordered</italic> list of items and examines them sequentially, until certain stopping condition is satisfied. Our objective is then to maximize the expected <italic>net reward</italic> in each step, i.e., the reward obtained in each step minus the total cost incurred in examining the items, by deciding the ordered list of items, as well as when to stop examination. We first consider the setting where the instantaneous cost of pulling an arm is unknown to the learner until it has been pulled. We study both the offline and online settings, depending on whether the state and cost statistics of the items are known beforehand. For the offline setting, we show that the Unit Cost Ranking with Threshold 1 (UCR-T1) policy is optimal. For the online setting, we propose a Cost-aware Cascading Upper Confidence Bound (CC-UCB) algorithm, and show that the cumulative regret scales in <inline-formula><tex-math notation="LaTeX">$O(\log T)$</tex-math></inline-formula>. We also provide a lower bound for all <inline-formula><tex-math notation="LaTeX">$\alpha$</tex-math></inline-formula>-consistent policies, which scales in <inline-formula><tex-math notation="LaTeX">$\Omega (\log T)$</tex-math></inline-formula> and matches our upper bound. We then investigate the setting where the instantaneous cost of pulling each arm is available to the learner for its decision-making, and show that a slight modification of the CC-UCB algorithm, termed as CC-UCB2, is order-optimal. The performances of the algorithms are evaluated with both synthetic and real-world data.