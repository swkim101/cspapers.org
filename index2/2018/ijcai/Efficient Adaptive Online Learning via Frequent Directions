By employing time-varying proximal functions, adaptive subgradient methods (ADAGRAD) have improved the regret bound and been widely used in online learning and optimization. However, ADAGRAD with full matrix proximal functions (ADA-FULL) cannot handle large-scale problems due to the impractical <inline-formula><tex-math notation="LaTeX">$O(d^3)$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="wan-ieq1-3096880.gif"/></alternatives></inline-formula> time and <inline-formula><tex-math notation="LaTeX">$O(d^2)$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="wan-ieq2-3096880.gif"/></alternatives></inline-formula> space complexities, though it has better performance when gradients are correlated. In this paper, we propose two efficient variants of ADA-FULL via a matrix sketching technique called frequent directions (FD). The first variant named as ADA-FD directly utilizes FD to maintain and manipulate low-rank matrices, which reduces the space and time complexities to <inline-formula><tex-math notation="LaTeX">$O(\tau d)$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="wan-ieq3-3096880.gif"/></alternatives></inline-formula> and <inline-formula><tex-math notation="LaTeX">$O(\tau ^2d)$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="wan-ieq4-3096880.gif"/></alternatives></inline-formula> respectively, where <inline-formula><tex-math notation="LaTeX">$d$</tex-math><alternatives><mml:math><mml:mi>d</mml:mi></mml:math><inline-graphic xlink:href="wan-ieq5-3096880.gif"/></alternatives></inline-formula> is the dimensionality and <inline-formula><tex-math notation="LaTeX">$\tau \ll d$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>τ</mml:mi><mml:mo>≪</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="wan-ieq6-3096880.gif"/></alternatives></inline-formula> is the sketching size. The second variant named as ADA-FFD further adopts a doubling trick to accelerate FD used in ADA-FD, which reduces the average time complexity to <inline-formula><tex-math notation="LaTeX">$O(\tau d)$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>O</mml:mi><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="wan-ieq7-3096880.gif"/></alternatives></inline-formula> while only doubles the space complexity of ADA-FD. Theoretical analysis reveals that the regret of ADA-FD and ADA-FFD is close to that of ADA-FULL as long as the outer product matrix of gradients is approximately low-rank. Experimental results demonstrate the efficiency and effectiveness of our algorithms.