Humans sense most of the environment through their eyes. Detecting people's activities and context from their visual behavior using mobile eye trackers is well studied. Typically, these systems require some kind of calibration prior to usage, unsuitable for the end user. Cameras that actively record the user's field of view became a valid alternative for lifelogging applications. But such approaches do not include gaze information and may cause privacy challenges. In this work we propose an in-depth analysis of the first long-term corneal imaging dataset (i.e., the reflection of the environment on the human eye). All data was manually labeled with information about the attended objects. This was compared to an automatic approach using a state-of-the art neural network.