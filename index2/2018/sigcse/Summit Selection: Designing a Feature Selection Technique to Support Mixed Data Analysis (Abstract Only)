Since data size is continuously increasing, analyzing large-scale data is considered as one of the major research challenges in computational data analysis. Although researchers have proposed numerous approaches, most of them still suffer from analyzing the data efficiently. To overcome the limitation, identifying the optimal number of features is critical for analyzing the data. In this paper, we introduce a newly designed feature selection technique, called Summit Selection, which boosts model performances by determining optimal features in noisy mixed data. First, testing all features is conducted to determine an initial base feature that satisfies a pre-defined criterion for maintaining the highest performance score. Then, a continuous evaluation is managed to build a model by successively adding or removing features based solely on the performance score tested with chosen computational models. To show the effectiveness of our proposed technique, a performance evaluation study was conducted to determine fraudulent activities in the UCSD Data Mining Contest 2009 Dataset. We compared our proposed technique with different feature extraction techniques such as PCA, ANOVA test, and Mutual Information (MI). Specifically, multiple machine learning techniques such as Decision Tree, Random Forest, and k-Nearest Neighbor (KNN) are tested with the feature extraction techniques to determine performance differences. As results, we found that our proposed technique showed about 8.78% performance improvement in detecting fraudulent activities. Since our technique can be extended to a cloud computing environment, we also performed a scalability testing with a known distributed cloud computing model (i.e., Apache Spark).