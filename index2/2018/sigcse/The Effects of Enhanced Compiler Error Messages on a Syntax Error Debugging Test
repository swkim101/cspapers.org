There is an active strand of research in the literature exploring the effects of Enhanced Compiler Error Messages on student programming behavior, however many results seem conflicting. This is compounded by the fact that directly comparing these results is difficult as these studies utilize different metrics, and what metrics are best suited to measure the effects of enhanced compiler error messages is not known. Common to most studies to-date is that the metrics employed measure how many errors students produce, and/or rectify while writing programs. This study takes a different approach by measuring how many pre-existing syntax errors are rectified by students while debugging programs. Specifically, we measured the effect of enhanced compiler error messages in an empirical control/intervention experiment where students were given the task of removing syntax errors from non-compiling source code they did not write. We find a significant positive effect on the overall number of errors rectified, as well as the number of certain specific error types, but no significant effect on the number of non-compiling submissions or student scores. These results (in different ways) support the findings of several recent studies and suggest that their results may not be as conflicting as they seem. This is evidence that enhanced error messages may be effective, but also that the signal of these effects are relatively weak, indicating that how and what is measured when attempting to observe these effects is important.