We consider the setting of prediction with expert advice; a learner makes predictions by aggregating those of a group of experts. Under this setting, and with the right choice of loss function and "mixing" algorithm, it is possible for the learner to achieve constant regret regardless of the number of prediction rounds. For example, constant regret can be achieved with \emph{mixable} losses using the \emph{Aggregating Algorithm} (AA). The \emph{Generalized Aggregating Algorithm} (GAA) is a name for a family of algorithms parameterized by convex functions on simplices (entropies), which reduce to the AA when using the \emph{Shannon entropy}. For a given entropy $\Phi$, losses for which constant regret is possible using the GAA are called $\Phi$-mixable. Which losses are $\Phi$-mixable was previously left as an open question. We fully characterize $\Phi$-mixability, and answer other open questions posed by \cite{Reid2015}. We also elaborate on the tight link between the GAA and the \emph{mirror descent algorithm} which minimizes the weighted loss of experts.