Graph neural networks (GNNs) have been successful in learning representations from graphs. Many popular GNNs follow the pattern of aggregate-transform : they aggregate the neighbors’ attributes and then transform the results of aggregation with a learnable function. Analyses of these GNNs explain which pairs of non-identical graphs have different representations. However, we still lack an understanding of how similar these representations will be. We adopt kernel distance and propose transform-sum-cat as an alternative to aggregate-transform to reﬂect the continuous similarity between the node neighborhoods in the neighborhood aggregation. The idea leads to a simple and efﬁcient graph similarity, which we name Weisfeiler–Leman similarity (WLS). In contrast to existing graph kernels, WLS is easy to implement with common deep learning frameworks. In graph classiﬁcation experiments, transform-sum-cat signiﬁcantly outperforms other neighborhood aggregation methods from popular GNN models. We also develop a simple and fast GNN model based on transform-sum-cat, which obtains, in comparison with widely used GNN models, (1) a higher accuracy in node classiﬁcation, (2) a lower absolute error in graph regression, and (3) greater stability in adversarial training of graph generation.