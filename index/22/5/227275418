Fine-tuning pre-trained deep networks on a small dataset is an important component in the deep learning pipeline. A critical problem in ﬁne-tuning is how to avoid over-ﬁtting when data are limited. Existing efforts work from two aspects: (1) impose regularization on parameters or features; (2) transfer prior knowledge to ﬁne-tuning by reusing pre-trained parameters. In this paper, we take an alternative approach by refactoring the widely used Batch Normalization (BN) module to mitigate over-ﬁtting. We propose a two-branch design with one branch normalized by mini-batch statistics and the other branch normalized by moving statistics. During training, two branches are stochastically selected to avoid over-depending on some sample statistics, resulting in a strong regularization effect, which we interpret as “architecture regularization.” The resulting method is dubbed stochastic normalization ( StochNorm ). With the two-branch architecture, it naturally incorporates pre-trained moving statistics in BN layers during ﬁne-tuning, exploiting more prior knowledge of pre-trained networks. Extensive empirical experiments show that StochNorm is a powerful tool to avoid over-ﬁtting in ﬁne-tuning with small datasets. Besides, StochNorm is readily pluggable in modern CNN backbones. It is complementary to other ﬁne-tuning methods and can work together to achieve stronger regularization effect.