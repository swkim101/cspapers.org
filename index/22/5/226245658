We propose a human-robot interface based on potentials recorded through surface Electroencephalographic sensors, aiming to decode human visual attention into motion in three-dimensional space. Low-frequency components are extracted and processed in real time, and subspace system identification methods are used to derive the optimal, in mean squared sense, linear dynamics generating the position vectors. This results in a human-robot interface that can be used directly in robot teleoperation or as part of a shared-control robotic manipulation scheme, feels natural to the user, and is appropriate for upper extremity amputees, since it requires no limb movement. We validate our methodology by teleoperating a redundant, anthropomorphic robotic arm in real time. The system's performance outruns similar EMG-based systems, and shows low long-term model drift, indicating no need for frequent model re-training.