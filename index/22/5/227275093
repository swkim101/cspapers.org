Operator-valued kernels have shown promise in supervised learning problems with functional inputs and functional outputs. The crucial (and possibly restrictive) assumption of positive deﬁniteness of operator-valued kernels has been instrumental in developing efﬁcient algorithms. In this work, we consider operator-valued kernels which might not be necessarily positive deﬁnite. To tackle the indeﬁniteness of operator-valued kernels, we harness the machinery of Reproducing Kernel Krein Spaces (RKKS) of function-valued functions. A representer theorem is illustrated which yields a suitable loss stabilization problem for supervised learning with function-valued inputs and outputs. Analysis of generalization properties of the proposed framework is given. An iterative Operator based Minimum Residual (OpMINRES) algorithm is proposed for solving the loss stabilization problem. Experiments with indeﬁnite operator-valued kernels on synthetic and real data sets demonstrate the utility of the proposed approach.