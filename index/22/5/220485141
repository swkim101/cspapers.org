We demonstrate a pattern-based MIDI music generation

system with a generation strategy based on

Wasserstein autoencoders and a novel variant of pianoroll

descriptions of patterns which employs separate

channels for note velocities and note durations

and can be fed into classic DCGAN-style convolutional

architectures. We trained the system on two

new datasets (in the acid-jazz and high-pop genres)

composed by musicians in our team with music

generation in mind. Our demonstration shows

that moving smoothly in the latent space allows us

to generate meaningful sequences of four-bars patterns.