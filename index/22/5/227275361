We present a series of new PAC-Bayes learning guarantees for randomized algorithms with sample-dependent priors. Our most general bounds make no assumption on the priors and are given in terms of certain covering numbers under the inﬁnite-Rényi divergence and the (cid:96) 1 distance. We show how to use these general bounds to derive leaning bounds in the setting where the sample-dependent priors obey an inﬁnite-Rényi divergence or (cid:96) 1 -distance sensitivity condition. We also provide a ﬂexible framework for computing PAC-Bayes bounds, under certain stability assumptions on the sample-dependent priors, and show how to use this framework to give more reﬁned bounds when the priors satisfy an inﬁnite-Rényi divergence sensitivity condition.