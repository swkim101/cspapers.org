There are rich synchronized audio and visual events in our daily videos. Inside the events, audio scenes are associated with the corresponding visual objects, meanwhile, sounding objects can indicate and help to separate their individual sounds in the audio track. Based on this observation, in this paper, we propose a cyclic co-learning (CCoL) paradigm that can jointly learn sounding object visual grounding and visually indicated audio separation in a unified framework. Concretely, we locate sounding objects from all objects in videos with a visual grounding network and then learn an audio-visual sound separation network to separate sounds from individual sounding objects. Moreover, in the framework, sounding object visual grounding labels can be adaptively adjusted based on sound separation results to simultaneously improve grounding and separation models, which builds a co-learning cycle for the two tasks. Extensive experiments show that the proposed framework achieves state-of-the-art performance on both sounding object visual grounding and visually indicated sound separation tasks and they can benefit from each other with our cyclic co-learning.