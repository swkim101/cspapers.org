Biomedical entity linking aims to map biomedical mentions,
such as diseases and drugs, to standard entities in a given
knowledge base. The specific challenge in this context is
that the same biomedical entity can have a wide range of
names, including synonyms, morphological variations, and
names with different word orderings. Recently, BERT-based
methods have advanced the state-of-the-art by allowing for
rich representations of word sequences. However, they often have hundreds of millions of parameters and require
heavy computing resources, which limits their applications
in resource-limited scenarios. Here, we propose a lightweight
neural method for biomedical entity linking, which needs just
a fraction of the parameters of a BERT model and much less
computing resources. Our method uses a simple alignment
layer with attention mechanisms to capture the variations
between mention and entity names. Yet, we show that our
model is competitive with previous work on standard evaluation benchmarks.