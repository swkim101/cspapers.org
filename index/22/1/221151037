Given a mixture between two populations of coins, "positive" coins that each have---unknown and potentially different---bias $\geq\frac{1}{2}+\Delta$ and "negative" coins with bias $\leq\frac{1}{2}-\Delta$, we consider the task of estimating the fraction $\rho$ of positive coins to within additive error $\epsilon$. We achieve a tight upper and lower bound of $\Theta(\frac{\rho}{\epsilon^2\Delta^2})$ samples for constant probability of success. In particular, our lower bound applies to all fully-adaptive algorithms. A crucial component of our lower bound proof is a decomposition lemma (Corollary 16) showing how to assemble partially-adaptive bounds into a fully-adaptive bound, which may be of independent interest: though we invoke it for the special case of Bernoulli random variables (coins), it applies to general distributions. We present simulation results to demonstrate the practical efficacy of our approach for realistic problem parameters for crowdsourcing applications, focusing on the "rare events" regime where $\rho=O(\frac{1}{\log 1/\epsilon})$. The fine-grained adaptive flavor of both our algorithm and lower bound contrasts with much previous work in distributional testing and learning.