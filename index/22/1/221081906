A recent trend in algorithm design consists of augmenting classic data structures with machine learning models, which are better suited to reveal and exploit patterns and trends in the input data so to achieve outstanding practical improvements in space occupancy and time efﬁciency. This is especially known in the context of indexing data structures where, despite few attempts in evaluating their asymptotic efﬁciency, theoretical re-sults are yet missing in showing that learned indexes are provably better than classic indexes, such as B + -trees and their variants. In this paper, we present the ﬁrst mathematically-grounded answer to this open problem. We obtain this result by discovering and exploiting a link between the original problem and a mean exit time problem over a proper stochastic process which, we show, is related to the space and time occupancy of those learned indexes. Our general result is then specialised to ﬁve well-known distributions: Uniform, Lognormal, Pareto, Exponential, and Gamma; and it is corroborated in precision and robustness by a large set of experiments.