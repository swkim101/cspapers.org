Deep learning requires volume, quality, and variety of training data. In neural question answering, a trade-off between quality and volume comes from the need to either manually curate or construct realistic question answering data, which is costly, or else augmenting, weakly labeling or generating training data from smaller datasets, leading to low variety and sometimes low quality. What can be done to make the best of this necessary trade-off? What can be understood from the endeavor to seek such solutions?