Machine learning (ML) has been run and applied by premising a series of presuppositions, which contributes both the great success of AI and the bottleneck of further development of ML. These presuppositions include (i) the independence assumption of loss function on dataset (Hypothesis I); (ii) the large capacity assumption on hypothesis space including solution (Hypothesis II); (iii) the completeness assumption of training data with high quality (Hypothesis III); and (iv) the Euclidean assumption on analysis framework and methodology (Hypothesis IV). We report, in this presentation, the effort and advances made by my group on how to break through these presuppositions of ML and drive ML development. For Hypothesis I, we introduce the noise modeling principle to adaptively design the loss function of ML, according to the distribution of data samples, which provides then a general way to robustlize any ML implementation. For Hypothesis II, we propose the model driven deep learning approach to define the smallest hypothesis space of deep neural networks (DNN), which yields not only the very efficient deep learning, but also a novel way of DNN design, interpretation and connection with the traditional optimization based approach. For Hypothesis III, we develop the axiomatic curriculum learning framework to learn the patterns from an incomplete dataset step by step and from easy to difficult, which then provides feasible ways to tackle very complex incomplete data sets. Finally, For Hypothesis IV, we introduce Banach space geometry in general, and XU-Roach theorem in particular, as a possibly useful tool to conduct non-Euclidean analysis of ML problems. In each case, we present the idea, principles, application examples and literatures.