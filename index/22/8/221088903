We consider the problem of allocating a ﬁxed budget of samples to a ﬁnite set of discrete distributions to learn them uniformly well (minimizing the maximum error) in terms of four common distance measures: (cid:96) 22 , (cid:96) 1 , f -divergence, and separation distance. To present a uniﬁed treatment of these distances, we ﬁrst propose a general optimistic tracking algorithm and analyze its sample allocation performance w.r.t. an oracle. We then instantiate this algorithm for the four distance measures and derive bounds on their regret. We also show that the allocation performance of the proposed algorithm cannot, in general, be improved, by deriving lower-bounds on the expected deviation from the oracle allocation for any adaptive scheme. We verify our theoretical ﬁndings through some experiments. Finally, we show that the techniques developed in the paper can be easily extended to learn some classes of continuous distributions as well as to the related setting of minimizing the average error (rather than the maximum error) in learning a set of distributions.