Newborn infants are naturally attracted to human faces, a crucial source of information for social interaction. In robotics, acquisition of such information is crucial and social robots should also learn to exhibit such social skill. Deep learning algorithms are valid candidates to address the problem of face localisation. However, a major drawback of these methods is the large amount of data and human supervision needed in the training procedure. In this work, we propose a cognitive architecture to address autonomous learning from raw sensory signals without supervision. We demonstrate the success of our cognitive framework for the task of face localisation. The proposed cognitive architecture builds on existing work and uses audiovisual attention and a proactive stereo vision mechanism to autonomously direct a robotâ€™s attentive focus towards human faces. The gathered information is used to incrementally generate a dataset that can be used to train a state-of-the-art deep network. The learning system imitates the typical learning process of infants and enhances the learning generalization process by leveraging on the interaction experience with people. The integration of HRI with machine learning, inspired by early development in humans, constitutes an innovative approach for improving autonomous learning in robots.