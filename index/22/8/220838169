As specialized hardware accelerators such as GPUs become increasingly popular, developers are looking for ways to target these platforms with high-level APIs. One promising approach is kernel libraries such as PyTorch or cuML, which provide interfaces that mirror CPU-only counterparts such as NumPy or Scikit-Learn. Unfortunately, these libraries are hard to develop and to adopt incrementally: they only support a subset of their CPU equivalents, only work with datasets that ﬁt in device memory, and require developers to reason about data placement and transfers manually. To address these shortcomings, we present a new approach called ofﬂoad annotations (OAs) that enables heterogeneous GPU computing in existing workloads with few or no code modiﬁ-cations. An annotator annotates the types and functions in a CPU library with equivalent kernel library functions and provides an ofﬂoading API to specify how the inputs and outputs of the function can be partitioned into chunks that ﬁt in device memory and transferred between devices. A runtime then maps existing CPU functions to equivalent GPU kernels and schedules execution, data transfers and paging. In data science workloads using CPU libraries such as NumPy and Pandas, OAs enable speedups of up to 1200 ⇥ and a median speedup of 6 . 3 ⇥ by transparently ofﬂoading functions to a GPU us-ing existing kernel libraries. In many cases, OAs match the performance of handwritten heterogeneous implementations. Finally, OAs can automatically page data in these workloads to scale to datasets larger than GPU memory, which would need to be done manually with most current GPU libraries.