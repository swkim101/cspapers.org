Recently, there have been signiﬁcant interests in studying the so-called “double-descent” of the generalization error of linear regression models under the overpa-rameterized and overﬁtting regime, with the hope that such analysis may provide the ﬁrst step towards understanding why overparameterized deep neural networks (DNN) still generalize well. However, to date most of these studies focused on the min (cid:96) 2 -norm solution that overﬁts the data. In contrast, in this paper we study the overﬁtting solution that minimizes the (cid:96) 1 -norm, which is known as Basis Pursuit (BP) in the compressed sensing literature. Under a sparse true linear regression model with p i.i.d. Gaussian features, we show that for a large range of p up to a limit that grows exponentially with the number of samples n , with high probability the model error of BP is upper bounded by a value that decreases with p . To the best of our knowledge, this is the ﬁrst analytical result in the literature establishing the double-descent of overﬁtting BP for ﬁnite n and p . Further, our results reveal signiﬁcant differences between the double-descent of BP and min (cid:96) 2 -norm solutions. Speciﬁcally, the double-descent upper-bound of BP is independent of the signal strength, and for high SNR and sparse models the descent-ﬂoor of BP can be much lower and wider than that of min (cid:96) 2 -norm solutions.