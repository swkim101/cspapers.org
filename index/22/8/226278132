This paper investigates a hybrid solution which combines deep reinforcement learning (RL) and classical trajectory planning for the "following in front" application. Here, an autonomous robot aims to stay ahead of a person as the person freely walks around. Following in front is a challenging problem as the userâ€™s intended trajectory is unknown and needs to be estimated, explicitly or implicitly, by the robot. In addition, the robot needs to find a feasible way to safely navigate ahead of human trajectory. Our deep RL module makes decisions at a high level by implicitly estimates the human trajectory and produces short-term navigational goals to guide the robot. These goals are used by a trajectory planner, which is responsible for low-level execution, to smoothly navigate the robot to the short-term goals, and eventually in front of the user. We employ curriculum learning in the deep RL module to efficiently achieve a high return. Our system outperforms the state-of-the-art in following ahead and is more reliable compared to end-to-end alternatives in both the simulation and real world experiments. In contrast to a pure deep RL approach, we demonstrate zero-shot transfer of the trained policy from simulation to the real world.