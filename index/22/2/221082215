A well-documented weakness of neural networks is the fact that they suffer from catastrophic for­ getting when trained on data provided by a non-stationary distribution. Recent work in the ﬁeld of continual learning attempts to understand and overcome this issue. Unfortunately, the majority of relevant work embraces the implicit assump­ tion that the distribution of observed data is per­ fectly balanced, despite the fact that, in the real world, humans and animals learn from observa­ tions that are temporally correlated and severely imbalanced. Motivated by this remark, we aim to evaluate memory population methods that are used in online continual learning, when dealing with highly imbalanced and temporally correlated streams of data. More importantly, we introduce a new memory population approach, which we call class-balancing reservoir sampling (CBRS). We demonstrate that CBRS outperforms the state-of­ the-art memory population algorithms in a consid­ erably challenging learning setting, over a range of different datasets, and for multiple architec­ tures.