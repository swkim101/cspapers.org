Recent advances in neural architecture search inspire many channel number search algorithms (CNS) for convolutional neural networks. To improve searching ef-ﬁciency, parameter sharing is widely applied, which reuses parameters among different channel conﬁgurations. Nevertheless, it is unclear how parameter sharing affects the searching process. In this paper, we aim at providing a better understanding and exploitation of parameter sharing for CNS. Speciﬁcally, we propose afﬁne parameter sharing (APS) as a general formulation to unify and quantitatively analyze existing channel search algorithms. It is found that with parameter sharing, weight updates of one architecture can simultaneously beneﬁt other candidates. However, it also results in less conﬁdence in choosing good architectures. We thus propose a new strategy of parameter sharing towards a better balance between training efﬁciency and architecture discrimination. Extensive analysis and experiments demonstrate the superiority of the proposed strategy in channel conﬁguration against many state-of-the-art counterparts on benchmark datasets.