Hypernetworks are architectures in which a learned {\em meta-network} produces the weights of a task-specific {\em primary network}. They have been demonstrated to obtain state of the art results on several notable meta-learning benchmarks, such as shape reconstruction. In this work, we study randomly initialized hypernetworks in the over-parameterized regime. We show that the function computed by an initialized hypernetwork exhibits Gaussian Process (GP) and Neural Tangent Kernel (NTK) behaviours -- but only when both the meta and primary networks are infinitely wide. In this dually infinite regime, we identify functional priors of these architectures by deriving the corresponding GP and NTK kernels, the latter of which we refer to as the {\em hyperkernel}. We provide empirical evidence to support our claims, and demonstrate the power of the hyperkernels on various meta learning tasks. As part of this study, we make a mathematical contribution by deriving tight bounds on high order Taylor expansion terms in standard fully connected ReLU networks.