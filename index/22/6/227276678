Boosting is a widely used learning technique in machine learning for solving classiﬁcation problems. In boosting, one predicts the label of an example using an ensemble of weak classiﬁers. While boosting has shown tremendous success on many classiﬁcation problems involving tabular data, it performs poorly on complex classiﬁcation tasks involving low-level features such as image classiﬁcation tasks. This drawback stems from the fact that boosting builds an additive model of weak classiﬁers, each of which has very little predictive power. Often, the resulting additive models are not powerful enough to approximate the complex decision boundaries of real-world classiﬁcation problems. In this work, we present a general framework for boosting where, similar to traditional boosting, we aim to boost the performance of a weak learner and transform it into a strong learner. However, unlike traditional boosting, our framework allows for more complex forms of aggregation of weak learners. In this work, we speciﬁcally focus on one form of aggregation - function composition . We show that many popular greedy algorithms for learning deep neural networks (DNNs) can be derived from our framework using function compositions for aggregation. Moreover, we identify the drawbacks of these greedy algorithms and propose new algorithms that ﬁx these issues. Using thorough empirical evaluation, we show that our learning algorithms have superior performance over traditional additive boosting algorithms, as well as existing greedy learning techniques for DNNs. An important feature of our algorithms is that they come with strong theoretical guarantees.