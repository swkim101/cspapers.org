Fine-tuning pre-trained deep neural networks (DNNs) to a target dataset, also known as transfer learning, is widely used in computer vision and NLP. Because task-speciﬁc layers mainly contain categorical information and categories vary with datasets, practitioners only partially transfer pre-trained models by discarding task-speciﬁc layers and ﬁne-tuning bottom layers. However, it is a reckless loss to simply discard task-speciﬁc parameters which take up as many as 20% of the total parameters in pre-trained models. To fully transfer pre-trained models, we propose a two-step framework named Co-Tuning : (i) learn the relationship between source categories and target categories from the pre-trained model with calibrated predictions; (ii) target labels (one-hot labels), as well as source labels (probabilistic labels) translated by the category relationship, collaboratively supervise the ﬁne-tuning process. A simple instantiation of the framework shows strong empirical results in four visual classiﬁcation tasks and one NLP classiﬁcation task, bringing up to 20% relative improvement. While state-of-the-art ﬁne-tuning techniques mainly focus on how to impose regularization when data are not abundant, Co-Tuning works not only in medium-scale datasets (100 samples per class) but also in large-scale datasets (1000 samples per class) where regularization-based methods bring no gains over the vanilla ﬁne-tuning. Co-Tuning relies on a typically valid assumption that the pre-trained dataset is diverse enough, implying its broad application areas.