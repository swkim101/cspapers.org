Safe and proactive planning in robotic systems generally requires accurate predictions of the environment. Prior work on environment prediction applied video frame prediction techniques to birdâ€™s-eye view environment representations, such as occupancy grids. ConvLSTM-based frameworks used previously often result in significant blurring of the predictions, loss of static environment structure, and vanishing of moving objects, thus hindering their applicability for use in safety-critical applications. In this work, we propose two extensions to the ConvLSTM architecture to address these issues. We present the Temporal Attention Augmented ConvLSTM (TAAConvLSTM) and Self-Attention Augmented ConvLSTM (SAAConvLSTM) frameworks for spatiotemporal occupancy grid prediction, and demonstrate improved performance over baseline architectures on the real-world KITTI and Waymo datasets. We provide our implementation at https: //github.com/sisl/AttentionAugmentedConvLSTM.