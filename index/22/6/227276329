In this paper, we revisit the problem of distribution-independently learning half-spaces under Massart noise with rate ⌘ . Recent work [DGT19] resolved a long-standing problem in this model of efﬁciently learning to error ⌘ + ✏ for any ✏ > 0 , by giving an improper learner that partitions space into poly( d, 1 / ✏ ) regions. Here we give a much simpler algorithm and settle a number of outstanding open questions: (1) We give the ﬁrst proper learner for Massart halfspaces that achieves ⌘ + ✏ . (2) Based on (1), we develop a blackbox knowledge distillation procedure to convert an arbitrarily complex classiﬁer to an equally good proper classiﬁer. (3) By leveraging a simple but overlooked connection to evolvability , we show any SQ algorithm requires super-polynomially many queries to achieve OPT + ✏ . We then zoom out to study generalized linear models and give an efﬁcient algorithm for learning under a challenging new corruption model generalizing Massart noise. Lastly, we empirically evaluate our algorithm for Massart halfspaces and ﬁnd it exhibits some intriguing fairness properties.