Modern storage systems leverage ﬂash caching to boost I/O performance, and enhancing the space efﬁciency and endurance of ﬂash caching remains a critical yet challenging issue in the face of ever-growing data-intensive workloads. Deduplication and compression are promising data reduction techniques for storage and I/O savings via the removal of duplicate content, yet they also incur substantial memory overhead for index management. We propose AustereCache, a new ﬂash caching design that aims for memory-efﬁcient indexing, while preserving the data reduction beneﬁts of deduplication and compression. AustereCache emphasizes austere cache management and proposes different core techniques for efﬁcient data organization and cache replacement, so as to eliminate as much indexing metadata as possible and make lightweight in-memory index structures viable. Trace-driven experiments show that our AustereCache prototype saves 69.9-97.0% of memory usage compared to the state-of-the-art ﬂash caching design that supports deduplication and compression, while maintaining comparable read hit ratios and write reduction ratios and achieving high I/O throughput.