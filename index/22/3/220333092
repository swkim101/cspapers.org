Establishing the signiﬁcance of experimental re-sults in reinforcement learning (RL) is difﬁcult. This is compounded by the additional complexity of meta-and multi-task RL (MTRL), a rapidly-growing research area which lacks well-deﬁned baselines. We analyze several design decisions each author must make when they implement a meta-RL or MTRL algorithm, and use over 500 experiments to show that these seemingly-small details can create statistically-signiﬁcant variations in a single algorithm’s performance that exceed the reported performance differences be-tween algorithms themselves. Informed by this analysis, we precisely deﬁne several important hyperparameters, design decisions, and evaluation metrics for meta-RL and MTRL methods, so that we can compare these methods reproducibly. We then provide multi-seed benchmark results for seven of the most popular meta-RL and MTRL algorithms on the most challenging benchmarks currently available. Finally, we share with the community an open source package of these al-gorithm reference implementations, which use our consistent deﬁnitions, achieve state-of-the-art-performance, and seek to follow the original works introducing these algorithms as closely as possible.