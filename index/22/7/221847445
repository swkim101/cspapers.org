In this paper, we propose a real-time human-robot interface (HRI) system, where Electrooculography (EOG) and Electromyography (EMG) signals were decoded to perform reach-to-grasp movements. For that, five different eye movements (up, down, left, right and rest) were classified in real-time and translated into commands to steer an industrial robot (UR-10) to one of the four approximate target directions. Thereafter, EMG signals were decoded to perform the grasping task using an attached gripper to the UR-10 robot arm. The proposed system was tested offline on three different healthy subjects, and mean validation accuracy of 93.62% and 99.50% were obtained across the three subjects for EOG and EMG decoding, respectively. Furthermore, the system was successfully tested in real-time with one subject, and mean online accuracy of 91.66% and 100% were achieved for EOG and EMG decoding, respectively. Our results obtained by combining real-time decoding of EOG and EMG signals for robot control show overall the potential of this approach to develop powerful and less complex HRI systems. Overall, this work provides a proof-of-concept for successful real-time control of robot arms using EMG and EOG signals, paving the way for the development of more dexterous and human-controlled assistive devices.