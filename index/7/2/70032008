Graphics Processing Unit (GPU) architectures are becoming an inevitable part of every computing system [1] because of their ability to provide orders of magnitude faster execution. They have become the default choice for accelerating innovations in various fields [2]–[10] such as high-performance computing (HPC), artificial intelligence, deep learning, and virtual/augmented reality. Traditionally, GPUs have relied on bandwidth to achieve high throughput [11]–[14]. The sources of bandwidth have been local/shared caches, scratchpad, and memory. Additionally, high bandwidth interconnect is required to support the data flow between caches/memory and cores. In this work, we focus on dynamically identifying and exploiting an additional source of bandwidth in GPUs, which we call as remote-core bandwidth. As also observed by previous works [15]–[17], the source of this additional bandwidth stems from inter-core locality. Specifically, the data required by one of the GPU cores (i.e., L1 misses) can also be found in the local L1 caches of other remote GPU cores. We find that this additional source of bandwidth leads to significant improvement in performance, however, can only be leveraged if an efficient inter-core communication is enabled. There are several challenges towards designing an efficient inter-core communication, which have not been addressed by prior works. In particular, this work systematically addresses the following research questions: 1) How to determine which data can also be found in remote cores, 2) How to determine which cores have the data of the requester core, and 3) How to get the data as soon as possible without congesting the interconnect bandwidth? Figure 1 shows the architectural diagram of our proposal. To the best of our knowledge, this is the first work that addresses these questions in a systematic manner. Specifically, this work makes the following contributions: • We observe a bi-modal distribution of inter-locality across different instructions – some instructions use data that is shared across cores and some do not. We leverage the observation and use the program counter (PC) to predict which L1 misses are likely to be found in the L1 caches of other cores ( A ). • We develop a low-overhead mechanism that can locally predict which cores are likely to have the shared data ( B ). It is based on our key observation that the data required by a core is generally shared across only a few cores, which can be detected via sampling a limited number of core replies. • We develop a novel two-level probing mechanism that searches the identified cores in parallel while considering the network bandwidth consumption ( C ). This ensures that the searching overhead does not hamper inter-core locality as well as performance. • Our combined schemes take advantage of the unutilized Selected