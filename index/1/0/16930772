Introduction This paper presents an initial exploration of the use of random forests in a relational context. Random forest induction is a bagging method that builds decision trees by selecting in each node the ”best” feature, not out of all available features, but out of a random subset of features (which may be different for each node). The motivation for this work is based on two observations. On the one hand, random forests have been shown to work well when many features are available. Hence, their use seems especially interesting for relational data mining, for which it is typical that there is a large number of features, many of which are expensive to compute. On the other hand, using random forests allows an extension of the feature space by including aggregate functions, possibly refined with selection conditions on the set to be aggregated. This combination of aggregation and selection in relation learning is not a trivial task, because the feature set grows quickly, and because the search space is less well-behaved due to the non-monotonicity problem. However, the use of random forests tackles both problems.