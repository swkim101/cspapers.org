We introduce coactive learning as a distributed learning approach to data mining in networked and distributed databases. The coactive learning algorithms act on independent data sets and cooperate by communicating training information, which is used to guide the algorithms' hypothesis construction. The exchanged training information is limited to examples and responses to examples. It is shown that coactive learning can offer a solution to learning on very large data sets by allowing multiple coacting algorithms to learn in parallel on subsets of the data, even if the subsets are distributed over a network. Coactive learning supports the construction of global concept descriptions even when the individual learning algorithms are provided with training sets having biased class distributions. Finally, the capabilities of coactive learning are demonstrated on artificial noisy domains, and on real world domain data with sparse class representation and unknown attribute values.