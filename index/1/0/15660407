We propose a Bayesian-network classifier with inverse-tree structure (BNCIT) for joint classification and variable selection. The problem domain of voxelwise magnetic-resonance image analysis often involves millions of variables but only dozens of samples. Judicious variable selection may render classification tractable, avoid over-fitting, and improve classifier performance. BNCIT embeds the variable-selection process within the classifier-training process, which makes this algorithm scalable. BNCIT is based on a Bayesian-network model with inverse-tree structure, i.e., the class variable C is a leaf node, and predictive variables are parents of C; thus, the classifier-training process returns a parent set for C, which is a subset of the Markov blanket of C. BNCIT uses voxels in the parent set, and voxels that are probabilistically equivalent to them, as variables for classification of new image data. Since the data set has a limited number of samples, we use the jackknife method to determine whether the classifier generated by BNCIT is a statistical artifact. In order to enhance stability and improve classification accuracy, we model the state of the probabilistically equivalent voxels with a latent variable. We employ an efficient method for determining states of hidden variables, thus reducing dramatically the computational cost of model generation. Experimental results confirm the accuracy and efficiency of BNCIT.