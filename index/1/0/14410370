Identifying the relevant functional degrees of freedom is a key prerequisite for the proper handling of everyday objects. Recognizing and exploiting these degrees of freedom in the context of non-rigid objects poses challenges that are significantly different from the rigid case. As a major generic subtask, we consider the identification and exploitation of boundary components during clothes manipulation, combining RGBD vision with uni- and bi-manual handling through a robot. Specifically, we present a novel graph-based approach to detecting boundary components by extracting closed contours from depth images. Based on that, we suggest a planner minimizing a heuristic energy function for an optimal grasp pose of a robot hand around the boundary of a garment. We demonstrate the effectiveness of the approach in interactive perception and regrasping experiments with a dual arm and two attached anthropomorphic hands. Furthermore, we show how to make use of these capabilities to implement a basic skill for a coat-check robot: hanging up a knit cap on a hat-stand.