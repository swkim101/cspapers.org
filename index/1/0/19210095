Active search is a learning paradigm with the goal of actively identifying as many members of a given class as possible. Many real-world problems can be cast as an active search, including drug discovery, fraud detection, and product recommendation. Previous work has derived the Bayesian optimal policy for the problem, which is unfortunately intractable due to exponential complexity. In practice, myopic approximations are used instead, only looking a small number (e.g., 1–3) of steps ahead in the decision process. We propose a novel active search policy that always considers the entire remaining budget and is thus nonmyopic, yet remains efﬁcient. Our approach automatically and dynamically balances exploration and exploitation in a manner consistent with the budget, without relying on a tradeoff parameter. We also develop a bounding technique to achieve greater efﬁciency when using certain natural probability models. Experimental results show superior performance of our method over myopic approximations to the optimal policy.