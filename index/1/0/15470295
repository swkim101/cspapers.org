Cognitive modeling with neural networks unrealistically ignores the role of knowledge in learning by starting from random weights. It is likely that effective use of knowledge by neural networks could significantly speed learning. A new algorithm, knowledge-based cascadecorrelation (KBCC), finds and adapts its relevant knowledge in new learning. Comparison to multi-task learning (MTL) reveals that KBCC uses its knowledge more effectively to learn faster. 1. Existing Knowledge and New Learning Neural networks typically learn de novo without the benefit of existing knowledge. However, when people learn, they routinely use their knowledge (Pazzani, 1991; Wisniewski, 1995). Such use of prior knowledge in learning is likely responsible for the ease and speed with which people learn, and for interference with new learning. The technical reason that neural networks fail to use knowledge is that they begin learning from initially random connection weights. This implements a tabula rasa view of each distinct learning task that very few cognitive psychologists would accept. In this paper, we compare two algorithms (KBCC and MTL) for their ability to use knowledge to speed learning. KBCC is an extension of cascade-correlation (CC), a generative learning algorithm often used in the simulation of cognitive development (Buckingham & Shultz, in press; Mareschal & Shultz, 1999; Oshima-Takane, Takane, & Shultz, 1999; Shultz, 1998, 1999; Shultz, Mareschal, & Schmidt, 1994; Sirois & Shultz, 1998). CC constructs its own network topology by recruiting new hidden units into the network as needed in order to reduce error (Fahlman & Lebiere, 1990). KBCC recruits previously learned networks in addition to the single hidden units recruited by CC (Shultz & Rivest, 2000). Following terminology in the literatures on analogy and transfer, we refer to existing networks as potential source knowledge and to a current learning task as a target. Previously learned source networks compete with each other and with single hidden units to be recruited into the target network. Caruana (1993, 1995, 1997) developed multi-task learning (MTL) in which he trained a network on several tasks taken from the same domain in parallel, with a single output unit for each task. Such networks typically learned a common hidden-unit representation, which produced better generalization than learning the same single tasks one at a time (STL). MTL can be adapted to sequential learning by having a source network generate responses to input values from a new task. These responses can then serve as target output values in parallel MTL of the new task. This paper reports a comparison of KBCC and MTL on the same sequential learning task. The goals are to determine whether each algorithm can use source knowledge to speed learning and to study of the effects of knowledge relevance on learning speed. 2. Previous Work on Knowledge and Learning Other previous neural network research on knowledge and learning has included studies of transfer (Pratt, 1993), sequential learning (Silver & Mercer, 1996), lifelong learning (Thrun & Mitchell, 1993), knowledge insertion (Shavlik, 1994), modularity (Jordan & Jacobs, 1994), and input re-coding (Clark & Thornton, 1997). Pratt (1993) pioneered the study of knowledge and learning in neural networks with a technique called discriminability-based transfer (DBT). DBT uses the weights from a previously trained network to initialize a new network. This seems the most straightforward idea for using knowledge in new neural learning. Because it did not actually work very well, Pratt re-scaled the previous network's hyper-planes so that useful ones had large weights and less useful ones had small weights. Silver and Mercer (1996) extended MTL to sequential learning in a method called task rehearsal (TRM). Here, old tasks are pseudo-rehearsed during new learning. In pseudo-rehearsal, a network generates its own target vectors, using its current weights, rather than merely accepting them from the environment (Robins, 1995). In a variation of MTL, separate learning rates for each task are used to control the impact of each source task, ensuring that the most related tasks have the most impact on learning. Thrun and Mitchell (1993) proposed a technique they called lifelong learning, in which a network meta-learns the slope of the desired function at each training example. This is the derivative of the function at an example output with respect to the input attribute vector. Then, in new learning, a meta-network predicts slopes and estimates its accuracy for each new training example. This technique would seem to trade not so much on knowledge representations as on search knowledge. Clark and Thornton (1997) emphasized the importance of networks being able to re-code their input in the learning of difficult, so-called Type-2 problems. Type-1 problems are those that can be solved by sampling the originally coded input data. Type-2 problems need re-coding in order to use Type-1 knowledge. Re-coding may require incremental learning, modularity, and representational redescription (Karmiloff-Smith, 1992), but no specific algorithm was proposed. Shavlik (1994) devised an algorithm for creating knowledge-based artificial neural networks (KBANN). KBANN converts a set of symbolic rules embodying a domain theory of a problem into a feed-forward neural network with the final rule conclusions as output units and intermediate rule conclusions as hidden units. Connection weights and biases are initialized to mimic the conjunctive and disjunctive structures of the original rules. Such knowledge-initialized networks are then trained with examples to refine the network's knowledge. Training with KBANN is typically faster than using standard networks with random weights and leads to better generalization. Following training, the modified rules can be extracted from the network. Jordon and Jacobs (1994) devised the Hierarchical Mixture of Experts (HME) model to decompose problems into separate network modules. Distinct network modules become expert on subtasks, and cooperate on an overall solution via gating networks that learn to weight the modular expert contributions for particular parts of a problem. HME was found to learn the dynamics of a fourdegree-of-freedom robot arm faster than a multi-layer back-propagation network did. Next we describe in some detail the two learning algorithms featured here: KBCC and MTL. 3. Knowledge-based Cascade-correlation KBCC learns like CC, except that KBCC treats its previously learned networks as if they were single candidate hidden units. Both single units and existing networks are candidates for recruitment into a target network. A candidate unit and a candidate network each define a function that can be differentiated, which is essential for weight adjustment by gradient descent. The connection scheme for a sample KBCC network is shown in Figure 1. This connection scheme is the same as in CC except that a recruited network can have multiple weighted sums as inputs and can have multiple outputs. In contrast, a single recruited unit, whether in CC or KBCC, has only one weighted sum as input and one output.