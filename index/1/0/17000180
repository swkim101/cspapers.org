The algorithm applies a novel, randomized construction of hash functions. These functions can be evaluated in constant time, constructed on sublinear space in sublinear expected time, and have many features of random functions. The algorithm further makes use of a new Monte Carlo type sequential dictionary with worst case constant time per instruction, which was recently developed by the authors. Applications of the distributed dictionary are e. g. two improvements of PRAM-simulations: A PRAM with p processors can be simulated by a complete network with p processors with expected delay log p / log log p (before: logp), and on one with p / l o g p processors with optimal expected delay logp (before: p l -e processors, delay pC). 1. I N T R O D U C T I O N A dictionary is a dynamic data structure that supports the operations Insert, Delete, and Lookup. It is one of the most fundamental data structures. In this paper we describe and analyze a distributed dictionary, implemented on a complete, synchronized network of p processors, i.e., randomized RAMs. Its construction is based on hashing: The keys z to be inserted, deleted or looked up are assigned to the processors via a hash function, and processed within these processors using a dynamic sequential hashing strategy from [DM90]. *Suppor ted in pa r t by DFG Grants ME 872/1-3 and W E 1066/2-1 Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. Some important properties of our distributed dictionary are as follows: 1.) Each processor is fed (by its virtual user) with a new instruction whenever it asks for one. 2.) n > pl+~ arbitrary instructions, n /p per processor, can be executed in expected time O(n/p) , where c > 0 can be chosen arbitrarily small. 3.) Each processor needs space O(n/p) , if n elements are in the dictionary. 4.) Every lookup has expected constant response time. 5.) If p lookups are executed concurrently, then the expected time needed to answer all of them is O(log p~ log log p). 6.) If k >__ logp lookups from each processor are executed, then all of them are answered after expected time o(k). The efficiency bounds in 2 . ) 4 . ) are asymptotically optimal. 5.) and 6.) can also be looked upon to be optimal, if one assumes that the keys are mapped to the distribution hash processors using a hash function, the " " " " function" D H , and if one does not allow any (or only constant) redundancy (as is necessary to achieve the optimal space bound). In this case, the statements in 5.) and 6.) provide the best possible bounds, because they correspond to a D H that is a truly random function. In fact, one of the contributions of this paper is an efficient randomized construction of functions with many features of random functions and constant evaluation time. More precisely, we present a class 7~ of hash functions h : U ~ { 1 , . . . , p } (U is a finite universe) such that given a set S C U of n keys, there is 7~(S) C ~ , [7~(S)1/17~1 close to 1, such that a randomly chosen h E 7~(S) fulfills for each i E { 1 , . . . , p } : Prob( Ihl ( i ) f3 5'1 > u) is roughly as small as if h were a random function. Further, the construction of a random h E 7~ needs time and space o(p) only. (If we allow time and space O(p 1+~) for arbitrary 6 > 0, we even get that a random h E R(S) is a random function on S.) Similar types of random functions can be found in [DM90]. Â© 1990 ACM 089791-361-2/90/0005/0117 $1.50 117 Thus, when analyzing our data structure, we can argue that , from the point of view of a single processor, the distribution hash function behaves like a random function. In order to handle duplicates of keys many extra considerations for the algori thm and its analysis are necessary (see Chapter 5). S e q u e n t i a l d i c t i o n a r i e s . The first opt imal sequential dictionary (based on hashing) was introduced in [DKM88] inspired by the perfect hashing scheme from [FKS84]. It guarantees that n instructions can be executed in t ime O(L.n) with probabil i ty 1 2 z , for all L. I t turns out that this is not sufficient for the purposes of our distributed dictionary, because we need that even the slowest one of p concurrently working dictionaries is fast. A substantial improvement is shown in [DM90]. There a Monte Carlo type dictionary is presented that needs worst case constant t ime per instruction, and has failure probabil i ty O(n-k), when n keys are currently stored in the dictionary, k can be made an arbitrarily large constant. This dictionary uses hash functions similar to the class R described above. R e l a t e d w o r k a n d a p p l i c a t i o n s o f t h e m a i n re su i t . An opt imal parallel dictionary, implemented on a PRAM, was presented in [DM89]. As PRAMs have a shared memory, one can essentially use a single dict ionary and does not have to distribute keys over the processors. Closely related to our results are the shared memory emulations in [Upf84], [KU86], [Ran87], [MV84], [KRS88]. The first three papers and parts of the fourth apply polynomials of degree ~ logp as distribution hash functions. Thus, these techniques cannot be used for our purposes, because they yield "best case" t ime bounds f~(logp) for the response t ime of lookups, and ~(n logp/p) for executing n instructions. In [Sie89] universal classes of hash functions are introduced that can be evaluated in constant t ime and yet perform much better than polynomials of constant degree (see Chapter 3). Possibly, those functions could also be used for the needs of the present paper. However, in [Sie89] the size of the universe is restricted to pk, where the evaluation t ime of the functions grows exponentially with k. In our approach we can handle a universe of arbi trary size. The algorithm for the distributed dictionary can be modified so as to obtain a method for simulating T steps of a (priority) PRAM with p processors and a shared memory of arbi t rary size on a complete network of p processors in O(T. logp/loglogp) steps. The space needed for representing the memory can be made as small as O(T/p +pC) per processor. (Compare property 5.) above.) Until now, the best known t imebound for a simulation of this kind was O(Tlogp), see [MV84]. In [KRS88], it is shown how to simulate a PRAM with p processors on a complete network of p l -~ processors with opt imal delay O(p~). In tha t paper, polynomials of constant degree as distribution hash functions are used. Combining these techniques with our improved sequential dictionary yields O(n/p) expected t ime for executing n instructions. But as p instructions have to be executed concurrently to obtain the above t ime bound, the expected response t ime for lookups is O(p*). It is easy to conclude from our result that a simulation as [KRS88] can already be achieved when the network has only p/logp processors. The delay becomes O(logp). (Compare property 6.) f rom above.) The paper is organized as follows. In Chapter 2 we define the computat ional model and distributed dictionaries in more detail. In Chapter 3 we describe and analyze our new hash functions and quote several useful results on hashing and sequential dictionaries. The remaining chapters contain the description and analysis of the distributed dictionary. 2. D E F I N I T I O N S In this paper we consider synchronized, complete networks of some number p of processors P 1 , . . . , Pp with usual sequential capabilities of randomized random access machines. We assume the uniform cost criterion. For communication purposes, each processor has a communicat ion window. Each processor can read from or write into any such window. We allow concurrent read and assume the P R I O R I T Y write conflict resolution rule for the windows: If several processors want to write simultaneously to the same window, then that one with the smallest index wins. Note that all our results also hold for the A R B I T R A R Y write conflict resolution rule. A distributed dictionary is a da ta structure implemented on a network that supports lookups, insertions and deletions of data identified by keys x from a given finite universe U. Each processor is connected to a user that feeds it with a new instruction whenever it asks for