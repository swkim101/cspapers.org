We compare different word embeddings from a standard window based skipgram model, a skipgram model trained using dependency context features and a novel skipgram variant that utilizes additional information from dependency graphs. We explore the effectiveness of the different types of word embeddings for word similarity and sentence classiﬁcation tasks. We consider three common sentence classiﬁcation tasks: question type classiﬁcation on the TREC dataset, binary sentiment classiﬁcation on Stanford’s Sentiment Treebank and semantic relation classiﬁcation on the SemEval 2010 dataset. For each task we use three different classiﬁcation methods: a Support Vector Machine, a Convolutional Neural Network and a Long Short Term Memory Network. Our experiments show that dependency based embeddings outperform standard window based embeddings in most of the settings, while using dependency context embeddings as additional features improves performance in all tasks regardless of the classiﬁcation method. Ourembeddings and code are available at https: