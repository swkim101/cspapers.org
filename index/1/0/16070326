Currently, our interactions with devices are constrained, as we need to program/configure devices, primarily through some artificial interface, instead of interacting through a dialog agent. This limitation in human-device interactions is a major obstacle to the integration of devices (e.g. PDA, GPS) in our daily activities. Considering the numerous advantages of using a dialog agent for communicating with devices, we are building a cognitive dialog agent that facilitates this communication. The agent has semantic knowledge about concepts and available commands of a device, and this knowledge is represented using an ontology. The agent gathers the necessary information, for execution of commands, by engaging users in a meaningful communication. Finally, the commands are sent to the device for execution. Consequently, the agent relieves users from the burden of acquiring the knowledge, which is necessary for operating the device. In order for the agent to process the human utterance, and make sense of the concepts that the human relates to, it requires a mapping between the concepts that the human is using, and the concepts that are understandable for the device. In this paper, we provide a novel algorithm to determine these matching concepts. The algorithm exploits various similarity metrics to disambiguate and match concepts. We formally define these metrics, and analyze the time complexity of computing the metrics, algorithmically. In addition, the effectiveness and scalability of the metrics are evaluated empirically, on real-world ontologies, through different experiments. In essence, the dialog agent interweaves the individual threads of meaning between human and device, using the ontology mapping algorithm, and prevents miscommunication between them.