Learning reliable embeddings requires large data and is not trivial to be adapted to specific tasks due to certain constraints. For queries, the lack of sufficient context can prohibit learning quality representations and thus effective query understanding. We propose to project embeddings from a source space learned with natural language into a target space on queries. The projection function is learned via an overlap vocabulary set shared by both source and target spaces. Experimental results show that both linear and nonlinear embedding projections can help query intent classification and query slot tagging, even when the amount of data used for learning the projection is limited.