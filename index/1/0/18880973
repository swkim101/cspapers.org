Modeling in constraint programming is a hard task that requires considerable expertise. Automated model reformulation aims at assisting a naive user in modeling constraint problems. In this context, formal specification languages have been devised to express constraint problems in a manner similar to natural yet rigorous specifications that use a mixture of natural language and discrete mathematics. Yet, a gap remains between such languages and the natural language in which humans informally describe problems. This work aims to alleviate this issue by proposing a method for detecting constraints in natural language problem descriptions using a structured-output classifier. To evaluate the method, we develop an original annotated corpus which gathers 110 problem descriptions from several resources. Our results show significant accuracy with respect to metrics used in cognate tasks.