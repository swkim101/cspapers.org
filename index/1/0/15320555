As the field of Computational Vision matures, more efforts are devoted to vision systems that are active and need to interact with their environment in real time. A prerequisite for integrating Vision and Action is the development of a set of representations of the visual system's space-time, where space includes the system itself. Thus we are faced with the problem of studying the nature of appropriate representations and also with the computational task of acquiring them in a robust manner and in real time. Both of these problems are addressed in this paper from a computational point of view. In particular, we study representations needed by active visual systems in order to understand their self-motion and the structure of their environment. 
 
The representations are of less metric information content than the ones traditionally used, including depth, surface normals, curvature and 3-D metric values for the parameters of rigid motion, etc.; but they are rich enough to allow the system to perform a large number of actions. These representations, indexed in image coordinates, are the direction of translation and the direction of rotation for the case of motion and a monotonic function of the depth value in the case of shape description. Their advantage comes from the fact that they can be computed from minimal and well-defined input (flow or disparity values along image gradients), as opposed to the traditional ones which require image correspondence or the utilization of assumptions about the environment.