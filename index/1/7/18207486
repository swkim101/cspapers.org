We define a model-theoretic reasoning formalism that is naturally implemented on symmetric neural networks (like Hopfield networks or Boltzman machines). We show that every symmetric neural network, can be seen as performing a search for a satisfying model of some knowledge that is wired into the network's weights. Several equivalent languages are then shown to describe the knowledge embedded in these networks. Among them is propositional calculus extended by augmenting propositional assumptions with penalties. The extended calculus is useful in expressing default knowledge, preference between arguments, and reliability of assumptions in an inconsistent knowledge base. Every symmetric network can be described by this language and any sentence in the language is translatable into such a network, A sound and complete proof procedure supplements the model-theoretic definition and gives an intuitive understanding of the nonmonotonic behavior of the reasoning mechanism. Finally, we sketch a connectionist inference engine that implements this reasoning paradigm.