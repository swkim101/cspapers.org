Lbr is a lazy semi-naive Bayesian classiier learning technique, designed to alleviate the attribute interdependence problem of naive Bayesian classiication. To classify a test example , it creates a conjunctive rule that selects a most appropriate subset of training examples and induces a local naive Bayesian classiier using this subset. Lbr can signii-cantly improve the performance of the naive Bayesian classiier. A bias and variance analysis of Lbr reveals that it signiicantly reduces the bias of naive Bayesian classiication at a cost of a slight increase in variance. It is interesting to compare this lazy technique with boosting and bagging, two well-known state-of-the-art non-lazy learning techniques. Empirical comparison of Lbr with boosting decision trees on discrete valued data shows that Lbr has, on average, signiicantly lower variance and higher bias. As a result of the interaction of these eeects, the average prediction error of Lbr over a range of learning tasks is at a level directly comparable to boosting. Lbr provides a very competitive discrete valued learning technique where error minimization is the primary concern. It is very eecient when a single classiier is to be applied to classify few cases, such as in a typical incremental learning scenario.