When dealing with light-weight robots with nonrigid limbs and joints and uncertain sensory readings, configuration state representations inevitably are approximate. Due to various types of sensory readings, which are usually body-grounded in different frames of reference, these configuration states may naturally be represented modularly distributed. From a different perspective, computational models of human motor planning suggest that the brain represents current body postures, such as the state of an arm, modularly within various frames of reference. Moreover, the different information sources, such as proprioceptive, anticipatory, and visual information, are probabilistically integrated. As a basis for such a representation, we propose a modular system that maintains a distributed self-representation in interactive frames of reference. We show that the resulting representation is highly noise robust and may be used to reach goals within various frames of reference while also considering other task constraints.