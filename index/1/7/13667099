3D hand pose estimation is a hot research topic in recent years. It's been widely used in many advanced applications for virtual reality and human-computer interaction, since it provides a natural interface for communication between human and cyberspace. Despite the fast development of this field, it is still a difficult task due to the various challenges. In this paper, we aim to build a 3D hand pose estimation system which can correctly detect human hands and accurately estimate its pose using depth images. To guarantee the robustness of our system, we design a hand model called spherical part model (SPM), and train a deep convolutional neural network using this model. Moreover, to reduce the influence of human's omissions, we use a data-driven approach to integrate them together. Our network can more accurately estimate hand pose based on prior knowledge of human hand. To demonstrate the superiority of our method, a complete experiment is conducted on two public and one self-built datasets. The results show that our system can detect human hands with average precision at almost 90% and the average error distance of the pose estimation is about 10 millimeters, and is better than the other state of the art works.