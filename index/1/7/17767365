Layered architecture genetic programming (LAGEP) has been applied on variety classification problems. It organizes populations as layers. Populations in different layers evolve with different training sets. Individuals produced by populations of layer Li transform training instances into new ones. Populations in Li+1 then evolve with the new training set instead of evolve with the original given training set. Each population in Li produces one feature for the new training instances. New training instances could have fewer features and are easier to be classified. Such mechanism makes consecutive layer gain better fitness value than preceding layers do. At this paper, we intend to analyze the enhancement of fitness value over all layers. We conduct experiments with a high-dimensional gene expression dataset to show the fitness enhancement.