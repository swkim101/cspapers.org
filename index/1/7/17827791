The modern knowledge worker has become very adept at working with desktop computers through familiar user interface devices such as keyboards, mice and screens. This interaction has relied on humans adapting to the context of the computer by going to the computer, sitting down and typing. This paradigm has been sufficient to assist people in an enormous number of tasks, but at the same time has limited the breadth of tasks in which a computer can assist. For general–purpose computers to extend their assistance further, it is necessary for them to adapt to the human context. Laptops and personal digital assistants (PDAs) are the first wave of this adaptation in that they partially adapt to a user’s location context. While miniaturization and mobilization have enabled this adaptation and provided new functionality, these devices generally remain blind to the world around them. As a result they have become intrusive in our daily lives as cell-phones ring in movie theaters, laptops interrupt presentations with instant messages, and PDAs rattle and beep for our attention in meetings. However, the goal of having computers fully aware of their user’s context is difficult and broad. Intel Research Seattle (IRS) (Intel Research Seattle 2003) and the Assisted Cognition project (Kautz et al. 2002b; 2002a) at the University of Washington in Seattle are developing a system entitled Guide to learn a small component of context by probabilistically inferring human activity. There are three research hypotheses that drive Guide’s development: