This article exploits the possibilities of mixed presentation modes in a situation where both public and private display screens as well as public and private audio channels can be accessed by the users. This will allow the users to share information with a group, while still being able to receive individual information at the same time. Special strategies are identified that readapt an already running public presen- tation to the interests of late arriving users. Following these strategies, the generation of multimodal presentations for both public and private devices is described. 1 Introduction and motivation Intelligent computing environments pose new challenges on the design of computer- user interfaces. In such environments, the classical input devices like mouse and keyboard will loose importance. In contrast, more human-like communication methods will play the key role. Interaction between user and computer will in- clude all kinds of modalities such as gestures, speech and haptics. Despite this development, large screens that may have to be shared with other users will still be used to display huge amounts of text and graphics, whereas small and portable screens will be used for the presentation of more private information. This will raise the need for multimodal interfaces that coherently mix audio and visual information in presentations and create user dialogues tailored for hybrid devices. This article exploits the possibilities of mixed presentation modes in a situa- tion where both public and private screens (e.g. small PDA and large wall-based displays) as well as public and private audio channels (e.g. loudspeakers and headphones) can be accessed by the users. This will allow the users to share information with a group, while still being able to receive private and individual information at the same time. We will focus on the concerns regarding the multimodal generation of text and audio for these situations, as well as analyse the question on how to combine public and private audio presentations that run at the same time. This is realised by exploiting the so-called cocktail-party-effect (1) that allows users to partially focus on different audio sources. Finally, we will explain how to adapt presen- tations for heterogenous user groups, allowing the system to react to changing