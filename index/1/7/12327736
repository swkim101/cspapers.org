Given an <i>n</i> × <i>d</i> matrix <i>A</i> and an <i>n</i>-vector <i>b</i>, the <i>l</i><inf>1</inf> <i>regression</i> problem is to find the vector <i>x</i> minimizing the objective function ||<i>Ax</i> - <i>b</i>||<inf>1</inf>, where ||<i>y</i>||<inf>1</inf> ≡ Σ<inf>i</inf>|<i>y</i><inf>i</inf>| for vector <i>y.</i> This paper gives an algorithm needing <i>O</i>(<i>n</i> log <i>n</i>)<i>d</i><i><sup>O</sup></i><sup>(1)</sup> time in the worst case to obtain an approximate solution, with objective function value within a fixed ratio of optimum. Given ∈ > 0, a solution whose value is within 1 + ≡ of optimum can be obtained either by a deterministic algorithm using an additional <i>O</i>(<i>n</i>)(<i>d</i>/∈)<i><sup>o</sup></i><sup>(1)</sup>) time, or by a Monte Carlo algorithm using an additional <i>O</i>((<i>d</i>/∈)<i><sup>O</sup></i><sup>(1)</sup>) time. The analysis of the randomized algorithm shows that weighted coresets exist for <i>l</i><inf>1</inf> regression. The algorithms use the ellipsoid method, gradient descent, and random sampling.