In this work we investigate the behavior of Gaussian processes as a regression technique for reinforcement learning. When confronted with too many mutually dependant learning examples, the matrix inversion needed for prediction of a new target value becomes numerically unstable. By paying attention to using suitable numerical techniques and employing QR-factorization these instabilities can be avoided. This leads to better and more stable performance of the attached reinforcement learner.