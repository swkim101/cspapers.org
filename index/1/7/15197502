A very successful approach in machine learning are neural networks. They learn an unknown regularity between real vector spaces, given a finite set of examples. Hence they can be applied in different areas such as signal processing, speech recognition, or time series prediction, to name just a few. Furthermore, a mathematical foundation, more precisely, their universal approximation ability, the information theoretical learnability, and the complexity of learning are investigated. However, they suffer mainly in two aspects compared to symbolic approaches: The functions implemented by neural networks are not understandable for humans. Different approaches deal with rule extraction or rule insertion in order to integrate prior knowledge – but the methods are not yet satisfactory and in-principle theoretical limitations exist [1, 7]. Furthermore, applications of networks for symbolic data require the encoding of the data in a finite dimensional vector space; often, structural information and prior information get lost during this process. Hence few applications in this area, for example, in order to guide theorem proving or to support symbolic learning methods exist – although possibly problem specific heuristics are necessary in symbolic learning mechanisms to make the problems tractable. The latter problem may be solved in a natural way via introducing recursive dynamics in standard networks. Discrete time recurrent neural networks constitute a simple example for this method, a general approach are folding networks [5, 9, 10]. They process symbolic data in a structure driven way. The recurrent connections enable neural networks to learn regularities on structured data such as terms or formulas as opposed to finite dimensional vectors. Hence they can be used to learn heuristics and control mechanisms in symbolic learning methods directly. Moreover, interpretations via classical formalisms are possible since the dynamics mirror the dynamics of finite automata or tree automata, respectively. Hence an interpretation which can be understood by humans seems easier than for standard feed-forward networks. Here we give an overview about this approach. We introduce folding networks formally and describe the in-principle way of how they are trained. The areas of application include automated theorem proving, picture processing, and QSAR prediction. Several theoretical results investigate their in-principle learnability.