Cache memories are often inefficiently managed, which results in significant memory penalties. An important reason for this poor performance is the homogeneous management of all memory references, even though different memory references may exhibit a very different locality. In this work, we present a novel data cache architecture composed of different modules, each module exploiting a particular type of locality. The information of which module each fetched data is placed on is passed to the hardware by means of a hint encoded in the memory instructions. This hint is set based on a locality analysis that can be performed by the compiler or using profiling data. The proposed data cache organization exhibits a high-performance for numerical codes, even with low capacity, A SKI3 cache has a miss ratio that is about 0.4 times the miss ratio of an SKI3 direct-mapped cache and it is very close to that of a 64KB fullyassociative cache, when data prefetching is not considered. Furthermore, the locality analysis allows for a selective one-block lookahead prefetch scheme, just for those references that exhibit spatial locality. This prefetch firther reduces the miss ratio to one half of that of a 64KB fully-associative cache, with a negligible increase in memory traffic compared with the non-prefetching scheme, due to the locality-driven selective approach. Although the proposed data cache organization is oriented towards numerical codes, which usually suffer more memory penalties than non-numerical ones, for the latest ones, a very simple profiling analysis results in a performance very close to a conventional cache, in spite of its lower capacity.