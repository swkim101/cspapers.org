Memory-based learning appears relatively successful when the learning data is highly disjunct, i.e., when classes are scattered over many small families of instances in instance space, as in many language learning tasks. Abstraction over borders of disjuncts tends to harm generalization performance. However , careful abstraction in memory-based learning may be harmless when it preserves the disjunctivity of the learning data. We investigate the eeect of careful abstraction in a series of language-learning task studies , and a small benchmark-task study. We nd that when combined with feature weight-ing or value-distance metrics, careful abstraction , as implemented in the new fambl algorithm , can equal the generalization accuracies of pure memory-based learning, while attaining fair levels of memory compression.