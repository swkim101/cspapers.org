In this paper we analyze joint attention between a robot that presents features of its surroundings and its human audience. In a statistical analysis of hand-coded video data, we find that the robot's physical indications lead to a greater attentional coherence between robot and humans than do its verbal indications.We also find that aspects of how the tour group participants look at robot-indicated objects, including when they look and how long they look, can provide statistically significant correlations with their self-reported engagement scores of the presentations. Higher engagement would suggest a greater degree of interest in, and attention to, the material presented. These findings will seed future gaze tracking systems that will enable robots to estimate listeners' state. By tracking audience gaze, our goal is to enable robots to cater the type of content and manner of its presentation to the preferences or educational goals of a particular crowd, e.g. in a tour guide, classroom or entertainment setting.