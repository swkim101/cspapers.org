Traditionally, path planning for field robotic systems is performed in Cartesian space: sensor readings are transformed into terrain costs in a (Cartesian) costmap, and a path to the goal is planned in that map. In this paper, we propose a new approach: planning a path for the robot in the image-space of an on-board camera. We apply a learned color- to-cost mapping to transform a raw image into a cost-image, which then undergoes a pseudo-configuration-space transform. We search in the resulting cost-image for a path to the projected goal point in the image. One benefit of our approach is the ability to react to obstacles at ranges well beyond our 3D sensor range - independent testing has confirmed our system has effectively reacted to obstacles at a range of 93 m while our stereo sensor provides reliable data only up to 5 m away. We describe the details of our technique and the results from testing under the DARPA LAGR and UPI programs.