Most modern approaches for tactile object recognition with robotic hands do not use proprioceptive data. In those that do, a limited number of objects with similar shapes is recognized. Furthermore, Self-Organizing Maps (SOM) based on raw values of joint angles/torques are frequently implemented which requires large sets of training data. In this paper, we present an approach based only on joint angles of a robotic hands to identify the shape of an object regardless its size and position within the hand. A representation of the joint angles is created to endow the robotic hand with proprioception. Support Vector Machine (SVM) is implemented for shape identification using patterns or signatures generated on this representation when the objects are grasped. To illustrate the scope of this method, tests are performed on five shapes present in common objects. Both SVM and SOM trained with signatures were at least 10% more accuracy than the ones trained with raw values of joint angles. Training sets are reduced at least 85% with respect to other works. An accuracy of 94% was obtained on large ranges of dimensions of the shapes and positions.