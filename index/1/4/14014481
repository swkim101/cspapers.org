While hardware-coherent distributed shared address space systems are increasingly successful at moderate scale, the decade-long goal of scalable performance on a wide range of applications has never been evaluated on real systems. A simulation study has examined scalability to over 100 processors for a few applications, but appropriate real systems are finally in place to take stock of the success of this approach. We examine the scalability of an aggressive casestudy machine, the SGI Origin2000, up to 128 processors for a range of applications. We find that for most applications, considered to be already highly optimized both algorithmically and in orchestration, the problem sizes predicted by the earlier simulation work (or those used in most evaluations of smaller machines) do not scale on this machine. Making problem sizes larger (to reasonable extents) achieves good scaling for surprisingly few applications. Rather, we find that substantial further application restructuring often has to be employed, and it is usually quite algorithmic in nature. With it, we are finally able to demonstrate scalable performance on a wide range of applications for the first time, including challenging kernels like FFT. With the restructured applications, the problem sizes needed to achieve good performance are reasonable (though still a lot larger than found in the simulation study). Interestingly, the restructuring is very similar to that used earlier to improve performance on moderate-scale shared virtual memory systems on clusters, indicating that generalizable programming guidelines may be developed for both scalability and performance portability across platforms. We examine where applications spend their time on this large machine, the impact of special hardware features that the machine provides, and the impact of mapping to the network topology. We conclude that the algorithmic burden at large scale begins to approach that needed for message passing for many applications. The burden of orchestration is still substantially lower but the process is far from easy and good algorithmic and programming guidelines need to be developed.