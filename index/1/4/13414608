There are three main contributions which this paper attempted to make: 1. Measurements of dynamic as well as static instruction utilization for a popular mainframe instruction set, which was then extended in the related Ph.D. thesis [I] to other instruction set architectures including microprocessors. There were few other comparable empirical studies in the literature, and we were pleased that our measurements were later used both for instruction-set design and for instruction implementation optimization. 2. Observations on the design of high-performance instructions sets that were implied by the measurements, which were again expanded upon in the thesis. The overall conclusion, counter to the movement in the 60’s and early 70’s towards complex instruction sets which reduced the semantic gap between programming languages and the hardware, was that “simpler is better”. We like to flatter ourselves in thinking that this study was one of the instigators of the RISC movement. 3. An “instruction timing” model of CPU timing analysis that was much simpler than a full simulation of the implementation but more accurate than just counting instructions. This hybrid model used timing formulas derived from the implementation (whose datadependent parameters were measured, such as average string lengths), augmented by simulations of subsystems that depended on instruction sequences, such as cache memory interlocks and instruction prefetch. The result was an easy-to-compute prediction of execution time that could be quickly adapted for proposed changes in the implementation. The model never was used in practice for that last purpose, perhaps because neither of the authors ever again worked for a mainframe manufacturer. Although this timing model was the motivation for the paper’s title, in retrospect it was the weakest of the three contributions.