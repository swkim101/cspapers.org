Multitask Learning is an inductive transfer method that improves generalization by using domain information implicit in the training signals of related tasks as an inductive bias. It does this by learning multiple tasks in parallel using a shared representation. Mul-titask transfer in connectionist nets has already been proven. But questions remain about how often training data for useful extra tasks will be available, and if multitask transfer will work in other learning methods. This paper argues that many real world problems present opportunities for multitask learning if they are not rst overly sanitized. We present eight prototypical applications of multitask transfer where the training signals for related tasks are available and can be leveraged. We also outline algorithms for multitask transfer in decision trees and k-nearest neighbor. We conclude that multitask transfer has broad utility.