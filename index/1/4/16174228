General representation, abstraction and exchange definitions are crucial for dynamically configurable context recognition. However, to evaluate potential definitions, suitable standard datasets are needed. This paper presents our effort to create and maintain large scale, multimodal standard datasets for context recognition research. We ourselves used these datasets in previous research to deal with placement effects and presented low-level sensor abstractions in motion based on-body sensing. Researchers, conducting novel data collections, can rely on the toolchain and the the low-level sensor abstractions summarized in this paper. Additionally, they can draw from our experiences developing and conducting context recognition experiments. Our toolchain is already a valuable rapid prototyping tool. Still, we plan to extend it to crowd-based sensing, enabling the general public to gather context data, learn more about their lives and contribute to context recognition research. Applying higher level context reasoning on the gathered context data is a obvious extension to our work. Motivation As of today, Context recognition systems, the core enablers of pervasive computing, are still handcrafted for specific application scenarios. To find the right sensors, features and classifiers to recognize non-trivial activities, is sadly more an art than science. Suitable context abstraction and representation formats for context would help for better re-use and selfconfiguration. Yet, they are missing. To develop and evaluate these formats, there’s the need for standardized, multimodal context data collections and corresponding software tools to manage them. As a basis for discussions about context representation formats, we present the following in this paper: • a description and pointers to several multimodal context datasets recorded with our toolchain for rapid prototyping. • a short overview of our toolchain for recording, handling and using large context data sets. Copyright c © 2012, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. • a placement categorization and low level sensor abstraction for motion based, on-body context recognition (evaluated on the datasets introduced). • the proposal towards a crowd-sourced context data collection. All tools introduced are open-sourced. The datasets are in the process of being published. Sensor-rich context data collection Compared to the computer vision or speech recognition fields, context recognition still lacks standardized datasets. This makes it very difficult to compare recognition systems, algorithms, models and the usefulness of higher level abstractions. However, compared to other research disciplines recording context recognition datasets proves to be difficult due to the following: Diverse Sensing Modalities – Often a multitude of different sensors needs to be managed. One needs to deal with the differing physical properties, sampling rate and other peculiarities. .If the application area is relatively unexplored, it is difficult to determine which sensing modality will work best. User/Environment Augmentation – From a research perspective, the more and the diverse the sensors are that are used in a setup the better. On the other hand, the more sensors the more complex the recordings and the more burden is on the user. Synchronization – Of course, the sensor streams also need to be synchronized. Activity class assignments get especially tricky if the sensing devices do not supply a steady sampling rate. Broad Application Scenarios – The user’s ”Context” a system is to recognize depends highly on a given application scenario. As there are wide application areas for pervasive computing technology, it is not feasible to provide a dataset for every use case. To tackle a part of these problems and manage the complexity of the recordings better, we developed an integrated toolchain for development, testing and deployment of context recognition systems.