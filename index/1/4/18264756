The design and implementation of embedded microprocessors is on the verge of change with MCM, flip-chip and integrated DRAM/ logic technologies moving from feasible to commercially viable. Embedded DRAM memory provides a high memory bandwidth together with low latency. However, when such a system is extended with further memory, the processor is exposed to a nonuniform memory hierarchy where some fraction of the total memory has significantly lower access latency than the remainder. In this paper, we evaluate various hardware-orientated approaches to the management of embedded memory. In addition to static page placement and conventional cache functionality, we explore novel memory management schemes that exploit temporal locality while still using the embedded DRAM as part of the physical memory space. Using execution driven simulation of real workloads, we show that this technique can deliver as much as a factor of two speedup over a conventional memory hierarchy. More importantly, we show how the embedded memory can be used to provide both data storage and a caching capability equivalent to a more complex processor device. These results imply that designers of embedded and small-scale systems can achieve significant performance wins, through the use of a combined processor/ memory device and the memory system design proposed.