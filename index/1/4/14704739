Most learning algorithms work most eeec-tively when their training data contain completely speciied labeled samples. In many diagnostic tasks, however, the data will include the values of only some of the attributes ; we model this as a blocking process that hides the values of those attributes from the learner. While blockers that remove the values of critical attributes can handicap a learner, this paper instead focuses on block-ers that remove only irrelevant attribute values , i.e., values that are not needed to classify an instance, given the values of the other unblocked attributes. We rst motivate and formalize this model of \superruous-value blocking", and then demonstrate that these omissions can be useful, by proving that certain classes that seem hard to learn in the general PAC model | viz., decision trees and DNF formulae | are trivial to learn in this setting. We also show that this model can be extended to deal with (1) theory revision (i.e., modifying an existing formula); (2) blockers that occasionally include superruous values or exclude required values; and (3) other corruptions of the training data.