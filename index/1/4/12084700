Dimensionality reduction is often a crucial step for the successfulapplication of machine learning and data mining methods. One way toachieve said reduction is feature selection. Due to the impossibilityof labelling many data sets, unsupervised approaches arefrequently the only option. The column subset selection problemtranslates naturally to this purpose, and has received considerableattention over the last few years, as it provides simple linear modelsfor data reconstruction. Existing methods, however, often achieveapproximation errors that are far from the optimum. In this paper wepresent a novel algorithm for column subset selection thatconsistently outperforms state-of-the-art methods in approximationerror. We present a series of key derivations that allow anefficient implementation, making it comparable in speed and in somecases faster than other algorithms. We also prove results that make itpossible to deal with huge matrices, which has strong implications for otheralgorithms of this type in the big data field. We validate our claimsthrough experiments on a wide variety of well-known data sets.