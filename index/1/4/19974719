We study decentralized learning dynamics for the classic assignment game with transferable utility [Shapley and Shubik 1972]. In our model agents follow an aspiration adjustment process based on their experienced payoffs (see [Sauermann and Selten 1962], [Nax and Pradelski 2014]). At random points in time firms and workers match, break up, and re-match in the search for better opportunities. Agents have aspiration levels that they adjust based on their experienced payoffs. When matched an agent occasionally tries to succeed with a higher bid than his current aspiration level. When single an agent lowers his aspiration level in the hope of attracting a partner. In particular agents have no knowledge about other players' payoffs or actions and they update their behavior in a myopic fashion. Behavior fluctuates according to a random variable that reflects current market sentiment: sometimes the firms exhibit greater price stickiness than the workers, and at other times the reverse holds. We show that this stochastic learning process converges in polynomial time to the core. While convergence to the core is known for some types of decentralized dynamics this paper is the first to prove {polynomial time convergence}, a crucial feature from an explanatory and market design standpoint. We also show that without market sentiment the dynamic exhibits exponential time convergence. The proof relies on novel results for random walks on graphs, and more generally suggests a fruitful connection between the theory of random walks and matching theory.