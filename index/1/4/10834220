Performance is an important quality attribute that needs to be planned and managed proactively. Abstract models of the system are not very useful if they do not produce reasonably accurate metrics. Detailed models are time consuming and expensive to build as well as to simulate. In order to strike a right balance, a framework is proposed in this paper that takes advantage of the flexibility of abstract modeling and intricacies of detailed modeling. Performance is modeled and verified per use case using a hierarchical queuing model of the system. Each component job is represented through characterization functions and service requests. Characterization functions may be parametric regression models derived from job measurements on system level model. A co-design framework is used to simulate and measure the performance of software components. The use case simulator analyzes the performance and verifies the use case requirements.