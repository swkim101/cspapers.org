Pseudo-relevance feedback (PRF) is an unreliable method, as it does not use any true relevance information. Typically, it extracts candidate expansion terms from the top R documents initially retrieved, sorts them using a term selection criterion, and adds the top T terms to the initial query. In [9], we explored exbile PRF for selecting appropriate R and/or T for each request to enhance the reliability of PRF. Although this did not show any substantial gain over traditional PRF, the results suggested that the top 10 average document scores in the initial ranked output and the number of initial search terms may be useful clues for predicting R and/or T . Since then, we tackled a simpler problem, of deciding whether PRF should be applied or not: using the above two statistics as predictors, we applied quadratic discriminant analysis to separate requests that PRF cannot improve from others. However, as the requests could not be discriminated accurately, we realised that even this \binary decision" version of the exible PRF problem is challenging. Thus this paper explores an alternative way of improving the reliability of PRF, namely, to enhance the term selection criterion itself. We re ne an existing criterion based on regression analysis, and compare various criteria using English and Japanese test collections of respectable size.