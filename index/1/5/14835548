In recent years, the l1,p-regularizer has been widely used to induce structured sparsity in the solutions to various optimization problems. Currently, such l1,p-regularized problems are typically solved by first-order methods. Motivated by the desire to analyze the convergence rates of these methods, we show that for a large class of l1,p-regularized problems, an error bound condition is satisfied when p e 2 [1, 2] or p = ∞ but fails to hold for any p e (2, ∞). Based on this result, we show that many first-order methods enjoy an asymptotic linear rate of convergence when applied to l1,p-regularized linear or logistic regression with p e [1, 2] or p = ∞. By contrast, numerical experiments suggest that for the same class of problems with p e (2, ∞), the aforementioned methods may not converge linearly.