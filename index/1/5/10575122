Most of the effort AI has put into common sense reasoning has involved inference by sequential rule application. This approach is most effective in well characterized domains where any valid chain of inference from a set of observations leads to an acceptable interpretation. In more realistic cases where there are multiple consistent interpretations that are not equally good, or where there are no consistent interpretations, it seems more natural to choose the best alternative based on the interpretations themselves rather than the chains of inference used to derive them. µKLONE is a connectionist network which uses simulated annealing to search the space of interpretations, or models. Inconsistent theories lead to generation of models which come as close as possible to satisfying all of the axioms, so counterfactual reasoning can be accomplished by the same mechanism as factual reasoning. An example involving conflicting information is presented for which µKLONE finds an intuitively plausible interpretation.