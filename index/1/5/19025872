In this paper we propose an biologically inspired attention-based vision system for the JAST interactive dialog robot. The robotvision system incorporates three submodules: object recognition, gesture recognition and self recognition. Herein two assumptions form the theoretical foundation: first and generally, attention is effected by bottom-up attractors. These may arise from high intensity / hue gardients or scene dynamics. Second, the focus of attention can be directed by higher level processes, whether volitionally or not, in an inhibitory or reinforcing way. The system proposed in this paper utilizes these assumptions to organize its computational efforts in an Attention Condensation Layer accordingly. Due to its efficient data management architecture, the system is capable of continuously publishing results to the robotâ€™s cognitive layer and thus operating in realtime. Furthermore, the modular structure and the asynchronous communication paradigm allows for efficient integration of additional modules, be it visual or from any other sensor. The main contribution of this work is the application of neuroscientific findings on human early visual processing to a real-world robotic setup. Here, our experimental results show tremendous speed-ups compared to naive implementations, reaching the peak in a combination of the top-down and bottom-up principles.