Three methods for improving the generalization performance of Linear Support Vector Machines are proposed and this in the case that some dimensions in the data can be considered irrelevant for the pattern recognition task at hand. In contrast to other methods, the generalization improvement is not obtained by modifying the system, but by modifying the training set itself. The first proposed method guarantees faultless performance and can be applied in the case of a small number of irrelevant dimensions. The two other proposed methods are approximations of this method for the case of larger number of irrelevant dimensions. In order to test these approximations, the methods are applied to a real-world application: 3D object recognition without segmentation. In this case the images of the objects contain a background that should be made irrelevant for the recognition task. The approximative method that uses a pair of black/white backgrounds as training set, gives rise to excellent results. We are able to report on correct recognition rates of unseen views of 3D objects placed on a wide variety of cluttered backgrounds that never are worse than 94.5% for an set-size of 30 objects. This is compared to the performance of SVMs without this training method: in this case the performance can drop below 20% on specific backgrounds.