Recent work [1], [2], [3] has shown promising results in learning from demonstrations for the manipulation of deformable objects. Their approach finds a non-rigid registration between points in the demonstration scene and points in the test scene. This registration is then extrapolated and applied to the gripper motions in the demonstration scene to obtain the gripper motions for the test scene. If more than one demonstration is available, a quality score for the non-rigid registration is used to determine the best matching training scene. For many manipulation tasks, however, the gripper's direction of approach with respect to the objects' surface normals is important in order to succeed at the task. This prior work only registers points across scenes and does not register the surface normals, often leading to warps between scenes that are inappropriate for transfer of manipulation primitives. The main contributions of this paper are (i) An algorithm for non-rigid registration that considers both points and normals, and (ii) An evaluation of this registration approach in the context of learning from demonstrations for robotic manipulation. Our experiments, which consider an insertion task in simulation and also knot-tying and towel-folding executions in a PR2, show that incorporating normals results in improved performance and qualitatively better grasps.