As signal speeds increase and gate delays decrease for high-performance digital integrated circuits, the gate delay modeling problem becomes increasingly more difficult. With scaling, increasing interconnect resistances and decreasing gate-output impedances make it more difficult to empirically characterize gate-delay models. Moreover, the single-input-switching assumption for the empirical models is incompatible with the inevitable simultaneous switching for today.s high-speed logic paths. In this paper a new empirical gate delay model is proposed. Instead of building the empirical equations in terms of capacitance loading and input-signal transition time, the models are generated in terms of parameters which combine the benefits of empirically derived k-factor models and switch-resistor models to efficiently: 1) handle capacitance shielding due to metal interconnect resistance, 2) model the RC interconnect delay, and 3)provide tighter bounds for simultaneous switching.