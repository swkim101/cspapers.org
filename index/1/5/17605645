In this research we present a non-invasive Brain-Machine Interface (BMI) system that allows patients with motor paralysis conditions to control electronic appliances in a hospital room. The novelty of our system compared to other BMI applications is that our system gradually becomes autonomous by learning user actions (i.e. turning on/off window, lights, etc.) under certain environment conditions (temperature, illumination, etc.) and brain states (i.e. awake, sleepy, etc.). By providing learning capabilities to the system, patients are relieved from mental fatigue or stress caused by continuously controlling appliances using a BMI. We present an interface that allows the user to select and control appliances using electromyogram signals (EMG) generated by muscle contractions such as eyebrow movement. Our learning approach consists in two steps: 1) monitoring user actions, input data from sensors distributed around the room, and Electroencephalogram (EEG) data from the user, and 2) using an extended version of the Bayes Point Machine approach trained with Expectation Propagation to approximate a posterior probability from previously observed user actions under a similar combination of brain states and environmental conditions. Experimental results with volunteers demonstrate that our system provides satisfactory user experience and achieves over 85% overall learning performance after only a few trials.