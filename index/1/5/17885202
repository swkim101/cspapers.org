This paper proposes a method for learning a hand-eye calibration and its application for visual servoing. The goal is to develop a technique that combines the strengths of existing visual servoing methods. Particularly, as in image-based visual servoing, the error is measured in the visual space while the motor command is position-based. Hence this method approximates the visuomotor function that relates variations in the visual space to variations in the motor space at a global scale. The method used for approximating the visuomotor function is derived from the field of reinforcement learning, making our hand-eye calibration autonomous, continuous and adaptable. The visuomotor function is modeled by a linear combination of polynomials, each spanning a non-mutually exclusive subset of the visual space. Each polynomial represents the utility of motor commands for the servoing task. The goal of the calibration is to approximate the parameters of these polynomials while the system interacts with its environment. Preliminary results include centering a target in the image in which the system learns the motor commands that eliminates the errors in the visual space and generalizes the result to neighboring states in the visual space, depths and motor commands.