We introduce two boosting algorithms that aim to increase the generalization accuracy of a given classiier by incorporating it as a level-0 component in a stacked generalizer. Both algorithms construct a complementary level-0 classiier that can only generate coarse hypotheses for the training data. We show that the two algorithms boost generalization accuracy on a representative collection of data sets. The two algorithms are distinguished in that one of them modiies the class targets of selected training instances in order to train the complementary classiier. We show that the two algorithms achieve approximately equal generalization accuracy, but that they create complementary classiiers that display diierent degrees of accuracy and diversity. Our study provides evidence that it may be useful to investigate families of boosting algorithms that incorporate varying levels of accuracy and diversity, so as to achieve an appropriate mix for a given task and domain.