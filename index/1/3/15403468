This paper presents and empirically compares solutions to the problem of vision and self-localization on a legged robot. Specifically, given a series of visual images produced by a camera on-board the robot, how can the robot effectively use those images to determine its location over time? Legged robots, while generally more robust than wheeled robots to locomotion in various terrains (Wettergreen & Thorpe 1996), pose an additional challenge for vision, as the jagged motion caused by walking leads to unusually sharp motion in the camera image. This paper considers two main approaches to this vision and localization problem, which we refer to as the object detection approach and the expectationbased approach. In both cases, we assume that the robot has complete, a priori knowledge of the three-dimensional layout of its environment. These two approaches are described in the following section. They are implemented and compared on a popular legged robotic platform, the Sony Aibo ERS-7. Many researchers have addressed the problem of vision and localization on a legged robot, in part because of the four-legged league of the annual RoboCup robot soccer competitions (Stone, Balch, & Kraetszchmar 2001). Nevertheless, of all of the approaches to this problem on a legged robot, we are not aware of any that take the expectationbased vision approach. However, we show that this approach can yield a higher overall localization accuracy than a state-of-the-art implementation of the object detection approach. This paperâ€™s contributions are an exposition of two competing approaches to vision and localization on a legged robot and an empirical comparison of the two methods.