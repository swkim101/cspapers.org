Learning an input-output mapping from a set of examples can be regarded as synthesizing an approximation of a multi-dimensional function. From this point of view, this form of learning is closely related to regularization theory, and we have previously shown (Poggio and Girosi, 1990a, 1990b) the equivalence between reglilari~at.ioll and a. class of three-layer networks that we call regularization networks. In this note, we ext.end the theory by introducing ways of <lealing with t.wo aspect.s of learning: learning in presence of unreliable examples or outlielÂ·s, an<llearning from positive and negative examples.