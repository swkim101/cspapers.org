We present a coactive algorithm for learning a human expert's preferences in planning trajectories for information gathering in scientific autonomy domains. The algorithm learns these preferences by iteratively presenting solutions to the expert and updating an estimated utility function based on the expert's improvements. We apply these algorithms, in the context of underwater data collection, using a pair of risk and reward maps. In simulated trials, the algorithm successfully learns the underlying weighting behind a utility map used by a human planning trajectories. We also present experimental trials demonstrating the algorithm using a temperature and depth monitoring task in an inland lake with an autonomous surface vehicle. This work shows it is possible to design algorithms for autonomous navigation with reward functions that capture the essence of a human's preferences.