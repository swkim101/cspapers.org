Today, creating systems that are capable of interacting naturally and efficiently with humans on many levels is essential. One step toward achieving this is the recognition of emotion from whole body postures of human partners. Currently, little research in this area exists in computer science. Therefore, our aim is to identify and measure the saliency of posture features that play a role in affective expression. As a case-study, we collected affective gestures from human subjects using a motion capture system. We first described these gestures with spatial features. Through standard statistical techniques, we verified that there was a statistically significant correlation between the emotion intended by the acting subjects, and the emotion perceived by the observers. We examined the use of discriminant analysis to measure the saliency of the proposed set of posture features in discriminating between 4 basic emotions: angry, fear, happy, and sad. Our results show that the set of features discriminates well between emotions, and also provides evidence about the strong overlap between descriptors in both acting and observing activities.