In statistical language modeling, one technique to reduce the problematic eects of data sparsity is to partition the vocabulary into equivalence classes. In this paper we investigate the eects of applying such a technique to higherorder n-gram models trained on large corpora. We introduce a modification of the exchange clustering algorithm with improved eciency for certain partially class-based models and a distributed version of this algorithm to eciently obtain automatic word classifications for large vocabularies (>1 million words) using such large training corpora (>30 billion tokens). The resulting clusterings are then used in training partially class-based language models. We show that combining them with wordbased n-gram models in the log-linear model of a state-of-the-art statistical machine translation system leads to improvements in translation quality as indicated by the BLEU score.