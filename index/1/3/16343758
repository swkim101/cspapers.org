Reinforcement learning agents attempt to learn and construct a decision policy which maximises some reward signal. In turn, this policy is directly derived from long-term value estimates of state-action pairs. In environments with real-valued state-spaces, however, it is impossible to enumerate the value of every state-action pair, necessitating the use of a function approximator in order to infer state-action values from similar states. Typically, function approximators require many parameters for which suitable values may be diicult to determine a-priori. Traditional systems of this kind are also then bound to the xed limits imposed by the initial parameters, beyond which no further improvements are possible. This paper introduces a new method to adaptively increase the resolution of a discretised action-value function based upon which regions of the state-space are most important for the purposes of choosing an action. The method is motivated by similar work by Moore and Atkeson but improves upon the existing techniques insofar as it: i) is applicable to a wider class of learning tasks, ii) does not require transition or reward models to be constructed and so can also be used with a variety of model-free reinforcement learning algorithms, iii) continues to improve upon policies even after a feasible solution to the learning problem has been found.