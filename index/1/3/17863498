Autonomous mobile robots should be capable to learn and maintain visual models of their environments. Maps are needed for path-planning, self-localization, and human-robot interaction. In contrary, stereo-vision based mobile robot navigation has always been a challenge. This is due to complexity of integrating massive amount of visual information gathered by visual system, while undergoing a motion. Visual perception of an environment, is also an important capability for mobile robots, where lots of efforts are being undertaken by mobile robots research community. Once it comes to integrating environment visual information for clever navigation maps learning, the issue gets even much complicated. Therefore, we shall highlight few recent attempts to close such gap between visual sensory, and AI related navigation maps learning.