A. Hua’ and Chiang Lee Mid-Hudson Laboratories Kingston, NY 12401 The capacity and performance of database management system (DBMS) using a conventional (von Newmann-type) computer are limited by the total I/O channel bandwidth, the aggregate processing power, and the amount of main memory. With the advent of micro-processor, memory, and communication technology, it is econominally feasible to develop a parallel database computer system. The parallel processing techniques are employed to utilize the available resources in a coordinated fashion to solve the DBMS capacity and performance problems. Relations in such an environment are declustered into fragments and spreaded across computers. To achieve the optimal performance in data processing, it is essential for each computer to have a perfectly balanced load (i.e., identical amount of data). However, fragment sizes may vary due to insertions to and deletions from a relation. To retain good performance, the system needs to periodically rebalance data loads among the computers. In this paper, we present an adaptive data placement scheme which balances computer work loads during query processing. The entire scheme is built on top of the popular grid file structure (but not limited to grid file). The adaptivity of the scheme and its relevant features are discussed. The cost of load rebalancing is estimated. The result shows that under our assumptions, it is always beneficial to perform load rebalancing before performing a join on skewed data. Permission to copy without fee all OIpart of this material i5 granted provided that the topics are not made or Ji~trihutd fog direct commercial advantage. the VLDB copyright notice and the title of the publication and it\ date appear. and notice i\ gi\cn that copying is hy permission of the Vq Large Data B;IW Endowment. To copy othcrwisc. or to rcpuhlish. rquircs ;I fee and/or specinl permission from the Endonmcnt. Proceedings of the 16th VLDB Conference Brisbane, Australia 1990 1. INTRODUCTIOW In a database processing environment, the fact that disk I/O is the main bottleneck has been a consensus according to researches in the past. As the speed of microprocessors improves rapidly with the development of RISC technology, the problem becomes even more serious. It is more than likely that in the foreseeable future, this situation will not change. This problem is usually addressed today by data declustering in which each relation is partitioned into fragments and spread equally among all disk drives in the system. For instance, a hashed strategy is employed in the Teradata DBC/1012 ]T’er88]. A randomizing function is applied to the key attribute of each tuple to select a storage unit A similar approach is used by the Grace database machine [Kit84]. This storage structure is very ellicient for relational operations such as JOIN. However, it supports some of the other operations, such as Range-SELECT, very poorly. Gamma [Dew861 offers more flexibility by allowing a relation to be partitioned in any of three ways: round-robin distribution, hashed or range partitioned. As implied by its name, round-robin distributes the tuplcs among all storage units in a round-robin fashion. The hashed strategy is similar to that used in the Teradata DBC/1012. In the range partitioned strategy, each computing unit is assigned a range of key values in such a way that the partition of the relation based on the key attribute will result in a balanced data load at each computing unit. The data placement strategies discussed so far are built around the ideas of hashing and sorting of relations so that relational operations applied on the partitioning attribute can be performed very efficiently. These storage structures, however, would l Author’s current address: University of Central Florida, Department of Computer Science, Orlando, Florida 3281 64362.