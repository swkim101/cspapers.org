Test collections are growing larger, and relevance data constructed through pooling are suspected of becoming more and more incomplete and biased [1]. Relevance data are incomplete if there exist some relevant documents among the unjudged documents in the test collection. Furthermore, incomplete relevance data are biased if they represent some limited aspects of the complete set of relevant documents. For example, if the number of pooled systems is small, the resultant test collection may overestimate these systems and underestimate systems that did not contribute to the pool. We will refer to this phenomenon as system bias. Bias may also be caused by shallow pools: If only documents at the very top of submitted ranked lists are judged, the resultant relevance data may contain relevant documents that are very easy to retrieve, but not those that are difficult to retrieve. We will refer to this phenomenon as pool depth bias. The objective of this paper is to examine the robustness of retrieval effectiveness metrics in the presence of pool depth bias, with an emphasis on those that can handle graded relevance. Several researchers have proposed evaluation metrics specifically for handling the incompleteness of relevance data, but most of them have only examined the metrics under incomplete but unbiased conditions, using random subsamples of the original relevance data (e.g. [1, 6, 7]). While random subsampling may mimic a situation where the number of judged documents is extremely small compared to the entire document collection, it does not address the problems due to system bias and pool depth bias. Therefore, this paper examines metrics in more realistic settings, by reducing the pool depth. We have also examined the effect of system bias, but will report on the results in a separate paper [5]. The metrics we examine are: Average Precision (AP), Q-measure (Q) [4, 6], nDCG [2], Rank-Biased Precision (RBP) [3], their condensed-list versions AP′, Q′, nDCG′,