We examine maximum spanning tree-based methods for learning the structure of tree Conditional Random Fields (CRFs) P(Y|Χ). We use edge weights which take advantage of local inputs Χ and thus scale to large problems. For a general class of edge weights, we give a negative learnability result. However, we demonstrate that two members of the class–local Conditional Mutual Information and Decomposable Conditional Influence– have reasonable theoretical bases and perform very well in practice. On synthetic data and a large-scale fMRI application, our methods outperform existing techniques.