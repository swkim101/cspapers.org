We propose an appearance-based head pose estimation method that can be automatically adapted to individual scenes. Appearance-based estimation methods usually require a ground-truth dataset taken from a scene which is similar to test video sequences. However, it is almost impossible to acquire many manually-labeled head images for each scene. To address the problem, we introduce a new approach for aggregating ground truth head pose labels automatically by inferring head pose labels from walking direction. Experimental results demonstrate that our proposed method achieves better accuracy in head pose estimation than the conventional approach using a scene-independent generic dataset.