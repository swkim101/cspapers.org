We propose a framework for devising empirically testable algorithms for bridging the communication gap between humans and robots. We instantiate our framework in the context of a problem setting in which humans give instructions to robots using unrestricted natu-ral language commands, with instruction sequences being subservient to building complex goal conﬁgurations in a blocks world. We show how one can collect meaningful training data and we propose three neural architectures for interpreting contextually grounded natural language commands. The proposed architectures allow us to correctly understand/ground the blocks that the robot should move when instructed by a human who uses unrestricted language. The architectures have more difﬁ-culty in correctly understanding/grounding the spatial relations required to place blocks correctly, especially when the blocks are not easily identiﬁable.