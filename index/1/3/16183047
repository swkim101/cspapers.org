In this paper we develop algorithms for approximating matrix multiplication with respect to the spectral norm. Let <i>A</i> ∈ R<sup><i>n</i>x<i>m</i></sup> and <i>B</i> ∈ R<sup><i>n</i>x<i>p</i></sup> be two matrices and ε > 0. We approximate the product <i>A</i><sup>T</sup> <i>B</i> using two sketches <i>Ã</i> ∈ R<sup><i>t</i>x<i>m</i></sup> and B ∈ R<sup><i>t</i>x<i>p</i></sup>, where <i>t</i> << <i>n</i>, such that
 [EQUATION]
 with high probability. We analyze two different sampling procedures for constructing <i>Ã</i> and B; one of them is done by i.i.d. non-uniform sampling rows from <i>A</i> and <i>B</i> and the other by taking random linear combinations of their rows. We prove bounds on <i>t</i> that depend only on the intrinsic dimensionality of <i>A</i> and <i>B</i>, that is their rank and their stable rank.
 For achieving bounds that depend on rank when taking random linear combinations we employ standard tools from high-dimensional geometry such as concentration of measure arguments combined with elaborate ε-net constructions. For bounds that depend on the smaller parameter of stable rank this technology itself seems weak. However, we show that in combination with a simple truncation argument it is amenable to provide such bounds. To handle similar bounds for row sampling, we develop a novel matrix-valued Chernoff bound inequality which we call low rank matrix-valued Chernoff bound. Thanks to this inequality, we are able to give bounds that depend only on the stable rank of the input matrices.
 We highlight the usefulness of our approximate matrix multiplication bounds by supplying two applications. First we give an approximation algorithm for the <i>l</i><sub>2</sub>-regression problem that returns an approximate solution by randomly projecting the initial problem to dimensions linear on the rank of the constraint matrix. Second we give improved approximation algorithms for the low rank matrix approximation problem with respect to the spectral norm.