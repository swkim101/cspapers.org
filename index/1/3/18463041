This paper investigates the problem of cross-dataset facial expression recognition. To the best of our knowledge, this problem has not been formally addressed in the literature. Conventional facial expression recognition methods assume expression images in the training and testing sets are collected under the same condition such that they are independent and identically distributed. In many real applications, this assumption may not hold as the testing data are usually collected online and generally more uncontrollable than the training data, and hence, they are likely different from the training data. This problem is referred to as cross-dataset facial expression recognition in this paper as the training and testing data are considered to be collected from different datasets due to different acquisition conditions. To address this, we propose a new transfer subspace learning approach to learn a feature space which transfers the knowledge gained from the training set to the target (testing) data to improve the recognition performance under cross-dataset scenarios. Experimental results for facial expression recognition tasks on different datasets are presented to demonstrate the efficacy of the proposed approach.