We analyze visual cues people used to evaluate a robot grasp. Participants were presented with two (front and side) orthogonal views of a robot hand grasping an object and asked how successful the grasp would be on a scale of 1-5; they were eye-tracked while completing this survey. Ground truth of the success of the grasps is known. Our primary observations were that (1) Most of the failed grasp predictions were false positives, and this was exacerbated for grasps that were ranked as human-like. (2) Two visual cues from human-grasp research (object center-line and top) were used, but not contact points. Instead, participants gazed at robot finger, wrist, and arm locations. (3) There was a difference in the visual patterns between the left and right images, indicating that the second image was primarily used to verify the locations of fingers and wrist while the first was used to establish the object's location and shape. Finally, we generate transition matrices to model the temporal aspect of the gaze patterns.