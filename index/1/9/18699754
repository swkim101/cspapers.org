When human talk, they exchange a lot of audio information along with the words. Computers, however, don’t hear between the lines, which is one reason why speech recognition application can seem so frustratingly stupid. The technical reason for this is in the Hidden Markow Model (HMM) that most speech recognition systems employ for speech processing. HMM rely only on tiny, 10 millisecond slices of speech and while this works well for picking up words, it cannot capture the conceptual cues that span words, phrases or sentences. In fact, a pause at the end of a sentence or a drop in pitch spans 10-100 times large than what HMM can capture. Prosodic information is slowly being recognised as an important source of information in speech understanding. Prosody includes the duration, pitch and energy of speech. Duration, or the way people stretch or speed certain parts of speech is, arguably, the most important component of prosodic information. Studies in psycho-linguistic have shown that people use the duration of speech sounds in certain ways to emphasise the information content of what they are saying. Currently, spoken document retrieval (SDR) is carried out using Information Retrieval (IR) techniques on transcripts of spoken documents. These transcripts are produced using speech recognition systems, often employing HMM, that disregard prosodic information, concentrating instead in producing the most faithful possible recognition of the spoken words. Although this approach has proven quite successful [1], we believe that prosodic information should be used for a more effective indexing and retrieval of spoken documents in all those situations in which it would be impossible to obtain acceptable levels of speech recognition accuracy. The work briefly presented in this paper aims at: 1) establishing a correspondence between prosodic information and semantic information to define ways of identifying “acoustic” keywords; 2) studying the relationship between “prosodic in-