We present a bias variance decomposition of expected misclassi cation rate the most commonly used loss function in supervised classi cation learning The bias variance decomposition for quadratic loss functions is well known and serves as an important tool for analyzing learning algorithms yet no decomposition was o ered for the more commonly used zero one misclassi cation loss functions until the recent work of Kong Dietterich and Breiman Their decomposition su ers from some ma jor shortcomings though e g potentially negative variance which our decomposition avoids We show that in practice the naive frequency based estimation of the decompo sition terms is by itself biased and show how to correct for this bias We illustrate the decomposition on various algorithms and datasets from the UCI repository