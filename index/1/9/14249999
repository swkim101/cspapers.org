This paper presents a unified framework to enable automatic exploration with a pretouch sensor to reduce object shape uncertainty before grasping. The robot starts with only the incomplete object shape data it acquires from a Kinect depth sensor-it has no model of the object. Combining the Kinect pointcloud with prior probability distributions for occlusion and transparency, it makes inferences about portions of the object that the Kinect was not able to observe. Operating on the inferred shape of the object, an iterative grasp replanning and exploration algorithm decides when further exploration is required, and where to explore in the scene using the pretouch sensor. The information gathered by the exploration action is added directly to the robot's environment representation and is considered automatically in the next grasp planning iteration.