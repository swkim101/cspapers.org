We model the limited buffer queueing process that occurs within the UNIX operating system's protocol processing layers. Our model accounts for the effects of user process multiprogramming and preemptive, priority scheduling of interrupt, operating system, and user tasks. After developing the model, we use it to predict message loss that occurs during local area network (LAN) multicast. Our service time model can be applied to window-and rate-based stream flow control.