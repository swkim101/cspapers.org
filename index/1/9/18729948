In this paper, an approach for learning optimal striking points is proposed. Based on a ball-flight model and a rebound model, a set of reachable striking points within the robot's workspace can be obtained. However, while these striking points are geometrically reachable, their success probability differs substantially due to the robot's nonlinear dynamics, the distance to the ball, the need to reach sufficient velocity as well as the right angle at interception and non-uniform sensitivity to errors. Thus, it is crucial for a ping-pong robotic system to select striking points well. As a successful ball interception is the result of various factors that cannot be modeled straightforwardly, we suggest determining optimal striking points based on a reward function that measures how well the ping-pong ball's trajectory and the racket's movement coincidence. In this approach, we propose to learn a stochastic policy over the reward given the prospective striking point in order to facilitate exploration of a wide range of prospective striking points. The resulting learning method takes both the amount of experience data and its confidence into account to reach optimal solutions reliably. Evaluation with a real robotic system demonstrates the applicability of the proposed method.