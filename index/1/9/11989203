Two different classifier representations based on dynamic artificial neural networks (ANNs) and genetic programming (GP) are being compared on a human action recognition task by an ubiquitous mobile robot. The classification methodologies used, process time series generated by an indoor ubiquitous 3D tracker which generates spatial points based on 23 reflectable markers attached on a human body. This investigation focuses mainly on class discrimination of normal and aggressive action recognition performed by an architecture which implements an interconnection between an ubiquitous 3D sensory tracker system and a mobile robot to perceive, process, and classify physical human actions. The 3D tracker and the robot are used as a perception-to-action architecture to process physical activities generated by human subjects. Both classifiers process the activity time series to eventually generate surveillance assessment reports by generating evaluation statistics indicating the classification accuracy of the actions recognized.