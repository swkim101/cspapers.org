In this paper, we propose a new generative approach for semantic slot ﬁlling task in spoken language understanding using a nonparametric Bayesian formalism. Slot ﬁlling is typically formulated as a sequential labeling problem, which does not directly deal with the posterior distribution of possible slot values. We present a nonparametric Bayesian model involving the generation of arbitrary natural language phrases, which allows an explicit calculation of the distribution over an inﬁnite set of slot values. We demonstrate that this approach signiﬁcantly improves slot estimation accuracy compared to the existing sequential labeling algorithm.