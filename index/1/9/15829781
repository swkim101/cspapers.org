Following up on numerous reports of analogy-based identiﬁcation of “linguistic regularities” in word embeddings, this study applies the widely used vector offset method to 4 types of linguistic relations: inﬂectional and derivational morphology, and lexicographic and en-cyclopedic semantics. We present a balanced test set with 99,200 questions in 40 categories, and we systematically examine how accuracy for different categories is affected by window size and dimensionality of the SVD-based word embeddings. We also show that GloVe and SVD yield similar patterns of results for different categories, offering further evidence for conceptual similarity between count-based and neural-net based models.