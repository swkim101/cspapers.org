We apply large deviation theory to assess the probability that the average, or the sum, of the response times of a sequence of consecutive aperiodic jobs is below a given threshold. This coarse-grained performance metric is for instance adapted to evaluate the responsiveness of a soft-real system or the freshness of input data consumed by an algorithm. The technique proposed works with distribution of response times as input but does not require that the distribution obeys a closed-form equation. Indeed, it can accept empirical distributions given under the form of frequency histograms obtained, for instance, by monitoring the system. Future work should be devoted to further assess the applicability of the proposal and relax some technical assumptions.