Motivated by the need for maintaining multiple, large queues of data in modern high-performance systems, we study the problem of caching queues in memory under the following simple, but widely applicable, model. At each clock-tick, any number of data items may enter the various queues, while data-items are consumed from the heads of the queues. Since the number of unconsumed items may exceed memory buffer size, some items in the queues need to be spilled to secondary storage and later moved back into memory for consumption. We provide online queue-caching algorithms under a number of interesting cost models.