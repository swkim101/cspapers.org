We describe CATS, a recently started project, endorsed by the ACM and the EATCS, to develop a Webbased collection of test sets. These test sets, each specialized for a specific problem (such as network 0ow or satisfiability), are maintained by volunteers (associate editors), using contributions from researchers (authors). The purpose is to facilitate experimental research by standardizing common benchmarks, providing a mechanism for their evolution, and making them easily accessible and usable; and to identify sign&ant open questions in the design of good test sets and the evaluation of performance of existing algorithms. The test sets should also facilitate algorithm selection for applications by characterizing subproblems and the behavior of competitive algorithms on these subproblems and encourage the development of high-quality implementations of advanced algorithms and data structures. 1 Background and Motivation Experimental work is a necessary, and a growing, part of the process of algorithm development. As recently as f?fteen years ago, however, experimental research was rare and not well regarded within the algorithm community. The lack of experimental work eventually resulted in the creation of many “paper layers” in the description of a new algorithm-layers of unimplemented previous results upon which the new result relied-and in the opening of a large gap between the research work in academic circles and the tools actually used in industry. Over the last ten years, we have seen a return to the idea of testing theoretical ideas by implementing them, along with a welcome narrowing in the gap between academic research and industrial practice. What remains lacking in many areas, however, is a well-developed experimental methodology, comparable to the methodole gies developed in the natural sciences. As a major step in that direction, the algorithm community needs an V Research Institute, 4 Independence Way, Princeton, NJ 08540, avgWesearck.nj .nec. corn *Department of Computer Science, University of New Mexico, Albuquerque, NM 87131, moretks .unm. edu Bernard M.E. Morett organized collection of data sets, both synthetic and derived fcom applications, with which to test its ideas. (Consider the speech recognition area: in the years since ARPA set up a research group whose main function has been to produce test sets and analyze and validate results from other groups, the area has seen very significant progress, following upon a prolonged period of stagnation.) A well-organized collection of data sets stimulates research by providing a basis for assessment, by ensuring reproducibility, and by encouraging industry to provide real-world data sets. The resulting codes have well-understood practical performance and thus readily transfer to applications, in contrast to theoretical results of unknown practical sign&ance. Such a collection greatly facilitates the choice of a particular algorithm or implementation for a specific application. This evolution will narrow the gap between theory and practice, in the process uncovering further algorithmic prob lems of interest in both applied and theoretical areas. Such a collection must include both real-world and synthetic data: the former show what can be done in the context of an application and the latter enable researchers to assess detailed characteristics of the algorithms. Such a collection cannot be static: as hardware, algorithms, and applications evolve, typical or demanding problem instances will change. (This evolution is the reason why static collections, such as The Stanford GrcrphBue, while remaining good starting points, cannot fulf3l the roles described above.) The collection should also include pointers to papers describing relevant experimental work and to state-ofthe-art codes.