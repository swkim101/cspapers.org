Center-embedding is difÔ¨Åcult to process and is known as a rare syntactic construction across languages. In this paper we describe a method to incorporate this assumption into the grammar induction tasks by restricting the search space of a model to trees with limited center-embedding. The key idea is the tabulation of left-corner parsing, which captures the degree of center-embedding of a parse via its stack depth. We apply the technique to learning of famous generative model, the dependency model with valence (Klein and Man-ning, 2004). Cross-linguistic experiments on Universal Dependencies show that often our method boosts the performance from the base-line, and competes with the current state-of-the-art model in a number of languages.