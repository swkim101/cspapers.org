A ubiquitous problem in robotics is determining policies that move robots with uncertain process and observation models (partially-observed state systems) to a goal configuration while avoiding collision. We propose a new method to solve this minimum uncertainty navigation problem. We use a continuous partially-observable Markov decision process (POMDP) model and optimize an objective function that considers both probability of collision and uncertainty at the goal position. By using information-theoretic heuristics, we are able to find policies that are effective for both minimizing collisions and stopping near the goal configuration. We additionally introduce a filtering algorithm that tracks collision free trajectories and estimates the probability of collision.