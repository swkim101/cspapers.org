Sensor networks allow continuous data collection on unprecedented scales. The primary limiting factor of such networks is energy, of which communication is the dominant consumer. The default strategy of nodes continually reporting their data to the root results in too much messaging. Suppression stands to greatly alleviate this problem. The simplest such scheme is temporal suppression, in which a node transmits its reading only when it has changed beyond some e since last transmitted. In the absence of a report, the root can infer that the value remains within Â±e hence, it is still able to derive the history of readings produced at the node. 
 
The critical weakness of suppression is message failure, to which sensor networks are particularly vulnerable. Failure creates ambiguity: a non-report may either be a suppression or a failure. Inferring the correct values for missing data and learning the parameters of the underlying process model become quite challenging. We propose a novel solution, BaySail, that incorporates the knowledge of the suppression scheme and application-level redundancy in Bayesian inference. We investigate several redundancy schemes and evaluate them in terms of in-network transmission costs and out-of-network inference efficacy, and the trade-off between these. Our experimental evaluation shows application-level redundancy outperforms retransmissions and basic sampling in both cost and accuracy of inference. The BaySail framework shows suppression schemes are generally effective for data collection, despite the presence of failures.