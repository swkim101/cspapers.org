Accurate data dependence analysis is the key function in vectorizing-restructuring compilers for supercomputers. However, the data dependence analysis algorithms currently available have limitations. Those that execute quickly, such as the Banerjee Test [3], [4], [21] are very limited in generality. Those that are general are too slow. 1 Compiler designers have been keenly aware of the need for an algorithm that is both general and fast. The algorithm that follows fills this need simply and uniformly.
Aside from fast execution, this algorithm has three main features: It can deal with arbitrary linear constraints whose variables are not limited to loop-control variables
It can deal with any number of these linear constraints simultaneously.
It only looks at integer solutions.

The algorithm organizes the multiple constraints into a constraint matrix and uses a method derived from linear programming techniques. We refer to this as the Constraint-Matrix algorithm. Burke and Cytron [6] have discussed linearization to deal with arrays of greater than one dimension. This approach was also referred to by Towle [18]. Linearization is limited unless additional constraints are added; it cannot deal with arbitrary additional constraints and it does not restrict its solutions to integers.
The Constraint-Matrix algorithm is organized specifically for a goal that is different from the standard linear programming problem. This goal is to prove the lack of a solution [i.e. the lack of a dependence) rather than minimize an “objective function.” further, this algorithm is designed to “short-circuit” when its goal is reached, in a way that a standard linear programming algorithm cannot.
In order to simplify the exposition, we begin with a precise definition of dependence. To provide some motivation for the later algorithm and to show the advantages of the matrix format, we then present two special case methods (Solvable-Matrix and Matrix-GCD). Finally we present the Constraint-Matrix algorithm.