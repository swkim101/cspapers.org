The authors address the problem of choosing synaptic weights in a recursive (Hopfield) neural network so as to “optimize�? the performance of the network on the recognition of binary strings. The problem has been called the net loading (or learning) problem in the literature. The objective is to maximize the basins of attraction around the desired fixed points (binary strings) of the net. It is known that it is NP-hard to evaluate even the two-step radius of attraction of a recursive neural net. They focus on the radius of direct (one-step) attraction and refer to this as the loading problem. They have both theoretical and computational results on this problem: a proof that the net loading problem can be solved in polynomial time using linear programming techniques. This resolves a standing problem in the complexity of recursive neural networks; an alternate formulation of the net loading problem as a proximity problem in high-dimensional convex geometry; the design and implementation of a hybrid algorithm for the said proximity problem; successful solution of large scale test problems including the optimal solution to a 900×900 Hopfield net with approximately $4X10^5$ synaptic weights. It may be noted that the experiments indicate that the radius of direct attraction is actually a very good proxy of the intractable (multi-step) radius of attraction. In all the test problems that they have solved, the synaptic weights obtained as a a solution to the maximum radius of direct attraction also maximize the radius of (multi-step) attraction