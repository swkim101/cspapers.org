We investigate the randomness requirements of the classical rumor spreading problem on fully connected graphs with <i>n</i> vertices. In the standard random protocol, where each node that knows the rumor sends it to a randomly chosen neighbor in every round, each node needs <i>O</i>((log <i>n</i>)<sup>2</sup>) random bits in order to spread the rumor in <i>O</i>(log <i>n</i>) rounds with high probability (w.h.p.). For the simple quasirandom rumor spreading protocol proposed by Doerr, Friedrich, and Sauerwald (2008), [log <i>n</i>] random bits per node are sufficient. A lower bound by Doerr and Fouz (2009) shows that this is asymptotically tight for a slightly more general class of protocols, the so-called <i>gate-model</i>.
 In this paper, we consider general rumor spreading protocols. We provide a simple push-protocol that requires only a total of <i>O</i>(<i>n</i> log log <i>n</i>) random bits (i.e., on average <i>O</i>(log log <i>n</i>) bits per node) in order to spread the rumor in <i>O</i>(log <i>n</i>) rounds w.h.p. We also investigate the theoretical minimal randomness requirements of efficient rumor spreading. We prove the existence of a (non-uniform) push-protocol for which a <i>total</i> of 2 log <i>n</i> + log log <i>n</i> + <i>o</i>(log log <i>n</i>) random bits suffice to spread the rumor in log <i>n</i> + ln <i>n + O</i>(1) rounds with probability 1 − <i>o</i>(1). This is contrasted by a simple time-randomness tradeoff for the class of all rumor spreading protocols, according to which any protocol that uses log <i>n</i> − log log <i>n</i> − ω(1) random bits requires ω(log <i>n</i>) rounds to spread the rumor.