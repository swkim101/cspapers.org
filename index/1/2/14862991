The generation of motion for robots and mobile manipulators in unstructured, dynamic environments has been a key research topic in the last years. Nevertheless, due to the high complexity of the problem, a ultimate solution has not been defined and the issue is far to be solved. In order to tackle uncertainties and unexpected events, several methods and diverse scientific communities have contributed to the development of artificial cognitive systems that aim at working in unknown, dynamic environments. Such methods include motion planning, machine learning, and perception-based control. The paper wants to briefly recall those heterogenous techniques and to propose a concept that embeds learning methods into a kinematic control framework. I. TAMING THE UNCERTAINTIES: A BRIEF SURVEY This work provides a general idea of some important methodologies adopted to tackle uncertainties in unstructured environments and proposes a direction for embedding learning capabilities into a wider framework based on null-spacebased multiple behaviors. 1) Reactive planning: When the goal of the robotic system is a known trajectory or a known pose in the task space but the environment is affected by unexpected events, local reactive planners can represent a feasible solution. Such methods are suitable when the preplanned trajectories may need local corrections around the nominal values in order to handle external, unexpected events like obstacles in the workspace or inaccurate position of scene elements. The first category of local planners is based on artificial potentials in the task and configuration space [1]. This method associates an attractive potential to the desired state and a repulsive potential to obstacles. Similar approaches applied to entire trajectories in the configuration space and in the task space are [2] and [3] respectively. More recent approaches are specifically designed in order to exploit in a straightforward way the redundancy of modern robots like mobile manipulators. A requirement for modern low-level planners is the capacity of combining several tasks potentially with different priorities. In [4], methods to combine different behaviors in the task space are presented. Another recent approach is presented in [5], which combines tasks with different priorities in the configuration space and adopts a smoothing techniques to avoid chattering during task transition. Modelbased, local planner methods are adopted for self-collision avoidance in highly redundant manipulators [6]. The concept of flexible planning has been introduced also in force trajectories [7][8]. Local, reactive planners are subject to local 1The authors are with the Department of Electrical and Computer Engineering, Technische Universität München (TUM). E-mail: pietro.falco@tum.de(P. Falco), dhlee@tum.de (D. Lee) minima and they need an initial-guess trajectory to work effectively, since the correction will be performed around the nominal trajectory. As a consequence, to work in real environments, those methods need to be somehow combined with classical global planners or with machine learning (ML) techniques. 2) Imitation Learning: Imitation learning techniques, called also Programming dy Demonstration (PbD) techniques, allow robots to define a mapping between the state of the world and the action to perform by observing humans. This association is called policy. The policy can be learned at different levels of abstraction: it can consists in lowlevel trajectories for motion control, basic high-level actions (called primitives), and complex high level actions. An excellent survey on imitation learning can be found in [9]. Even if the PbD community has accomplished significant results, there are several open issues when dealing with tasks in unstructured environments. It is difficult, in fact, to adapt the learned policy to unexpected events that can be counteracted only with high-rate perception and reaction capabilities. Also, when possible unexpected changes in the environment bring the robot in states completely uncovered by the teacher’s examples, new demonstrations are required to complete the task. As a consequence, the possibility can be deeply investigated to combine leaning process with classical global and local planning approaches [10]. 3) Reinforcement Learning and Adaptive Optimal Control: In the Reinforcement Learning (RL) framework [11], the association between the robot state, the environment state and the action to perform is learned through the experience collected by the robotic system, called also agent, acting in the world. The robot explores possible strategies and it receives a feedback on the outcome of each action. The action is evaluated according to a reward function. Adopting this method, the robot can keep learning during its whole life cycle. Reinforcement Leaning and Optimal Control are strictly interconnected. In [12] it is shown how RL can be seen as an adaptive optimal control problem. Nevertheless, in unstructured environments several problems occur that limit the effectiveness of RL and optimal control. First of all, the definition of the reward function, defined as reward shaping, requires a significant manual contribution and expertise in the application domain [13]. As remarked in [11], the problem of extracting reward functions from the data has not been solved yet. Moreover, the problem remains of integrating the RL methods with the perception and reaction to sudden, unexpected events. II. IS A UNIFYING THEORY FEASIBLE? The question arises [14] if it is possible to find a unifying theory in robotics motion planning and generation. It has the potential to combine the flexibility of machine learning and the fast reactivity of kinematic control methods. A first step can be to develop a framework that allows a robot to refine on-line the policy and, at the same time, to quickly react to unexpected events through predictable, local corrections of motion/force trajectories. For example, when a robot is learning the execution of a task in an industrial or a general anthropic environment, the reaction to unexpected obstacles, even in presence of stochastic ML methods, has to be deterministic and independent from the particular task and reward function. To discuss in more detail a possible direction to follow, let us consider the following discretetime system that can model a robot provided with joint-level controllers: