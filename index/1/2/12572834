The performance of machine learning algorithms is known to be impaired if the representation of the individual classes in the training set is imbalanced, i.e., one class significantly outnumbering the other class(es). Several approaches to deal with this problem have been developed, none of them totally satisfactory. This paper proposes membership-based minority oversampling (MeMO), as yet another possible solution, and explores, experimentally, the conditions under which it outperforms earlier attempts.