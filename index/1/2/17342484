A pilot system for performing real-time brain imaging in naturalistic environments have been developed using wireless EEG headsets, motion sensors, smart telephones and ubiquitous computing servers. This paper described its pervasive architecture and introduced its enabling technologies, which include machine-to-machine publish/subscribe protocols, interoperable data/meta-data formats, multi-tier fog/cloud computing infrastructure and semantic linked data web. A live demonstration of this system was first performed at the US Army Research Lab meeting in March 2013. An expanded system capable of supporting real-time brain state classification and continuous model calibration will soon be made available as a web services.