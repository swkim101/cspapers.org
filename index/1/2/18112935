Simplicity of linear representations makes them a popular tool; in image analysis, the two widely used linear representations are: (i) linear projections of images to low-dimensional Euclidean subspaces, and (ii) linear spectral filtering of images. In view of the orthogonality and other constraints imposed on these representations (the subspaces or the filters), they take values on nonlinear manifolds (Grassmann, Stiefel, or rotation group). We use a family of stochastic algorithms that exploit the geometry of the underlying manifolds to find optimal linear representations for specified tasks. As applications, we demonstrate the effectiveness of algorithms by finding subspaces with optimal generalization through separation maximization and filters that are both sparse and effective for recognition. We also show empirically how the learned representations can improve the performance of support vector machines.