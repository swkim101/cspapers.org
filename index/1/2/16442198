An essential feature for personal robots in domestic environments is the quality of its interaction. This work describes a Contextually Informed MultiModal Integrator, CIMMI, that fuses speech and symbolic gestures probabilistically and is informed by contextual knowledge in an assistive technology robotic application. Symbolic gestures are gestures that have semantic meaning, such as a wave gesture meaning ‘hello’. Contextual knowledge is used to resolve ambiguities associated with object and location words in a command, where it is defined as both conversational and situational. Conversational contextual knowledge uses the dialogue history to resolve ambiguities in the selected command. Situational contextual knowledge consists of the last known locations of the user, the robot and a list of objects that exist in the environment. The accuracy of the speech recognition system alone (53%) was compared to using the speech with contextual knowledge, both conversational and situational, increasing the accuracy of the system to 72%. The accuracy of the system increased again to 76% with the addition of gestures as well.