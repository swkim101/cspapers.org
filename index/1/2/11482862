The notion of <italic>private approximation</italic> was introduced recently by Feigenbaum, Fong, Strauss and Wright. Informally, a private approximation of a function <italic>f</italic> is another function <italic>F</italic> that approximates <italic>f</italic> in the usual sense, but does not yield any information on <italic>x</italic> other than what can be deduced from <italic>f(x)</italic>. As such, <italic>F(x)</italic> is useful for private computation of <italic>f(x)</italic> (assuming that <italic>F</italic> can be computed more efficiently than <italic>f</italic>.
In this work we examine the properties and limitations of this new notion. Specifically, we show that for many NP-hard problems, the privacy requirement precludes non-trivial approximation. This is the  case even for problems that otherwise admit very good approximation (e.g., problems with PTAS). On the other hand, we show that slightly relaxing the privacy requirement, by means of leaking â€œjust a few bits of informationrdquo; about <italic>x</italic>, again permits good approximation.