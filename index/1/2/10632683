Writing meaningful software tests requires students to think critically about a problem and consider a variety of cases that might break the solution code. Consequently, to overcome bugs in their code, it would be beneficial for students to reflect over their work and write robust tests rather than relying on trial-and-error techniques. Automated grading systems provide students with prompt feedback on their programming assignments and may help them identify where their interpretation of requirements do not match the instructor's expectations. However, when automated grading systems help students identify bugs in their code, the systems may inadvertently discourage students from thinking critically and testing thoroughly and instead encourage dependence on the instructor's tests. In this paper, we explain a framework for identifying whether a student has adequately tested a specific feature of their code that is failing an instructor's tests. Using an implementation of the framework, we analyzed an automated grading system's feedback for programming assignments and found that it often provided hints that may discourage reflective testing.