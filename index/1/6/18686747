In this paper, we explore the use of Random Forests (RFs) (Amit and Geman, 1997; Breiman, 2001) in language modeling, the problem of predicting the next word based on words already seen before. The goal in this work is to develop a new language modeling approach based on randomly grown Decision Trees (DTs) and apply it to automatic speech recognition. We study our RF approach in the context of (cid:2) -gram type language modeling. Unlike regular (cid:2) -gram language models, RF language models have the potential to generalize well to unseen data, even when a complicated history is used. We show that our RF language models are superior to regular (cid:2) -gram language models in reducing both the per-plexity (PPL) and word error rate (WER) in a large vocabulary speech recognition system.