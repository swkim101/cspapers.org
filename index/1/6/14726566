Multi-view algorithms reduce the amount of required training data by partitioning the domain features into separate subsets or views that are sufficient to learn the target concept. Such algorithms rely on the assumption that the views are sufficiently compatible for multi-view learning (i.e., most examples are labeled identically in all views). In practice, it is unclear whether or not two views are sufficiently compatible for solving a new, unseen learning task. In order to cope with this problem, we introduce a view validation algorithm: given a learning task, the algorithm predicts whether or not the views are sufficiently compatible for solving that particular task. We use information acquired while solving several exemplar learning tasks to train a classifier that discriminates between the tasks for which the views are sufficiently and insufficiently compatible for multi-view learning. Our experiments on wrapper induction and text classification show that view validation requires only a modest amount of training data to make high accuracy predictions.