Deep convolutional neural networks are being increasingly deployed for image classification tasks as they can learn sensor and environmental independence from large quantities of training data. Most, however, have focused on classifying uploaded photographs rather than the often occluded, arbitrary height and camera angles images found commonly in robotic applications. In this work, we look at the performance of the popular AlexNet architecture to detect people in different robotic scenarios using different sensors and/or environments. Furthermore, we demonstrate how fusing this architecture with the depth-based layered detection system, a more traditional geometric feature-based classifier, leads to significant improvements in classification precision/recall, whether working with depth data alone or a combination of depth and RGB images.