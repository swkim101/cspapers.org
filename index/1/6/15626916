Imitation learning of tasks in multi-component robotic systems requires capturing concurrency and synchronization requirements in addition to task structure. Learning time-critical tasks depends furthermore on the ability to model temporal elements in demonstrations. This paper proposes a modeling framework based on Petri nets capable of modeling these aspects in a programming by demonstration context. In the proposed approach, models of tasks are constructed from segmented demonstrations as task Petri nets, which can be executed as discrete controllers for reproduction. We present algorithms that automatically construct models from demonstrations, showing how elements of time-critical tasks can be mapped into task Petri net elements. The approach is validated by an experiment in which a robot plays a musical passage on a keyboard.