For two layer networks with n sigmoidal hidden units, the generalization error is shown to be bounded by O(E 1/K) + O((EK)d/N log N), where d and N are the input dimension and the number of training samples, respectively. E represents the expectation on random number K of hidden units (1 ≤ K ≤ n). The probability Pr(K = k) (1 ≤ k ≤ n) is determined by a prior distribution of weights, which corresponds to a Gibbs distribution of a regularizer. This relationship makes it possible to characterize explicitly how a regularization term affects bias/variance of networks. The bound can be obtained analytically for a large class of commonly used priors. It can also be applied to estimate the expected network complexity EK in practice. The result provides a quantitative explanation on how large networks can generalize well.