We propose a natural, mathematically tractable model of TCP which captures both its additive-increase, multiplicative-decrease behavior and its feedback mechanism. Neither a fluid nor a mean-field model, our model does not explicitly model the loss process; the losses are entirely determined by the rates of the sources at the time of buffer overflow. The system involves two sources competing to send packets into one recipient buffer of size B, from which bytes are drained at the rate of d per step. We prove that for many choices of the pairs (B,d), the long term behavior of the system is fractal. We conjecture that this fact continues to hold for all B > d and d > 2.