A fundamental problem in the study of sensor networks is collecting data to a central server from a set of <i>k</i> distributed sensor nodes. A considerable amount of recent research in this area attempts to reduce the number of bits sent by the nodes by taking advantage of correlations between the data items collected from different nodes. In this paper, we study this problem in the following model: let <i>D</i> be a probability distribution over <i>k</i> binary strings of length <i>n.</i> A sample <i>&Xmacr;</i> is drawn from <i>D</i> and the <i>k</i> strings of <i>&Xmacr;</i> are revealed to the nodes, one string per node. The goal is to inform the server of all <i>k</i> strings of <i>&Xmacr;</i>. Our primary objective is to minimize the total number of bits sent by the nodes, but we also seek to minimize both the bits sent by the server and the number of rounds required. This problem is a natural parallelization of the model introduced in [2], and is also well motivated by recent work on distributed source coding for sensor networks. Our main result is a protocol that allows the server to correctly determine <i>&Xmacr;</i>. In this protocol, the nodes send <i>O</i>(<i>H</i>(<i>D</i>) + <i>k</i>) bits in expectation, where <i>H</i>(<i>D</i>) is the binary entropy of <i>D;</i> this is asymptotically optimal. The server sends <i>O</i>(<i>kn</i> + <i>H</i>(<i>D</i>) log <i>n</i>) bits in expectation, and the number of rounds is <i>O</i>(1 + log[<i>H</i>(<i>D</i>)]) in expectation. We also demonstrate that if the server is allowed to produce an incorrect result with probability up to Δ, then the expected number of bits sent by the server can be reduced to <i>O</i>(<i>k</i> log(<i>nk</i>/Δ) + <i>H</i>(<i>D</i>) log <i>n</i>), without increasing the other measures of performance.