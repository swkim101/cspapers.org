Optimizing on-chip primary data caches for parallel scientific applications is challenging because different applications exhibit different behavior. Indeed, while some applications exhibit good spatial locality, others have accesses with long strides that prevent the effective use of cache lines. Finally, other applications cannot exploit long lines because they exhibit false sharing. To help processors execute these three types of apphcatlons efficiently, we introduce the Pool Bu~er, a small direct-mapped cache accessed in parallel with the primary cache. The function of the pool buffer is to fetch long sectors of relatively short cache lines from memory on a miss, while only letting into the cache the lines that the processor actually references. The pool buffer can also perform sequential prefetching of sectors. An evaluation of the pool buffer based on simulations of five 32-processor Perfect Club codes yields encouraging result s Adding a pool buffer of one-quarter the size of the cache causes a small increase in area while usually achieving large reductions in execution time. For example, for a range of caches with 32-byte lines. the execution time decreases by an average of about 20~c. We also show that small l-Kbvte buffers are often large enough to get most of the poten~ial benefits. Finally, caches with pool buffers are more effective than caches with long lines and no pool buffer.