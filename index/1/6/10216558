This work presents some experiments of a real-time navigation system driven by two cameras pointed laterally to the navigation direction (divergent stereo). The approach is based on the observation that the stereo set-up traditionally used in vision (i.e. with the optical axis pointing forward) may not be the best one for navigation, and particularly for continuous control of a mobile actor moving in unconstrained environment. For navigation purposes, the driving information is not distance (as it is obtainable by a stereo set-up) but motion and, more precisely, by optical flow information computed over different areas of the visual field. Following this idea, a mobile vehicle has been equipped with a pair of cameras looking laterally (much like honeybees) and a controller based on fast, real-time computation of optical flow, has been implemented. The control of the mobile robot (ROBEE) is based on the comparison between the apparent image speed of the left and the right eye. A detailed description of the control structure is presented to demonstrate the feasibility of the approach in driving the mobile robot within a very cluttered environment. A discussion about the potentialities of the approach and the implications in terms of sensor's structure is also presented.