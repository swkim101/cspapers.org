Kearns et al. [18] de(cid:12)ned two notions for learning a distribution D. The (cid:12)rst is with generator, where the learner presents a generator that outputs a distribution identical or close to D. The other is with an evaluator, where the learner presents a procedure that on input x evaluates correctly (or approximates) the probability that x is generated by D. They showed an example where e(cid:14)cient learning by a generator is possible, but learning by an evaluator is computationally infeasible. Though it may seem that generation is, in general, easier than evaluation, in this paper we show that the converse may be true: we provide a class of distributions where e(cid:14)cient learning with an evaluator is possible, but coming up with a generator that approximates the given distribution is infeasible. We also show that some distributions may be learned (with either a generator or an evaluator) to within any (cid:15) > 0, but the learned hypothesis must be of size proportional to 1=(cid:15). This is in contrast to the distribution-free PAC model where the size of the hypothesis can always be proportional to log1=(cid:15). (cid:3)