Experimental evaluation plays a critical role in driving progress in information retrieval (IR) today. There are few alternative ways of exploring, in depth, the empirical merits (or lack thereof) of newly devised search techniques. Careful evaluation is necessary for advancing the state-of-theart; yet, many published papers present work that was illevaluated. Indeed, this phenomenon has garnered attention from the community recently, after the publication of a controversial, but eye-opening, study by Armstrong et al. that suggested ad hoc search quality has not meaningfully advanced since 1984 [1]. The authors noted that the root of the problem was generally lax evaluation methodologies (e.g., weak baselines, etc.). Furthermore, many submissions to top IR research venues (e.g., SIGIR, CIKM, ECIR, WSDM, WWW, etc.) are rejected primarily due to insufficient or inappropriate evaluation. There is therefore a strong need to educate students, researchers, and practitioners about the proper way to carry out IR experiments. This is unfortunately not something that is taught in IR courses or covered in IR textbooks. Indeed, to the best of our knowledge, there is very little written work that lays down some principles for running an IR experiment [3]. More specifically, there have not been any recent tutorials or written works that have specifically, and comprehensively, addressed the question of “how to run an IR experiment” in terms of effectiveness evaluation. This has potentially yielded a number of detrimental effects, as described above. The goal of the tutorial is to provide an initial set of training material for researchers interested in rigorous evaluations of information retrieval systems. Although the primary focus is on ad hoc retrieval experiments, the principles and concepts described in the tutorial are general and can easily be applied to a wide range of experimental scenarios both within, and beyond, the field of information retrieval. The tutorial is primarily inteded for graduate students, re-