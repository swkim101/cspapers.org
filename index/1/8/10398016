Since the 1970s AI as a science has progressively fragmented into many activities that are very narrowly focused. It is not clear that work done within these fragments can be combined in the design of a human-like integrated system â€“ long held as one of the goals of AI as science. A strategy is proposed for reintegrating AI based around a backward-chaining analysis to produce a roadmap with partially ordered milestones, based on detailed scenarios, that everyone can agree are worth achieving, even when they disagree about means. The Fragmentation of AI As numbers of AI researchers grew and more and more problems were seen to have deep sub-problems, the field naturally fragmented, with people working on more narrowlyfocused problems as the years progressed. Individual researchers found it hard enough to keep up with the latest developments in their own area, without also trying to keep up with what was happening in other subfields. So they formed sub-communities studying their own problems, using their own methods, going to their own conferences, publishing their own journals etc. Moreover, most people came into AI only as graduates in another discipline so they had to pick up an accelerated education to get to PhD level, and often that meant reading only things their supervisor read. A further source of fragmentation arose from the fact that extravagant predictions for AI had been made, which had not been fulfilled. When the optimism proved ill founded, the reason for lack of success was repeatedly mis-diagnosed as being due to the wrong tools, the wrong forms of representation, the wrong mechanisms. So waves of fashion surged and ebbed, focusing on alternative tools, representations, mechanisms, etc., for instance, connectionism, evolutionary computation, dynamical systems, situated cognition, use of reactive systems, and probably more. This led to more fragmentation and mutual ignorance between factions.