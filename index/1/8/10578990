AdaCost, a variant of AdaBoost, is a misclas-siﬁcation cost-sensitive boosting method. It uses the cost of misclassiﬁcations to update the training distribution on successive boosting rounds. The purpose is to reduce the cumulative misclassiﬁcation cost more than AdaBoost. We formally show that AdaCost reduces the upper bound of cumulative mis-classiﬁcation cost of the training set. Empirical evaluations have shown signiﬁcant reduction in the cumulative misclassiﬁcation cost over AdaBoost without consuming additional computing power.