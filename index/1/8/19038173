I entered the field of computational linguistics in 1967 and one of my earliest recollections is of studying the Harvard Syntactic Analyzer. To this date, this parser is one of the best documented programs and the extensive discussions cover a wide range of English syntax. It is sobering to recall that this analyzer was implemented on an IBM 7090 computer using 32K words of memory with tape as its mass storage medium. A great deal of attention was focussed on means to deal with the main memory and mass storage limitations. It is also interesting to reflect back on the decision made in the Harvard Syntactic Analyzer to use a large number of parts of speech, presumably, to aid the refinement of the analysis. Unfortunately, this introduction of such a large number of parts of speech (approximately 300) led to a large number of unanticipated ambiguous parsings, rather than cutting down on the number of legitimate parsings as had been hoped for. This analyzer functioned at a time when revelations about the amount of inherent ambiguity in English (and other natural languages) was a relatively new thing and the Harvard Analyzer produced all possible parsings for a given sentence. At that time, some effort was focused on discovering a use for all these different parsings and I can recall that one such application was the parsing of the Geneva Nuclear Convention. By displaying the large number of possible interpretations of the sentence, it was in fact possible to flush out possible misinterpretations of the document and I believe that some editing was performed in order to remove these ambiguities.