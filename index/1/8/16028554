In this paper we present a word decom-pounding method that is based on distributional semantics. Our method does not require any linguistic knowledge and is initialized using a large monolingual corpus. The core idea of our approach is that parts of compounds (like “candle” and “stick”) are semantically similar to the entire compound, which helps to exclude spurious splits (like “candles” and “tick”). We report results for German and Dutch: For German, our unsupervised method comes on par with the performance of a rule-based and a supervised method and sig-niﬁcantly outperforms two unsupervised base-lines. For Dutch, our method performs only slightly below a rule-based optimized compound splitter.