The ℓ<sub><i>p</i></sub> <i>regression problem</i> takes as input a matrix <i>A</i> ∈ ℝ<sup><i>n</i></sup>, a vector <i>b</i> ∈ ℝ<i><sup>n</sup></i>, and a number <i>p</i> ∈ [1, ∞), and it returns as output a number Z and a vector <i>x</i><sub>OPT</sub> ∈ ℝ<sup>d</sup> such that Z = min<sub><i>x</i>∈ℝ<i>d</i></sub> ||<i>Ax</i> - <i>b</i>||<i><sub>p</sub></i> = ||<i>Ax</i><sub>OPT</sub> - <i>b</i>||<i><sub>p</sub></i>. In this paper, we construct coresets and obtain an efficient two-stage sampling-based approximation algorithm for the very overconstrained (<i>n</i> ≫ <i>d</i>) version of this classical problem, for all <i>p</i> ∈ [1, ∞). The first stage of our algorithm non-uniformly samples <i>&rcirc;</i>1 = <i>O</i>(36<i><sup>p</sup></i><i>d</i><sup>max</sup>{<i>p</i>/2+1, <i>p</i>}+1) rows of <i>A</i> and the corresponding elements of <i>b</i>, and then it solves the <i>l<sub>p</sub></i> regression problem on the sample; we prove this is an 8-approximation. The second stage of our algorithm uses the output of the first stage to resample <i>&rcirc;</i>1/ε<sup>2</sup> constraints, and then it solves the <i>l<sub>p</sub></i> regression problem on the new sample; we prove this is a (1 + ε)-approximation. Our algorithm unifies, improves upon, and extends the existing algorithms for special cases of ℓ<sub><i>p</i></sub> regression, namely <i>p</i> = 1,2 [10, 13]. In course of proving our result, we develop two concepts--well-conditioned bases and subspace-preserving sampling--that are of independent interest.