Given a program P in a structured programming language, we propose the first polynomial-time algorithm for compiletime memory allocation for the source objects of P (eg arrays, C structures) with a performance guarantee. Further, we present a new and simple O(n log n) time bapproximation algorithm for (off-line) Dynamic Storage Allocation and, thus, improve the best previous approximation ratio of 5. 1 Problem Motivation and Definition Consider the following simple fragment of a C function: foo(int i) { char A[40]; intB[lO];if(i){...}else{... } }. The dots in the i !=O (resp. i==O) branch of the if statement denote a C block without read or write access to the array B (resp. array A), ie either only A or only B is needed at run time. Assume that an int (char) is represented by 4 (1) bytes. Now, let us slightly simplify the compile-time memory allocation task and ask: how much storage does the compiler need to allocate for the arrays A and B ? First, the compiler can use two nonoverlapping storage areas for A and B, ie 40 + 10.4 = 80 bytes. Alternatively, the compiler could find out that A and B will never “exist” at the same time when the program is running. Hence, a storage area of 40 bytes is sufficient for both arrays, and the first solution can be improved by a factor of 2. In general, given a source program .the compiler can use control-flow analysis and similar techniques to determine pairs of (source-code) objects such that the compiler is allowed to map the objects in each pair to overlapping memory regions. Now, assume that we have derived the following information from a program in a structured programming language: (1) the set of all (source-code) objects and their memory requirements; (2) a set of pairs of objects such that the objects in a pair cannot “interfere” with each other at run time and, hence, can share memory; (3) the associated controlflow graph, and, for any object, the vertices of the control-flow graph (ie source-code statements) at which the object is accessed as well as the associated access type information. Then, the Compile-time Memory ‘Max-Planck Institut ftir Informatik, Im Stadtwald, 66 123 Saarbriicken, Germany; E-Mail: gergov@mpi-sb.mpg.de AZlocation problem (CMA) is to compute a mapping from objects to memory regions such that: (1’) the size of the memory region that is allocated to an object equals the memory requirement of that object; (2’) if two memory regions overlap, then the associated object pair is in the set of object pairs that are allowed by (2) to share memory; and (3’) the memory usage is minimized. The trend towards more complex embedded systems increases the interest in memory optimization tools, and CMA algorithms are also potentially useful in optimization of software with a high memory demand. The performance of all previous heuristics for this NPhard task can deviate significantly from optimality, cf [l]. In Section 3, we discuss a novel approach to fast CMA algorithms with a performance guarantee. Fabri [l] gives a more detailed treatment of the CMA definition as well as an account of previous research. One special case of CMA with an independent research history as well as with numerous applications is the off-line Dynamic Storage Allocation (DSA). Assume that we are given a straight-line program, ie its source code does not contain branch or loop statements and, for ease of presentation, no function calls. It turns out that in this case CMA has a nice geometric interpretation. A DSA input I consists of n triples of numbers, ie I= {(Sl,~l,C1)7-.-, (sn,m,c,.,)}. Each triple (s~,Q,c~) can be interpreted as an axis-parallel rectangle with a projection (ri,ci) on the z-axis and a projection of length si on the y-axis. We are only allowed to slide the rectangles along the y-axis while the s-projections of all rectangles stay fixed as in the input I. The objective is to pack all rectangles in a horizontal strip of minimum height. Intuitively speaking, (Si, Ti) Ci) is associated with a source-code object that is used only between the rith and the c&h statement and requires Si contiguous memory cells. The previous research on fast DSA algorithms with a performance guarantee was based on on-line coloring, and results were obtained independently by Woodall, Kierstead, Chrobak, and Slusarek, cf [2] for references. The best approximation ratio of 5 for this NP-hard problem is based on a different approach and is due to Gergov [2]. In the remaining sections, we give a rather condensed presentation of our results.