Relevance feedback (RF) aims to overcome query and document disagreements on vocabulary, and user and system disagreements on relevancy for a given information need. It can be implemented as an interactive (IRF) or automatic (pseudo-relevance feedback PRF) process and relies on the initial document ranking returned by the system for the original query. The underlying assumption is that the initial retrieval will yield the relevant documents to refine the query[4, 7]. It is expected that they will provide new terms, such as synonyms, that will improve the original query. The feedback performance of retrieval systems depends on many factors and a great variability in precision occurs from topic to topic [1]. On average, the overall system performance improves after feedback, but for some topics the average precision achieved is actually lower than the initial run, particularly in PRF. This effect may be caused by the use of non-relevant documents (in the case of PRF) or due to the use of a single feedback parameter set for all topics [2]. In this work we present a new alternative explanation for lowered topic performance: under specific feedback parameter settings, relevant documents may act as “poison pills”[9] and harm topic precision after feedback.