The social interaction, guidance and support that a socially assistive robot can provide a person can be very beneficial to patient-centered care. However, there are a number of conundrums that must be addressed in designing such a robot. This work addresses two main limitations in the development of intelligent task-driven socially assistive robots: (i) recognition and identification of human gesticulation as a source of determining the affective state of a person, and (ii) robotic control architecture design and implementation with explicit social and assistive task functionalities. In this paper, the development of a unique task-driven robotic system capable of quantitatively interpreting human body language and in turn, effectively responding via task-driven behavior during assistive social interaction is presented. In particular, a novel gesture identification and classification technique is proposed capable of interpreting human gestures as semantically meaningful commands for inputs into a multi-layer decision making control architecture. The learning-based control architecture is then utilized to determine the effective and appropriate assistive behavior of the robot.