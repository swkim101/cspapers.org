Information Theory is concerned with the reliable transmission of information through noisy environments. This relates to communicating agents, as well as one agent communicating with the environment by taking measurements (e.g., robotic sensing). Typically information theory is formulated in the context of probability on either discrete spaces or continuous Euclidean spaces in which the operation of addition makes sense. Some have extended information theory techniques to differential geometric settings. However, only in the context of group theory can the concept of addition be replaced in a meaningful way with a group operation. This paper presents concepts of information theory on Lie groups developed by the author, and illustrates their application to mobile robotics problems. In particular, the concepts of Shannon entropy, Kullback-Leibler divergence, the Cram√©r-Rao bound for pose data are developed, and some theorems about their properties are proved. It is also illustrated how these concepts might be integrated into pose estimation, localization, and odor-plume source detection.