The primary vehicle for demonstrating the performance of Ballistic Missile Defense (BMD) software is testing in a simulated environment. The complexity of BMD software logic and the large volumes of input data preclude exhaustive testing. The problem is compounded by limitations on testing time, complicated test procedures, and lack of systematic procedures for performance analysis. This paper describes an approach towards providing an effective means for identifying the boundary of performance of BMD software. The approach, called Adaptive Testing, uses performance data to systematically redefine input data that will produce some specified level of system performance. Components of the Adaptive Testing technology include an interactive scenario generator, a performance evaluator, an adaptive algorithm for test case perturbation, and a System, Environment, and Threat Simulator (SETS). Problems associated with the approach are discussed as well as the expected payoff.