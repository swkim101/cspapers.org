We propose a topology-based Bayesian visual navigation framework, with which we represent the world environment as a collection of scenes. The proposed topological map consists of nodes represented as a bag of visual line words and edges represented as both an adjacency list and relative motion information to perform the transition between topological nodes. Our proposed Bayesian localization framework uses two measurement models: a visual line word-based place model and a path-matching model. To enable a mobile robot to reach to a desired destination, a coastal path is planned in such a way that maximizes the possibility of following the reference path defined as a sequence of scenes encountered when building the topological map, while avoiding possible collisions by leveraging a local grid map constructed in real time from Kinect depth information. To show the validity of our proposed framework, we provide several extensive experimental results in several indoor environments under different conditions, such as illumination changes and visual occlusions.