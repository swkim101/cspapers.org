We propose the so-called TeleSensor programming concept that uses sensory perception to achieve local autonomy in robotic manipulation. Sensor based robot tasks are used to define elemental moves within a high level programming environment. This approach is applicable in both, the real robot's world and the simulated one. Beside the graphical off-line programming concept, the range of application lies especially in the field of teleoperation with large time delays. A shared autonomy concept is proposed that distributes intelligence between man and machine. The feasibility of graphically simulating the robot within its environment is extended by emulating different sensor functions to achieve a correct copy of the real system behaviour as far as possible. The programming paradigm is supported by a sophisticated graphical man machine interface. Sensor fusion aspects with respect to autonomous sensor controlled task execution are discussed as well as the interaction between the real and the simulated system.<<ETX>>