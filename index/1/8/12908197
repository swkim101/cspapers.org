In natural situations, we can obtain sensory information in different modalities such as visual, auditory and tactile from an object. The human-computer interface with a mouse can give us both visual and kinesthetic information. However, the tactile information that occurs when we touch an object in a natural situation is not available from a mouse. In psychophysical studies, it has been shown that tactile information added to visual information helps to increase the velocity of finger movements and reduces the dependence on vision [1]. It is, therefore, expected that an interface device capable of presenting tactile information would improve the efficiency of operation and reduce the visual load. Thus, we have developed a human-computer interface device based on a mouse that gives tactile and force sensations to the operator in addition to visual information. This interface device can be defined as a multi-modal integrative mouse.