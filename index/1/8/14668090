Building practical expert database systems requires an effective inferencing capability over large data sets. rnferencing in this context means repeatedly executing a fixed set of queries, interleaved with update transactions, until a fix:3 pint is reached. The effectiveness of ,.lircncing mechanism is heavily depcnde; !:on :he amount of state space needed and the ability of the underlying algorithms to avoid unnecessary work. Common techniques used in the design of rule-based systems store large amounts of state in order to derive precise query support information that will enable better performance. These techniques were intended for use in main memory on small data sets and are not ncccssarily suited for a database environment. When confronted with a large database these techniques may experience severe performance problems severe enough to render them useless. In this paper we examine the effects of database size on live test cases. The use of real programs with real data pr0vidc.s insights mat are not to be found through analysis and simulation. We compare two different rule systems, one based on the TREAT match algorithm and the other on LEAPS, a lazy matching algorithm. The results show that state can be a problem in rule systems and that by using lazy matching it is possible to eliminate some state while improving performance.