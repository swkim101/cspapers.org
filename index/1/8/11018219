The Johnson-Lindenstrauss transform is a dimensionality reduction technique with a wide range of applications to theoretical computer science. It is specified by a distribution over projection matrices from R<sup><i>n</i></sup> → <i>R</i><sup><i>k</i></sup> where <i>k</i> ≪ <i>d</i> and states that <i>k</i> = <i>O</i>(ε<sup>−2</sup> log 1/Δ) dimensions suffice to approximate the norm of any fixed vector in R<sup><i>d</i></sup> to within a factor of 1 ± ε with probability at least 1 − Δ. In this paper we show that this bound on <i>k</i> is optimal up to a constant factor, improving upon a previous Ω(ε<sup>−2</sup> log 1/Δ)/log(1/ε) dimension bound of Alon. Our techniques are based on lower bounding the information cost of a novel one-way communication game and yield the first space lower bounds in a data stream model that depend on the error probability Δ.
 For many streaming problems, the most naïve way of achieving error probability Δ is to first achieve constant probability, then take the median of <i>O</i>(log 1/Δ) independent repetitions. Our techniques show that for a wide range of problems this is in fact optimal! As an example, we show that estimating the <i>l</i><sub><i>p</i></sub>-distance for any <i>p</i> ∈ [0, 2] requires Ω(ε<sup>−2</sup> log <i>n</i> log 1/Δ) space, even for vectors in {0, 1}<sup><i>n</i></sup>. This is optimal in all parameters and closes a long line of work on this problem. We also show the number of distinct elements requires Ω(ε<sup>−2</sup> log 1/Δ + log <i>n</i>) space, which is optimal if ε<sup>−2</sup> = Ω(log <i>n</i>). We also improve previous lower bounds for entropy in the strict turnstile and general turnstile models by a multiplicative factor of Ω(log 1/Δ). Finally, we give an application to one-way communication complexity under product distributions, showing that unlike in the case of constant Δ, the VC-dimension does not characterize the complexity when Δ = <i>o</i>(1).