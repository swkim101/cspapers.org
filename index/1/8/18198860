Robot navigation systems are typically engineered to suit certain platforms, sensing suites and environment types. In order to deploy a robot in an environment where its existing navigation system is insuffiï¿½cient, the system must be modified manually, often at significant cost. In this paper we address this problem, proposing a system based on multimodal deep autoencoders that enables a robot to learn how to navigate by observing a dataset of sensor input and motor commands collected while being teleoperated by a human. Low-level features and cross modal correlations are learned and used in initialising two different architectures with three operating modes. During operation, these systems exploit the learned correlations in generating suitable control signals based only on the sensor information. In simulation these systems are shown to run indefinitely in simple environments, are capable of generalising to more complex environments and found to be robust to significant amounts of sensor noise. The system has been deployed on a real robot and operated in a simple corridor for over 30 minutes without collision.