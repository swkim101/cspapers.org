This paper examines the relationship between probability theory and reasoning in uncertainty, and argues that (contra opposing views) probability theory does have a place in reasoning in uncertainty, but that its place is more restricted than many of its advocates claim. Two major theses are argued for. (1) Reasoning from probabilities works well in domains which permit a clear analysis in terms of events over outcome spaces and for which either large bodies of evidence or long periods of "training" are available; but such domains are relatively rare, and even there, care must be taken in interpreting probability results. (2) Some generalizations with which AI applications must concern themselves are not statistical in nature, in the sense that statistical generalizations neither capture their meanings nor even preserve their truth values. For these contexts, different models will be needed.