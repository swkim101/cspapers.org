Scene modeling is an important first stage in visual robot localization. In recent years, the bag-of-words (BoW) scene modeling approach has attracted considerable attention as a method for obtaining compact discriminative scene descriptors for map retrieval. However, a BoW scene descriptor alone cannot address partial view changes and often produces poor localization in practice. In this work, we address this issue by proposing a simple effective approach, “unsupervised part-based scene modeling,” in which a set of useful parts is discovered via scene parsing and the parts are used as additional queries for the map retrieval. We also address the issue of discovering useful parts in a scene, and present a solution that provides similar parts for similar scenes. The next contribution of this work is that we present a practical robot self-localization system that consists of three distinct steps: (1) robust hierarchical scene parsing to obtain multiple scene/part queries, (2) saliency-based selection of useful parts, and (3) aggregation of ranking results from multiple scene/part queries to obtain a reliable ranking result. For rank aggregation, we consider multiple search engines for multiple part queries and adopt the idea of unsupervised rank fusion. Experimental results obtained using a challenging outdoor scene dataset show that our approach is an improvement over previous approaches despite the fact that we do not rely on domain-specific scene/part models nor supervision.