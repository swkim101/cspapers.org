This paper reports on a novel two-step algorithm for the estimation of full 6-degree-of-freedom (DOF) [t<inf>x</inf>, t<inf>y</inf>, t<inf>z</inf>, θ<inf>x</inf>, θ<inf>y</inf>, θ<inf>z</inf>] rigid body transformation between any two overlapping point-clouds that have a dominant ground plane. We first estimate the ground plane (X-Y plane) from the two 3D point-clouds and align them to obtain a good estimate of the distance between the ground planes (i.e. t<inf>z</inf>) and rotations θ<inf>x</inf> and θ<inf>y</inf> about the X and Y axis respectively using the Rodrigues rotation formula. The remaining parameters (t<inf>x</inf>, t<inf>y</inf>, θ<inf>z</inf>) are then estimated by maximizing the total mutual information (MI) between the 2D feature maps generated from the multi-modal sensor data. Experimental results using scans obtained by a vehicle equipped with a 3D laser scanner and an omnidirectional camera are used to validate the robustness of the proposed algorithm over a wide range of initial conditions. The proposed method provides an efficient framework for multi-modal sensor data fusion and provides a robust solution to the scan alignment problem.