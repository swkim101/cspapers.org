It is well-known that Kernel Based Regression (KBR) with a least squares loss has some undesirable properties from robustness point of view. KBR with more robust loss functions, e.g. Huber or logistic losses, often give rise to more complicated computations and optimization problems. In classical statistics, robustness is improved by reweighting the original estimate. We study reweighting the KBR estimate using four different weight functions. In addition, we show that both the smoother as well as the cross-validation procedure have to be robust in order to obtain a fully robust procedure.