This paper examines an instance of a collaborative perception problem across aerial and ground robots, specifically in the context of transferring learned appearance classifiers from one to the other. This problem is extremely challenging, as the two robots differ significantly in target resolution, pixels on target, and perspectives. We empirically explore a set of state of the art features used to distinguish the target and ask the question: What features are the most transferable between the aerial and ground robots for the purposes of target handoff, where a target must be distinguished from `confuser' objects? We show that while these features are successful on each individual robot, they are not able to be transferred naively. However, we show that using feature alignment techniques, where sparse features are mapped from one robot to the other using a shared context, we are able to successfully transfer classifiers that use color and texture features. We demonstrate these results on an extremely challenging outdoor data set simultaneously collected using an aerial vehicle and sensor-mounted ground vehicle viewing the same scene and target.