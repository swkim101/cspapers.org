generalized instructions from direct manipulation. The problem is how to get instructions from the use~ Should she write them out? Should the system infer them from her behavior? Should direct manipulation interfaces include memory icons, expression boxes and event labels? I view the demonstrational interface as “instructible,” combining inference and direct manipulation to disambiguate action — to identify the relevant constraints on selecting and inserting data. In this case, the problem (for users and researchers) is to find instructions that specify constraints. Pure demonstration is activity without explanation. Typically, a machine learning system must see several examples before converging on the right constraint (for instance, “VI .ps” and “v2.ps” imply “v#.ps”; add “wow.ps” and you get “*.ps”). A direct instruction, “select file names ending in ‘ .ps’ ‘‘, eliminates inference. A focusing instruction, selecting or typing” .ps”, makes “*.ps” one of the first guesses to be tried, focusing is crucial if the user expects automation after very few examples. Finally, augmenting a direct manipulation interface with memory icons provides additional means of instruction. The other important problem is to identify control structure: loops, branches and subroutines. Loops and subroutines are inferred by matching and predicting actions; branches, by finding differences in preconditions before bad predictions. Direct instruction of a loop or subroutine requixes some means of selecting a sequence of events from history. Focusing instructions indicate approximate ranges of history. Direct and focusing commands help identify constraints as branching conditions. Tuwy: Experiments in Demonstration Protocol Turvy is a ‘‘ Wizard of Oz’ ‘, an instructible system portrayed by a human being. I chose this approach for two reasons: first, I wanted to use real applications like Word, MacDraw and HyperCard, whose internal data structures cannot be accessed. Second, I wanted experimental subjects to set their own protocol for instructing; hence Turvy has to speak and recognize language. Initially, the subject is told only that Turvy “watches what you do and understands a bit of English.” Owing to its familiarity with the tasks, Turvy appears to have programmatic rules of inference and limited background knowledge of basic spatial relations (contact, sequence and containment), basic structures of text and hypermedia (words, paragraphs, cards, fiekis, etc.), and that some characters are delimiters. The subject is allowed to practice an editing task, then invited to “teach” Turvy. At some point during a demonstration, the subject asks Turvy (or Turvy offers) to take over. My goal is to find the instructions people give with little or no training, and the types of prompts and feedback needed. Subjects invited to “tell” about a task either assume a human level of knowledge or give too much detail; those asked to “show” Turvy fare better. Users often teach kmps by demonstrating one iteration, then asking Turvy to “do it again” — without delimiting “it” exactly. In pilot studies I found that prompts for focusing confuse the use~ Turvy elicits instructions indirectly by verbalizing. Its choice of constraints sometimes appears incorrect, but people quickly learn Turvy’s “syntactic-level” bias. I have found a small set of useful instruction~ “watch what I do,” “you take over”, “ok”, “no — stop,” “do the next one,” “do the res$” “look at this,” “put it there;” “ this is an exception,” “skip this one,” “I made a mistake.” Useful prompts artx “looking for <constraints> — ok?”, “show me what to do,” “is this an exception because econstraints>?”, “is this <selects a structure> a useful concept? — if so, name i~” The use of instructions and responses to prompts have been highly consistent across subjects and tasks. The next step in my research is to replicate such interaction in a real system.