The range of possible domain models on which an explanation can be based is often large, yet human explainers are able to choose models that address a questioner's informative needs without undue obscurity. However, few existing explanation systems use knowledge bases providing multiple models of their topics of explanation let alone account for the selection of a model for a given explanation. This paper demonstrates the utility of a preference-based mechanism for model selection. Selection heuristics are made explicit as preferences and can be added, modified, or removed without modifying the interpreter or other planning knowledge. The mechanism is more general than previous mechanisms for model selection or “perspective,” and can apply previously identified model selection criteria.