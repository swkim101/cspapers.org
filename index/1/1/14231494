Most current Artificial Intelligence systems require a complete and correct model of their domain of application. However, for any domain of reasonable size, it is not feasible to construct such a model. The main thrust of this project is to build a system that can continuously update its model through a constant monitoring of the real world. The project involves the development of a system that starts with an incomplete and incorrect model of the world. While performing its tasks the system is occasionally confronted by observations which are inconsistent with its current beliefs. It attempts to explain these observations by hypothesizing reasons for the inconsistencies and devising experiments to pinpoint the flawed belief. Based on the results of the experiments the system revises its beliefs to accommodate the previously inconsistent observations.