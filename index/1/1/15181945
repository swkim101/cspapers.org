Genetic mapping is a common technique by which biologists approximately locate important genes on the genome of an organism. The laboratory work needed to build highly precise genetic maps can be too expensive for all but very well-studied organisms [4]. Our recent work presents algorithms to reduce the cost of mapping projects [5, 3, 2]. We focus on shrinking the populations used in laboratory work, since the costs are roughly proportional to their size. Our experiments on real biological data show that our algorithms may be used to reduce population sizes by as much as 60% with little effect on mapping precision. This is surprising, since the problems we optimize are NP -complete to approximate within any polynomial factor of the population size. By contrast, randomly chosen population samples perform much worse in practice. Here, we begin to explain the advantage of our algorithms over naive ones. We analyze the expected performance of an abstraction of an algorithm we proposed for sample selection. A measure of the performance of our algorithm converges to a factor of 2 of a simple lower bound, while a common naive algorithm has performance that is a logarithmic factor away from optimal. Our analysis is a by a martingale argument, using an extension to Azuma’s inequality proved independently by the author [2] and by Alon, Boppana and Spencer [1]. The setting A mapping population, P = {p1, . . . , pn}, is derived from an experimental breeding between two distinguishable parents. Due to the biological process of crossover, the genome of each population member pi is a mosaic of DNA derived from the two parents; the sites Ci where the mosaic switches from one parent to the other are its crossovers. Given a population sample S ⊆ P , its usefulness for mapping depends on the lengths of the bins induced by the sample. These are the regions between consecutive