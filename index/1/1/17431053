Traditional convolutional neural network ( CNN ) based query classiﬁcation uses linear feature mapping in its convolution operation. The recurrent neural network ( RNN ), differs from a CNN in representing word sequence with their ordering information kept explicitly. We propose using a deep long-short-term-memory ( DLSTM ) based feature mapping to learn feature representation for CNN . The DLSTM , which is a stack of LSTM units, has different order of feature representations at different depth of LSTM unit. The bottom LSTM unit equipped with input and output gates, extracts the ﬁrst order feature representation from current word. To extract higher order nonlinear feature representation, the LSTM unit at higher position gets input from two parts. First part is the lower LSTM unit’s memory cell from previous word. Second part is the lower LSTM unit’s hidden output from current word. In this way, the DLSTM captures the nonlinear nonconsecutive interaction within n -grams. Using an architecture that combines a stack of the DLSTM layers with a tradition CNN layer, we have observed new state-of-the-art query classiﬁcation accuracy on benchmark data sets for query classiﬁcation.