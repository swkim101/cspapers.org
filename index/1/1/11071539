Free Augmented Naive Bayes (TAN) has shown to be competitive with state-of-the-art machine learning algorithms [3]. However, the TAN induction algorithm that appears in [3] can be improved in several ways. In this paper we identify three shortcomings in it and introduce two ideas to overcome those problems: the multinomial sampling approach to learning bayesian networks and local bayesian model averaging. These ideas approaches are generic and can thus be reused to improve other learning algorithms. We empirically test the new algorithms, and conclude that in most of the cases they lead to an improvement in accuracy in the classification and in the quality of the probabilities given as predictions.