We here demonstrate robot learning from demonstration using interactive tutelage. Our Dogged Learning architecture (introduced in [1] and shown in Figure 1) combines concepts of teleoperative demonstration, mixed-initiative control, feedback and transparency, and real time policy inference. It is designed to be abstract, applicable to many platforms (robots), demonstration interfaces, and learning algorithms. Briefly, a robot platform provides perceptual inputs to the system, which are displayed to a demonstrator, as well as analyzed by a learned approximate policy. Both the demonstrator and the learner generate desired action outputs, as well as confidence values. The outputs can be NULL, and confidences 0, if either is not present. Using the confidences, the system arbitrates between the two outputs by, for example, taking the more confident output. Results of arbitration are displayed to the user, and the chosen action is passed to the robot, which performs as commanded. Additionally, if the commanded action is from the demonstrator, the action is paired with the perception that led to it, and the pair is used by the learner to update its policy approximation. The DL architecture is embedded within the RGame program, which allows for users who are not co-located with the robots to generate demonstration data for them. Further, the system collects data for learning, and manages the learned policies. It is a goal of our work that remote users can log in, train robots to perform tasks, and then continue to monitor and improve their performance over time.