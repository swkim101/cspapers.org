This paper addresses the question what is the outcome of multi-agent learning via no-regret algorithms in repeated games? Speci(cid:12)cally, can the outcome of no-regret learning be characterized by traditional game-theoretic solution concepts, such as Nash equilibrium? The conclusion of this study is that no-regret learning is reminiscent of (cid:12)ctitious play: play converges to Nash equilibrium in dominance-solvable, constant-sum, and general-sum 2 (cid:2) 2 games, but cycles exponentially in the Shapley game. Notably, however, the information required of (cid:12)ctitious play far exceeds that of no-regret learning.