We study a generalization of the <i>k</i>-median problem with respect to an arbitrary dissimilarity measure D. Given a finite set <i>P</i>, our goal is to find a set <i>C</i> of size <i>k</i> such that the sum of errors D(<i>P, C</i>) = Σ<i><sub>p∈P</sub></i> min<i><sub>c∈C</sub></i>{D(<i>p, c</i>)} is minimized. The main result in this paper can be stated as follows: There exists an <i>O</i>(<i>n</i>2<i><sup>k/ε)<sup>O(1)</sup></sup></i>) time (1 + ε)-approximation algorithm for the <i>k</i>-median problem with respect to D, if the 1-median problem can be approximated within a factor of (1 + ε) by taking a random sample of constant size and solving the 1-median problem on the sample exactly. Using this characterization, we obtain the first linear time (1 + ε)-approximation algorithms for the <i>k</i>-median problem in an arbitrary metric space with bounded doubling dimension, for the Kullback-Leibler divergence (relative entropy), for Mahalanobis distances, and for some special cases of Bregman divergences. Moreover, we obtain previously known results for the Euclidean <i>k</i>-median problem and the Euclidean <i>k</i>-means problem in a simplified manner. Our results are based on a new analysis of an algorithm from [20].