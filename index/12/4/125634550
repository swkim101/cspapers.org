Generative adversarial networks (GANs) excel in generating images with complex generative models for which maximum likelihood is infeasible. However training GANs is still not proved to converge. We propose a two time-scale update rule (TTUR) for training GANs with different learning rates for the discriminator and the generator. GANs trained with TTUR can be proved to converge under mild assumptions. The TTUR convergence carries over to the Adam stochastic optimiza-tion, which can be described by a second order differential equation. Experiments show that TTUR improves learning for original GANs, Wasserstein GANs, deep convolutional GANs,and boundary equilibrium GANs. TTUR is compared to conventional GAN training on MNIST, CelebA, Billion Word Benchmark, and LSUN bedrooms. TTUR outperforms conventional GAN training both in learning time and performance. time and performance.