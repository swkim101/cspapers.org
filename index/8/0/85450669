—GPUs employ a high degree of thread-level paral- lelism (TLP) to hide the long latency of memory operations. However, the consequent increase in demand on the memory system causes pathological effects such as cache thrashing and bandwidth bottlenecks. As a result, high degrees of TLP can adversely affect system throughput. In this paper, we present Poise , a novel approach for balancing TLP and memory system performance in GPUs. Poise has two major components: a machine learning framework and a hardware inference engine. The machine learning framework comprises a regression model that is trained ofﬂine on a set of proﬁled kernels to learn best warp scheduling decisions. At runtime, the hardware inference engine uses the previously learned model to dynamically pre- dict best warp scheduling decisions for unseen applications. Therefore, Poise helps in optimizing entirely new applications without posing any proﬁling, training or programming burden on the end-user. Across a set of benchmarks that were unseen during training, Poise achieves a speedup of up to 2.94 × and a harmonic mean speedup of 46.6%, over the baseline greedy- then-oldest warp scheduler. Poise is extremely lightweight and incurs a minimal hardware overhead of around 41 bytes per SM. It also reduces the overall energy consumption by an average of 51.6%. Furthermore, Poise outperforms the prior state-of- the-art warp scheduler by an average of 15.1%. In effect, Poise solves a complex hardware optimization problem with consider- able accuracy and efﬁciency.