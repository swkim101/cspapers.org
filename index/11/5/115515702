Most semantic role labeling (SRL) research has been focused on training and evaluating on the same corpus. This strategy, although appropriate for initiating research, can lead to over-training to the particular corpus. This article describes the operation of ASSERT , a state-of-the art SRL system, and analyzes the robustness of the system when trained on one genre of data and used to label a different genre. As a starting point, results are ﬁrst presented for training and testing the system on the PropBank corpus, which is annotated Wall Street Journal (WSJ) data. Experiments are then presented to evaluate the portability of the system to another source of data. These experiments are based on comparisons of performance using PropBanked WSJ data and PropBanked Brown Corpus data. The results indicate that whereas syntactic parses and argument identiﬁcation transfer relatively well to a new corpus, argument classiﬁcation does not. An analysis of the reasons for this is presented and these generally point to the nature of the more lexical/semantic features dominating the classiﬁcation task where more general structural features are dominant in the argument identiﬁcation task.