Passive Radio Frequency IDentification (RFID) tags are commonly used to provide Radio Frequency (RF) accessible unique identifiers for physical objects due to their low-cost, lack of battery, and small size. Besides this basic function, many novel RFID-based sensing applications have been proposed in the last decade, including localization, gesture sensing, and touch sensing, among others. Nevertheless, none of these systems are in widespread use today. We hypothesize that this is because the accuracy of these systems does not meet application requirements when there are even minor changes in the RF environment or in tag geometry, i.e., changes in a tag's orientation or flexing. This paper uses both theoretical analysis and real-world experiments to test this hypothesis. Our theoretical analysis shows that even a small phase or RSS noise level can result in significant estimation errors. Our extensive real-world experiments find that both the absolute and differential values of phase and RSS readings of an RFID tag's signal can vary as much as by Ï€ radians and 10 dB, respectively, due to small changes in the tag's orientation or flexing. Because of these large variations, RFID-based application systems relying on the signal phase or RSS cannot meet application requirements, confirming our hypothesis. In addition to this strong negative result, we also present some insights into designing robust RFID systems that are suitable for use in the real world.