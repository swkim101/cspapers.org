In this paper we present a novel method that fuses the ensemble meta-techniques of Stacking and Dynamic Integration (DI) for regression problems, without adding any major computational overhead. The intention of the technique is to benefit from the varying performance of Stacking and DI for different data sets, in order to provide a more robust technique. We detail an empirical analysis of the technique referred to as weighted Meta - Combiner (wMetaComb) and compare its performance to Stacking and the DI technique of Dynamic Weighting with Selection. The empirical analysis consisted of four sets of experiments where each experiment recorded the cross-fold evaluation of each technique for a large number of diverse data sets, where each base model is created using random feature selection and the same base learning algorithm. Each experiment differed in terms of the latter base learning algorithm used. We demonstrate that for each evaluation, wMeta-Comb was able to outperform DI and Stacking for each experiment and as such fuses the two underlying mechanisms successfully.