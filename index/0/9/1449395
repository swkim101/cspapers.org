Poorly translated text is often disﬂuent and difﬁcult to read. In contrast, well-formed translations require less time to process. In this paper, we model the differences in reading patterns of Machine Translation (MT) evaluators using novel features extracted from their gaze data, and we learn to predict the quality scores given by those evaluators. We test our predictions in a pairwise ranking scenario, measuring Kendall’s tau correlation with the judgments. We show that our features provide information beyond ﬂuency, and can be combined with BLEU for better predictions. Furthermore, our results show that reading patterns can be used to build semi -automatic metrics that anticipate the scores given by the evaluators.