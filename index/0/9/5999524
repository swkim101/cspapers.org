Constraint satisfaction problems, where values are sought for problem variables subject to restrictions on which combinations of values are acceptable, have many applications in artificial intelligence. Conventional learning methods acquire individual tuples of inconsistent values. These learning experiences can be generalized. We propose a model of generalized learning, based on inconsistency preserving mappings, which is sufficiently focused so as to be computationally cost effective. Rather than recording an individual inconsistency that led to a failure, and looking for that specific inconsistency to recur, we observe the context of a failure, and then look for a related context in which to apply our experience opportunistically. As a result we leverage our learning power. This model is implemented, extended and evaluated using two simple but important classes of constraint problems.