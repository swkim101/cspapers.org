Graph coloring is a central problem in distributed computing. Both vertex- and edge-coloring problems have been extensively studied in this context. In this paper we show that a (2Δ − 1)-edge-coloring can be computed in time smaller than loge n for any e > 0, specifically, in eO([EQUATION]log log n) rounds. This establishes a separation between the (2Δ − 1)-edge-coloring and Maximal Matching problems, as the latter is known to require Ω([EQUATION]log n) time [15]. No such separation is currently known between the (Δ + 1)-vertex-coloring and Maximal Independent Set problems. 
 
We devise a (1 + e)Δ-edge-coloring algorithm for an arbitrarily small constant e > 0. This result applies whenever Δ ≥ Δe, for some constant Δe which depends on e. The running time of this algorithm is O(log* Δ + [EQUATION]). A much earlier logarithmic-time algorithm by Dubhashi, Grable and Panconesi [11] assumed Δ ≥ (log n)1+Ω(1). For Δ = (log n)1+Ω(1) the running time of our algorithm is only O(log* n). This constitutes a drastic improvement of the previous logarithmic bound [11, 9]. 
 
Our results for (2Δ − 1)-edge-coloring also follows from our more general results concerning (1 − e)-locally sparse graphs. Specifically, we devise a (Δ + 1)-vertex coloring algorithm for (1 − e)-locally sparse graphs that runs in O(log* Δ + log(1/e)) rounds for any e > 0, provided that eΔ = (log n)1+Ω(1). We conclude that the (Δ + 1)-vertex coloring problem for (1 − e)-locally sparse graphs can be solved in O(log(1/e)) + eO([EQUATION]log log n) time. This imply our result about (2Δ − 1)-edge-coloring, because (2Δ − 1)-edge-coloring reduces to (Δ + 1)-vertex-coloring of the line graph of the original graph, and because line graphs are (1/2 + o(1))-locally sparse.