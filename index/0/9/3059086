The widespread use of machine learning to make consequential decisions about individual citizens (such as those involving credit, employment, insurance, and education) has been accompanied by rising alarm over instances of bias or discrimination in the algorithms and models used. While legal, regulatory and watchdog challenges to discriminatory algorithms will play an important role, it is also crucial to examine and quantify the extent to which social norms such as fairness can be "endogenized" into the learning process itself. Can we develop a rigorous and useful science of fair machine learning? The early answer appears to be positive, although we also know it will not be simple, and that there will not be a single "right" definition of fairness for learning. Much research to date has also focused on making the final output of a learning algorithm fair --- for instance, by ensuring that a learned classifier has approximately equal false positive or false negative rates across two subpopulations. Less attention has been paid to making the algorithms themselves fair, including during the training process. I will survey these developments, with special attention on recent work on defining and designing fair learning algorithms for sequential decision-making, reinforcement learning, cross-population ranking, and other problems. Of particular interest is the effort to quantify the informational, computational and other costs of fairness compared to the classical non-fair settings.