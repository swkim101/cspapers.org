The Multi-Armed Bandit problem with Un-observed Confounders (MABUC) considers decision-making settings where unmeasured variables can inﬂuence both the agent’s decisions and received rewards (Bareinboim et al., 2015). Recent ﬁndings showed that unobserved confounders (UCs) pose a unique challenge to algorithms based on standard randomization (i.e., experimental data); if UCs are naively averaged out, these algorithms behave sub-optimally, possibly incurring inﬁnite regret. In this paper, we show how counterfactual-based decision-making circumvents these problems and leads to a coherent fusion of observational and experimental data. We then demonstrate this new strategy in an enhanced Thompson Sampling bandit player, and support our ﬁndings’ efﬁcacy with extensive simulations.