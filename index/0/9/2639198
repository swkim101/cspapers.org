We present an algorithm for the training of feedforward and recurrent neural networks. It detects internal representation conflicts and uses these conflicts in a constructive manner to add new neurons to the network. The advantages are twofold: (1) starting with a small network neurons are only allocated when required; (2) by detecting and resolving internal conflicts at an early stage learning time is reduced. Empirical results on two real-world problems substantiate the faster learning speed; when applied to the training of a recurrent network on a well researched sequence recognition task (the Reber grammar), training times are significantly less than previously reported.