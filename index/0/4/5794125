Condition-specific biological networks are networks of genes and proteins induced in cells in response to different environmental conditions. The dependency structure of these networks provides important insight into how cells respond to healthy and stressful conditions. Unfortunately, for most conditions the network structure is unknown and must be inferred from condition-specific measurements of the network nodes. My research goal is to develop a machine learning framework for inferring condition-specific networks by integrating static (e.g. interaction databases) and dynamic (e.g. gene expression) data. Machine learning approaches for condition-specific network inference must achieve two high-level goals: (a) infer different dependencies including pairwise, higher-order and cyclic, and (b) capture specific and generic subnetworks across different conditions. I, with my advisors, Drs. Lane and Werner-Washburne, propose to model condition-specific networks using Markov random fields (MRFs), a class of undirected probabilistic graphical models. Unlike directed models such as Bayesian nets (Friedman 2004), which cannot explicitly represent cyclic dependencies, MRFs can represent cyclic as well as higher-order and pairwise dependencies. Unlike existing undirected model approaches, where learning is often restricted to structure refinements (Jaimovich et al. 2006), or to lower-order, often pairwise, dependencies (Margolin et al. 2005), we perform a complete search over MRF structures capturing different dependencies. We describe an algorithm for learning the structure of MRFs, and methods to evaluate a structure learning algorithmâ€™s ability to capture different dependencies. We also propose to use the physical network (with edges corresponding to physical interactions among the network nodes), to bias our structure search. Because the physical network is largely incomplete, we employ classification methods to predict protein interactions absent from interaction databases.