We prove that log-linearly interpolated backoff language models can be efﬁciently and exactly collapsed into a single normalized backoff model, contradicting Hsu (2007). While prior work reported that log-linear interpolation yields lower per-plexity than linear interpolation, normalizing at query time was impractical. We normalize the model ofﬂine in advance, which is efﬁcient due to a recurrence relationship between the normalizing factors. To tune interpolation weights, we apply Newton’s method to this convex problem and show that the derivatives can be computed ef-ﬁciently in a batch process. These ﬁndings are combined in new open-source interpolation tool, which is distributed with KenLM. With 21 out-of-domain corpora, log-linear interpolation yields 72.58 per-plexity on TED talks, compared to 75.91 for linear interpolation.