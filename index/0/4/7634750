To be considered fast, algorithms for operations on large data structures should operate in polylog time, i.e., with the number of steps bounded by a polynomial in log(N) where N is the size of the data structure. Example: an ordered list of reasonably short strings can be searched in log2 (N) time via binary search. To measure the time and space complexity of such operations, the usual Turing machine with its serial-access input tape is replaced by a random access model. To compare such problems and define completeness, the appropriate relation is loglog reducibility: the relation generated by random-access transducers whose work tapes have length at most log(log(N)). The surprise is that instead of being a refinement of the standard log space, polynomial time, polynomial space, ... hierarchy, the complexity classes for these random-access Turing machines form a distinct parallel hierarchy, namely, polylog time, polylog space, exppolylog time, ... . Propositional truth evaluation, context-free language recognition and searching a linked list are complete for polylog space. Searching ordered lists and searching unordered lists are complete for polylog time and nondeterministic polylog time respectively. In the serial-access hierarchy, log-space reducibility is not fine enough to classify polylog-time problems and there can be no complete problems for polylog space even with polynomial-time Turing reducibility