As the bandwidth of CPUs and networks continues to grow, it becomes more attractive, for efficiency reasons, to share such resources among several applications with the minimum level of interference. This can be achieved using temporal partitions, with each application assigned to its own partition and executing as if it was executing alone on a resource with lower bandwidth. The partitions are associated to servers that execute the application tasks according to a given application-level scheduler. On the other hand, the set of servers is scheduled by a system-level scheduler. This paper addresses the particular case of fixed priorities-based application-level schedulers together with a periodic server model at the system level. It starts with an adequate response time analysis based on the notion of server availability for a known server. Then it addresses the inverse problem of designing a server with minimum system-level resource requirements to fulfill the application time constraints. In this context, the paper shows that response time based schedulability tests with linear time bounds do not need to consider all tasks but just a small subset, which may lead to substantial speed-ups. The proposed method goes a step further with respect to other recent works in the literature by considering a more complete task model, effectively computing the server parameters and establishing a better trade-off concerning complexity and tightness.