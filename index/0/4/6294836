With the advent of Massive Open Online Courses (MOOCs), students from all over the world can access to quality courses via a web browser. Due to their great convenience, a popular MOOC can easily attract tens of thousands of students to enroll. Hence, a challenging problem in MOOCs is to find an efficient way to grade a large scale of assignments. To address this problem, peer assessment was proposed to grade the assignments in a scalable way. In peer assessment, each student is asked to access a subset of his/her peers' assignments via a web interface, then all these peer grades are aggregated to predict a final grade for each submitted assignment. These peer grades are very noisy due to the fact that different students have different bias and reliability. Several probabilistic models were proposed to improve the accuracy of the predicted grades by explicitly modeling the bias and reliability of each student. However, existing methods assumed that all students are independent of each other while ignoring the social interactions among the students. In real life, students' grading bias are easily affected by their friends. For example, a student tends to have a tough grading standard if his/her friends are harsh graders. Following this intuition, we propose three probabilistic models for peer assessment by incorporating social connections to model the dependencies of bias among the students. Moreover, we evaluate our models in a new peer grading dataset, which is enhanced with the social information of users in the discussion forums of the MOOC platform. Experimental results show that our models improve the accuracy of the predicted grades by leveraging social connections of students.