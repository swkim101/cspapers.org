Eliminating timing violations using clock tree optimization (CTO) persist to be a tedious problem in ultra scaled technologies. State-of-the-art CTO techniques are based on predicting the final timing quality by specifying a set of delay adjustments in the form of delay adjustment points (DAPs). Next, the DAPs are realized to eliminate the timing violations. Unfortunately, it is difficult to realize delay adjustments of exact magnitudes. In this paper, the correlation between the predicted and achieved timing quality is improved by specifying delay adjustments in the form of delay adjustment ranges (DARs). The DARs are formed such that the predicted timing quality is achieved if each delay adjustment is realized within the respective DAR. The framework first predicts the final timing quality. Next, the DARs are specified and optimized while treating the predicted timing quality as a constraint. The optimization is a trade-off between the ease of delay adjustment realization, the total amount of delay adjustment, and the number of delay adjustments. Moreover, the framework accounts for delay adjustment induced on-chip variations (OCV) and transition time constraints. On a set of synthesized circuits, it is demonstrated that the framework improves the correlation between the predicted and achieved timing quality compared with in earlier studies. The average total negative slack (TNS) and worst negative slack (WNS) are improved by 91% and 85%, respectively.