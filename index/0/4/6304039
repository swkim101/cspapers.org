In this paper, we rst explore an intrinsic problem that exists in the theories induced by learning algorithms. Regardless of the selected algorithm, search methodology and hypothesis representation by which the theory is induced, one would expect the theory to make better predictions in some regions of the description space than others. We term the fact that an induced theory will have some regions of relatively poor performance the problem of locally low predictive accuracy. Having characterised the problem of locally low predictive accuracy in Instance-Based and Naive Bayesian classiiers, we propose to counter this problem using a composite learner that incorporates both classiiers. The strategy is to select an estimated better performing classiier to do the nal prediction during classiication. Empirical results show that the strategy is capable of partially overcoming the problem and at the same time improving the overall performance of its constituent classiiers. We provide explanations of why the proposed composite learner performs better than the cross-validation method and the better of its constituent classiiers.