We are developing a new paradigm for a world model construction system which interprets a scene and builds a world model for a mobile robot using dynamic semantic constraints. The system represents a world model in hierarchical form sensor-based maps to a global map with both numerical and symbolic descriptions. At the beginning of interpretation, sensory data (video and range images) are analyzed in bottom-up fashion. A range image is transformed into a height map, and analyzed for the purpose of generating a geometrical property list for both obstacle and traversable regions that is used as the initial input to the interpretation process. At each step of the scene interpretation process, the most reliable feature of an object is selected in the region property list to propagate semantic constraints on other objects close to it. Geometrical modeling for individual objects in the scene is performed, and parameters of each model are dynamically refined by the scene interpretation process. These model parameters and their interrelationships make spatial reasoning robust. Preliminary results with video and range images are shown.