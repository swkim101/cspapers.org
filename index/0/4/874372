Let <i>X</i> be a space and <i>F</i> a family of 0, 1-valued functions on <i>X</i>. Vapnik and Chervonenkis showed that if <i>F</i> is "simple" (finite VC dimension), then for every probability measure μ on <i>X</i> and ε > 0 there is a finite set <i>S</i> such that for all <i>f</i> ε <i>F</i>, Σ<sub><i>x</i>ε<i>s</i></sub> <i>f(x</i>)/|<i>S</i>| = |<i>f f(x)d</i>μ(<i>x</i>)] ± ε.
 Think of <i>S</i> as a "universal ε-approximator" for integration in <i>F. S</i> can actually be obtained w.h.p. just by sampling a few points from μ. This is a mainstay of computational learning theory. It was later extended by other authors to families of bounded (e.g., [0, 1]-valued) real functions.
 In this work we establish similar "universal ε-approximators" for families of unbounded nonnegative real functions --- in particular, for the families over which one optimizes when performing data classification. (In this case the ε-approximation should be multiplicative.)
 Specifically, let <i>F</i> be the family of "<i>k</i>-median functions" (or <i>k</i>-means, etc.) on <i>R</i><sup><i>d</i></sup> with an arbitrary norm ϱ. That is, any set <i>u</i><sub>1</sub>,..., <i>u</i><sub><i>k</i></sub> ε <i>R</i><sup><i>d</i></sup> determines an <i>f</i> by <i>f(x</i>) = (min<sub><i>i</i></sub> ϱ(<i>x</i> - <i>u</i><sub><i>i</i></sub>))<sup>α</sup>. (Here α ≥ 0.) Then for every measure μ on <i>R</i><sup><i>d</i></sup> there exists a set <i>S</i> of cardinality poly(<i>k, d</i>, 1/ε) and a measure <i>v</i> supported on <i>S</i> such that for every <i>f</i> ε <i>F</i>, Σ<sub><i>x</i>ε<i>s</i></sub> <i>f(x)v(x)</i> ε (1 ± ε) · (<i>f f (x)d</i>μ(<i>x</i>)).