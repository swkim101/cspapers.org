To learn text understanding models with millions of parameters one needs massive amounts of data. In this work, we argue that generating data can compensate for this need. While deﬁning generic data generators is dif-ﬁcult, we propose to allow generators to be “weakly” speciﬁed in the sense that a set of parameters controls how the data is generated. Consider for example generators where the example templates, grammar, and/or vocabulary is determined by this set of parameters. Instead of manually tuning these parameters, we learn them from the limited training data at our disposal. To achieve this, we derive an ef-ﬁcient algorithm called G ENE R E that jointly estimates the parameters of the model and the undetermined generation parameters. We illustrate its beneﬁts by learning to solve math exam questions using a highly parametrized sequence-to-sequence neural network.