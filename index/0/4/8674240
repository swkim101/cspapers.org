Communicating over a noisy channel is typically much easier when errors are drawn from a fixed, known distribution than when they are chosen adversarially. This paper looks at how one can use schemes designed for random errors in an adversarial context, at the cost of few additional random bits and without relying on unproven computational assumptions.
 The basic approach is to permute the positions of a bit string using a permutation drawn from a t-wise independent family, where t = o(n). This leads to several new results:
 • We show that concatenated codes can correct errors up to the Shannon capacity even when the errors are only slightly random --- it is sufficient that they be t-wise independently distributed, for t roughly ω(log n).
 • We construct computationally efficient information reconciliation protocols correcting pn adversarial binary Hamming errors with optimal communication complexity and entropy loss n(h(p) + o(1)) bits, where n is the length of the strings and h() is the binary entropy function.
 Information reconciliation protocols allow cooperating parties to correct errors in a shared string. They are important tools in two applications: first, for dealing with noisy secrets in cryptography; second, for synchronizing remote copies of large files. Entropy loss measures how much information is leaked to an eavesdropper listening in on the protocol.
 • We improve the randomness complexity (key length) of efficiently decodable capacity-approaching private codes from Θ(n log n) to n + o(n).
 We also present a simplified proof of an existential result on private codes due to Langberg (FOCS '04).