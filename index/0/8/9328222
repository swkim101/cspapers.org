Few would disagree that verification takes the lion's share of today's project resources. If we examine the available research, we quickly discover that verification is a significant pain point that consumes massive amounts of time and resources across a multitude of market segments. Per Gary Smith at Gartner Dataquest, verification consumes 30% to 70% of total schedule, depending on design size. According to Collett International Research, Inc., a majority of ASICs and integrated circuits (ICs) require at least one respin with 71% of respins are due to functional bugs "verification should have caught".With such statistics, it is easy to understand why many contend that the verification challenge is growing at a double exponential rate (that is, exponential with respect to Moore's law). Given verification's importance and its significant impact on fundamental design quality and time-to-market demands, what is our industry doing in response? This panel explores where the methodology highway is taking us - is the destination heaven or just another level of Dante's inferno?Respected authors and experts in verification methodology will share their insights and opinions of the two methodologies used today: verify-after-the-fact (traditional) and verify-as-you-design (emerging). For decades, simulation has necessitated a verify-after-the-fact methodology and yet we can see from the industry research that a high percentage of silicon requires respins. With the latest advances in simulation testbenches and languages, can the verify-after-the-fact approach scale? Or, is it time for a move to a higher level of abstraction that enables a verify-as-you-design methodology.Industry leading chip and systems companies will discuss the methodologies they employ today to address the enormous challenge of functional verification. Questions to be addressed by our esteemed panelists include: How can we bring in schedules? What can we do to increase design quality? What cultural and organizational changes have to take place to bring quality back to the forefront of design? Where is the measurable proof of quality? What are the questions that managers should be asking themselves? What are the engines being used? What formal techniques deliver the greatest success? How important is HW/SW verification? What are the processes or methodologies being used to overcome tool or technology limitations? What is the value of assertions? How does a geographically dispersed engineering team impact design quality? What are the metrics being used to measure progress and success? And how do you know when you are done?Today we currently don't design quality in - we TEST it in (using simulation). But, what would happen if quality was designed in from the beginning? How much could we improve the overall quality level and reduce verification time, and what would this take to do it? Finally, can migration to a new methodology be the highway out of verification hell?