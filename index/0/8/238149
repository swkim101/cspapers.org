A fundamental goal of information retrieval research is to develop new retrieval techniques, and to demonstrate that they attain improved effectiveness compared to their predecessors. To quantitatively compare IR techniques, the community has developed a range of standard test collections, in particular the TREC collections; see Voorhees and Harman [2005]. Researchers use these collections as experimental test-beds, and use the observed improvements as evidence of the significance of their research contribution. Most commonly, a baseline system is chosen and improvements relative to this are measured and then presented as evidence of superiority. However, these baselines are frequently inappropriate, and there is often little consistency between researchers or research groups as to how effectiveness experiments are carried out and then reported. Ideally, the current best published results would be used as a baseline, but such practice is rare; and – a further confound on good practice – researchers usually only publish summary metrics, which cannot be used to establish statistical significance when used in subsequent comparisons. The original TREC runs are available for detailed analysis, but are rarely referred to when new methods are proposed. Instead, authors make use of off-the-shelf software, or of variants of their own software, but neither of these approaches is particularly compelling. Any claims based on comparison to such baselines must be treated with scepticism, and researchers can easily (either inadvertently or deliberately) publish non-competitive “improvements” simply by comparing to an even poorer baseline. For example, in some papers the developers of query expansion techniques compare to unexpanded baselines; whether the methods improve on other expansion techniques is not demonstrated. More broadly, it is often the case that a method that improves on a poor baseline is in effect doing no more than compensating for a defect, and the method cannot improve a system that is already effective. These issues mean that a reader or referee cannot easily establish whether published results demonstrate a genuine advance in effectiveness, and the enormous labor invested in developing test