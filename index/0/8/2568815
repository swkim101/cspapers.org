Memory cost is typically responsible for up to 80% of the chip and/or board area of most video and image processing system realizations. We present a novel technique - founded on data-flow analysis - which allows us to address the problem of background memory size evolution for a given nonprocedural algorithm specification. Usually, the number of signal instances is huge, so a new data-flow model grouping scalar signals in so-called basic sets is proposed. The method also incorporates a way to trade-off memory size with computational and controller complexity.