A folk theorem is developing which suggests that parallel solution of AI programs will not afford a speedup of more than one order of magnitude. We critically review this folk theorem by analyzing some of the problems used to "prove" it, and then cite work that provide examples of better than one order of magnitude improvement for these problems. We examine two representative AI algorithms where parallelism would achieve speedups of two orders of magnitude with a reasonable number of processors.