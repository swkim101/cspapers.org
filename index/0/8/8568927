We present an iterative learning scheme for improving the performance of highly dynamic open-loop maneuvers with quadrocopters. A probabilistic estimate of the state deviation at the end of the maneuver is obtained by fusing two data sources that are available on-board: 1) an inertial measurement unit, and 2) control inputs from an external pilot that performs a recovery after the open-loop maneuver has been executed. A computationally lightweight policy gradient method is applied in order to adapt a set of characteristic maneuver parameters, which in turn reduces the expected value of the final state deviation for the next execution of the maneuver. The performance of the learning algorithm is demonstrated in the ETH Zurich Flying Machine Arena by improving the performance of a triple flip.