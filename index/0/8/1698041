We analyze in this paper the performance of TCP/IP-like congestion control in the presence of random losses. The input rate in the control scheme that we consider has a linear growth rate; whenever a loss occurs, the input rate is halved. This approximates the performance of several versions of TCP/IP that divide their congestion window by two whatever is the cause of the loss. We propose a mathematical model that allows to account for burstiness as well as for correlation in the loss process. Our aim is to study the impact of burstiness on the throughput of the connection. We compute the expected instantaneous input rate and its moments at some potential loss instants, and provide for a useful implicit expression for the Laplace Stieltjis Transform. This allows us to compute explicitly the average throughput and its moments. We show that the average throughput is indeed sensitive to the distribution of the loss process, and not just to the average loss rate: for a given average loss rate, we show that the throughput increases with the burstiness of the loss process. We finally examine the impact of burstiness of losses on the throughput variability.