Point location problems for n points in d-dimensional Euclidean space (and ep spaces more generally) have typically had two kinds of running-time solutions: (Nearly-Linear) less than dpoly(d) · n logO(d) n time, or (Barely-Subquadratic) f(d) · n2−1/Θ(d) time, for various f. For small d and large n, "nearly-linear" running times are generally feasible, while the "barely-subquadratic" times are generally infeasible, requiring essentially quadratic time. For example, in the Euclidean metric, finding a Closest Pair among n points in Rd is nearly-linear, solvable in 2o(d) · n logO(1) n time, while the known algorithms for finding a Furthest Pair (the diameter of the point set) are only barely-subquadratic, requiring Ω(n2--1/Θ(d)) time. Why do these proximity problems have such different time complexities? Is there a barrier to obtaining nearly-linear algorithms for problems which are currently only barely-subquadratic? We give a novel exact and deterministic self-reduction for the Orthogonal Vectors problem on n vectors in {0, 1}d to n vectors in Zω(log d) that runs in 2o(d) time. As a consequence, barely-subquadratic problems such as Euclidean diameter, Euclidean bichromatic closest pair, and incidence detection do not have O(n2−ϵ) time algorithms (in Turing models of computation) for dimensionality d = ω(log log n)2, unless the popular Orthogonal Vectors Conjecture and the Strong Exponential Time Hypothesis are false. That is, while the poly-log-log-dimensional case of Closest Pair is solvable in n1+o(1) time, the poly-log-log-dimensional case of Furthest Pair can encode difficult large-dimensional problems conjectured to require n2−o(1) time. We also show that the All-Nearest Neighbors problem in ω(log n) dimensions requires n2−o(1) time to solve, assuming either of the above conjectures.