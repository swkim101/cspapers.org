Ontology-based context models are widely used in a ubiquitous computing environment. Among many benefits such as acquisition of conceptual context through inference, context sharing, and context reusing, the ontology-based context model enables context-aware applications to use conceptual contexts which cannot be acquired by sensors. However, inferencing causes processing delay and it becomes a major obstacle to context-aware applications. The delay becomes longer as the size of the contexts managed by the context management system increases. In this paper, we propose a method for reducing the size of context database to speed up the inferencing. We extend the query-tree method to determine relevant contexts required to answer specific queries from applications in static time. By introducing context types into a query-tree, the proposed scheme filters more relevant contexts out of a query-tree and inference is performed faster without loss of the benefits of ontology.