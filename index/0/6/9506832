Sentence pair modeling is critical for many NLP tasks, such as paraphrase identiﬁcation, semantic textual similarity, and natural language inference. Most state-of-the-art neural models for these tasks rely on pretrained word embedding and compose sentence-level semantics in varied ways; however, few works have attempted to verify whether we really need pretrained embeddings in these tasks. In this paper, we study how effective subword-level (character and character n-gram) representations are in sentence pair modeling. Though it is well-known that subword models are effective in tasks with single sentence input, including language modeling and machine translation, they have not been systematically studied in sentence pair modeling tasks where the semantic and string similarities be-tween texts matter. Our experiments show that subword models without any pretrained word embedding can achieve new state-of-the-art re-sults on two social media datasets and competitive results on news data for paraphrase iden-tiﬁcation.