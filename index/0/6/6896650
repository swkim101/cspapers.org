Autonomous systems are often difficult to program. Reinforcement learning (RL) is an attractive alternative, as it allows the agent to learn behavior on the basis of sparse, delayed reward signals provided only when the agent reaches desired goals. However, standard reinforcement learning methods do not scale well for larger, more complex tasks. One promising approach to scaling up RL is hierarchical reinforcement learning (HRL) (Sutton, Precup, & Singh 1999; Kim & Dean 2003; Dietterich 2000; Givan, Leach, & Dean 2000; Parr 1998). Here, low-level policies, which emit the actual, primitive actions at a fast time-scale, solve only parts of the overall task. Higher-level policies solve the overall task, but they may consider only few abstract, high-level observations and actions (often referred to as macro-actions or options), at a slower time scale. This reduces each levelâ€™s search space and facilitates temporal credit assignment. Another advantage is that low-level policies can be re-used easily, either within the same task or in other tasks. One of the fundamental steps toward HRL is to automatically establish subgoals. Methods for automatically introducing subgoals have been studied in the context of adaptive production systems, where subgoals are created based on examinations of problem-solving protocols. For RL systems, several researchers have proposed methods by which policies learned for a set of related tasks are examined for commonalities or are probabilistically combined to form new policies. Subgoal discovery has been addressed by several researchers such as (McGovern & Barto 2001; Digney 1996; Drummond 1997), However the most closely related research is that of Digney (Digney 1996). In his system, states that are visited frequently or states where the reward gradient is high are chosen as subgoals. Drummond (Drummond 1997) proposed a system where an RL agent detected walls and doorways through the use of vision processing techniques applied to the learned value function. This paper presents a new method for the autonomous construction of hierarchical action and state representations in reinforcement learning, aimed at accelerating learning and extending the scope of such systems. In this approach, the agent uses information acquired while learning one task