—Low-dimensional embeddings, computed by LSTMs or other techniques, are a popular approach for capturing the “meaning” of text and a useful form of unsupervised learning. However, their power is not theoretically understood. We derive formal understanding by looking at the subcase of linear embedding schemes. Using compressed sensing theory we show that representations combining the constituent word vectors can be information-preserving linear measurements of Bag-of-n- Grams (BonG) representations of text. This leads to a new theoretical result about LSTMs: embeddings derived from a low-memory LSTM are provably at least as powerful on classiﬁcation tasks as a linear classiﬁer over BonG vectors, a result that extensive empirical work has thus far been unable to show. Our experiments support these ﬁndings and establish strong baselines on standard benchmarks. We also show a surprising new property of pretrained word embeddings: they form a sensing matrix for text that is more efﬁcient than random matrices, which may explain why they lead to better representations in practice. The full version of this work appears in the Proceedings of the 6th International Conference on Learning Representations (ICLR 2018). 1