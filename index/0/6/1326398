It has recently been shown that different NLP models can be effectively combined using dual decomposition. In this paper we demon-strate that PCFG-LA parsing models are suit-able for combination in this way. We exper-iment with the different models which result from alternative methods of extracting a gram-mar from a treebank (retaining or discarding function labels, left binarization versus right binarization) and achieve a labeled Parseval F-score of 92.4 on Wall Street Journal Sec-tion 23 â€“ this represents an absolute improve-ment of 0.7 and an error reduction rate of 7% over a strong PCFG-LA product-model base-line. Although we experiment only with bina-rization and function labels in this study, there is much scope for applying this approach to other grammar extraction strategies.