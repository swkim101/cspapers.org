Advances in hardware and wireless technology have made mobile devices ubiquitous in our daily life. Consequently, extending the battery life has become a major challenge needed to improve the usability of laptops. The purpose of a power management (PM) policy is to prolong a laptop’s battery life while not affecting the system performance as perceived by the user. The optimal strategy is to turn off certain components when their services are not going to be needed and turning them back on just before they are needed. This uncertainty about the future is the core challenge in PM. Most current PM techniques are based on timeout policies. These policies turn off a component if it hasn’t been used for some predefined time. They are fairly simple and robust. However, these policies may be too fast or too slow to react. Recent research has addressed the importance of adaptive decision making for PM. This research can be roughly divided into two categories: direct prediction and stochastic optimization. The direct prediction approach (e.g. (Hwang & Wu 1997)) generally attempts to predict future idleness by associating current observable events with future idleness for PM policies. The stochastic optimization approach considers formulation of the system’s state as stochastic processes, for example, a Markov model (Benini et al. 1999; Simunic 2002). The main contribution of our work is to demonstrate the importance of incorporating a user model into adaptive PM. We use a Dynamic Bayesian Network (DBN) (Thomas & Kanazawa ) to capture the relationship between the latent state of the user and his/her observable activities. The DBN model is learned from the user’s data and is therefore adapted to individual user. Besides, the future idle duration probability density functions (PDF) differ significantly when conditioned on different latent states. Based on the PDF associated with each individual latent state, our estimated future idle duration is more accurate. This information allows us to estimate the expected power savings and the expected next service requested time. By trading-off these two factors, the devised PM strategy is able to adjust to different level of aggressiveness. Furthermore, the PM decisions are also calibrated according to the latent states of