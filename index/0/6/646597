The Cranfield paradigm was designed in the early 1960s when information access was via Boolean queries against manually indexed documents and there was (virtually) no text online. Cyril Cleverdon, Librarian of the College of Aeronautics, Cranfield, England, built a test collection that modeled university researchers, including abstracts of aeronautical papers, one-line queries based on questions gathered from the researchers, and complete relevance judgments for each query submitted by these users. The idea of carefully modeling some user application continued with Prof. Gerard Salton and the SMART collections, such as searching MEDLINE abstracts using real questions submitted to MEDLINE, or searching full text TIME articles with real questions from several sources, etc. A 1969 paper by Michael Lesk and Salton used experiments on the ISPRA collection to show that relevance judgments made by a person who was not the user would still allow valid system comparison, a precursor to the paper by Ellen Voorhees in SIGIR 1998. Implementation of the Cranfield paradigm has undergone extensive modifications over the years in TREC and other evaluation forums as the data and tasks have gotten more complex. However it still stands as the model of choice, both for these (mostly) academic evaluations and at least partially for commercial evaluations, especially for straight-forward searching tasks where clicks and dwell times can be used to predict relevance. However the world of information access has exploded in recent years to encompass online shopping, social networking, personal desktop organization, etc. Is it time to have a new paradigm, and if so, how do we ensure that information retrieval evaluation remains scientifically valid? Categories & Subject Descriptors: H.3.4 [Systems and Software]: Performance evaluation General Terms: Measurement, Experimentation Bio Donna Harman graduated from Cornell University as an Electrical Engineer, and started her career working with Professor Gerard Salton in the design and building of several collections, including the first MEDLARS one. Later work was concerned with searching large volumes of data on relatively small computers, starting with building the IRX system at the National Library of Medicine in 1987, and then the Citator/PRISE system at the National Institute of Standards and Technology in 1988. In 1990 she was asked by DARPA to put together a realistic test collection on the order of 2 gigabytes of text, and this test collection was used in the first Text Retrieval Conference (TREC). TREC is now in its 19th year, and along with its sister evaluations such as CLEF, NTCIR, INEX, and FIRE, serves as a major testing ground for information retrieval algorithms. Copyright is held by the author/owner(s). SIGIR’10, July 19–23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.