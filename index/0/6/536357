In this paper we consider the problem of learning from examples classes of functions when there are no restrictions on the allowed hypotheses other than that they are polynomial time evaluatable. We prove that for Boolean formulae, finite automata, and constant depth threshold circuits (simplified neural nets), this problem is computationally as difficult as the quadratic residue problem, inverting the RSA function and factoring Blum integers (composite number p q where p and q are both primes congruent to 3 modulo 4). These results are for the distributionfree model of learning [31]. They hold even when the inference task is that of deriving a probabilistic polynomial-time classification algorithm that predicts the correct value of a random input with probability $ + &, where s is the size of the formula, automaton or circuit, and p is any polynomial. (We call this model weak learning). Previously the only nonlearnability results that were similarly independent of hypothesis representation were those implied by the work of Goldreich, Goldwasser and Micali [17], for such classes as unrestricted Boolean circuits [25,31]. In addition to the particular results stated above we can abstract from our method a general technique for proving nonlearnability based on the existence of trapdoor functions in the sense of Yao [34].