Since a humanoid robot takes the morphology of human, users as pilots will intuitively expect that they can freely manipulate the humanoid extremities. However, it is difficult to simultaneously issue such multiple control inputs to the whole body with simple devices. It is useful for motion pattern generation to get mapping functions bidirectionally between a large number of control inputs for a humanoid robot and a small number of control inputs that a user can intentionally operate. For the purpose of generation of voluntary movement of humanoid extremities, we introduce hierarchical NLPCA neural networks that forms low dimensional variables out of multi-variate inputs of joint angles. The problem is to find common space that affords unified manipulable variables not only for specific motion like walk but also multiple whole body motion patterns. The interesting result is shown that 1 dimensional inputs can generate an approximate walking pattern, and also 3 dimensional inputs does 9 types of motion patterns.