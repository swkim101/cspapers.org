Smart environments are improving their performance and services by increasingly using ubiquitous sensing and complex inference mechanisms. However, this comes at a cost of reduced intelligibility, user trust and control. The Intelligibility Toolkit was developed to support the automatic generation and provision of explanations to help users understand context-aware inference. We have extended the toolkit to generate explanations for a wider range of inference models and to provide two styles of explanations --- rule traces and weights of evidence. We describe explanations generated from several inference models for a smart home dataset for activity recognition. This demonstrates the versatility of using the Intelligibility Toolkit to retain explanatory capabilities across different inference models.