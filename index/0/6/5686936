We study the evolution of the generalization ability of a simple linear perceptron with N inputs which learns to imitate a "teacher perceptron". The system is trained on p = αN binary example inputs and the generalization ability measured by testing for agreement with the teacher on all 2N possible binary input patterns. The dynamics may be solved analytically and exhibits a phase transition from imperfect to perfect generalization at α = 1. Except at this point the generalization ability approaches its asymptotic value exponentially, with critical slowing down near the transition; the relaxation time is ∞ (1 - √α)-2. Right at the critical point, the approach to perfect generalization follows a power law ∞ t-1/2. In the presence of noise, the generalization ability is degraded by an amount ∞ (√α - 1)-1 just above α = 1.