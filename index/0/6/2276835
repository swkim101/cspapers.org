In this paper, we examine implementing a vision-based localization system that can locate objects in the scene from their images by homography and 3D geometry and how to reduce errors arising from radial distortion of cameras, noise during imaging processing and errors associated with manual measurement. By analyzing these errors and trying different scenarios, including simple camera calibration and fine-tuning measurement data, it was possible for us to implement a system that used an appropriate pair of cameras in a multi-camera environment to achieve more accurate localization. Both synthetic and real scene data were used to verify the implemented localization system's results. Experimental results showed that the proposed approach could indeed reduce localization error and improve system stability.