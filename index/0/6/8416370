The popularity of cameras for perception is enabled in part by powerful intrinsic calibration routines, commonly requiring a user to manually collect images of a known calibration target. The manual nature of this process produces training data that is unevenly spread in the camera relative pose space. If we desire camera parameters that perform well on average over the entire relative pose space, training on such a dataset results in poor performance. To address this, we show that reasoning about the training data distribution to select a more uniformly-spread subset of images produces more accurate and stable calibrations with fewer images. Our approach can be used easily with most camera calibration algorithms. We demonstrate in large-scale physical experiments the effect of non-uniform training data and show that our approach outperforms baselines in reprojection error and parameter variance.