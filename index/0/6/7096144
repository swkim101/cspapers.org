In this work we have embodied a full 6DOF SLAM solution into a wearable stereo device working in near realtime. In order to serve on-line metric (map) and positional (localization) to the blind or visually impaired we introduce three basic elements: (i) a real-time egomotion estimation integrating 3D and 2D (appearance) information; (ii) a randomized algorithm for global rectification by entropy minimization; and (iii) a time-dependent view integration strategy for reducing the number of variables and achieving near real-time performance in global rectification. We show successful experimental results which illustrate the capabilities of both the egomotion and the global-rectification algorithm in terms of supporting 6DOF, yielding statistical robustness and enabling scalable solutions.