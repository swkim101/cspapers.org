In the last two SIGIR conferences we presented evidence that users perform equally well with search engines running a basic Cosine weighting scheme, or a state-of-the-art Okapi based ranking system [2, 3]. However, the Okapi based system clearly outperforms the basic system on the standard precision and recall measures that are commonly used to compare IR systems in forums such as TREC [1] and SIGIR. These observations call into question the appropriateness of relying on measurements of the performance of IR systems obtained in a batch setting using the Cranfield style methodology. In both of our experiments the Okapi based system found more relevant documents than the basic system, and ranked them higher in the list of results presented to the users. Users of the Okapi based systems, however, ignored the “extra” relevant documents, hence on average users of either system performed equally on any given query. It is not clear why the users tended to ignore the extra relevant documents that were presented with the Okapi based systems. One possible reason is that our user interface discouraged users from selecting these documents. The interface was a web-browser style window broken into three panes. The top pane allowed entry of the query and contained a “Search” button which, once clicked, would cause a list of 50 document titles as returned by the underlying search engine to be presented in the bottom-left pane. The users could click on these document titles and have the full document displayed in the bottom-right pane. Initial user judgments, therefore, were made on the doc-