Mobile robots suffer from sensory data corruption due to body oscillations and disturbances. In particular, information loss on images captured with onboard cameras can be very high, and such loss may become irreversible or computationally costly to undo. In this paper, we propose a novel method to minimize average motion blur captured by such mobile visual sensors. To this end, we derive a motion blur metric (MMBM) that can be computed in real-time by using only inertial sensor measurements and validate it through comparisons with optic flow computations. The applicability of MMBM is illustrated through a motion blur minimizing system implemented on the SensoRHex hexapod robot by externally triggering an onboard camera based on MMBM values computed in real-time while the robot is walking straight on a flat surface. The resulting motion blur is compared to motion blur levels obtained with a regular, fixed frame-rate image acquisition schedule by both qualitative inspection and using a blind blur metric on captured images. MMBM based motion blur minimization system not only reduces average motion blur, but also avoids frames with extreme motion blur before an image gets corrupted by appropriately delaying the triggering of frame acquisition.