Representing and reasoning about the knowledge an agent (human or computer) must have to accomplish some task is becoming an increasingly important issue in artif icial intelligence (AI) research. To reason about an agent's beliefs, an AI system must assume some formal model of those beliefs. An attractive candidate is the Deductive Belief model: an agent's beliefs are described as a set of sentences in some formal language (the base sentences), together w i th a deductive process for deriving consequences of those beliefs. In part icular, a Deductive Belief model can account for the effect of resource l imitations on deriving consequences of the base set: an agent need not believe all the logical consequences of his beliefs. In this paper we develop a belief model based on the notion of deduction, and contrast i t w i th current AI formalisms for belief derived from Hint ikka/Kr ipke possible-worlds semantics for knowledge. 1 . I n t r o d u c t i o n As AI planning systems become more complex and are applied in more unrestricted domains that contain autonomous processes and planning agents, there are two problems (among others) that they must address. The first is to have an adequate model of the cognitive state of other agents. The second is to form plans under the constraint of resource l imitations: i.e., an agent does not always have an infinite amount of t ime to sit and think of plans while the world changes under h im; he must act. These two problems are obviously interlinked since, to have a realistic model of the cognitive states of other agents, who are presumably similar to himself, an agent must reason about the resource l imitations they are subject to in reasoning about the world. In this paper we address both problems wi th reference to AI planning system robots and one part of their cognitive state, namely beliefs. Our goal is to pursue what might be called robot psychology: to construct a plausible model of robot beliefs by examining robots' internal representations of the world. The strategy adopted is both descriptive and constructive. We examine a generic AI robot planning system (from now on we use the term agent) for commonsense domains, and isolate the subsystem that represents its beliefs. It is then possible to form ' Th i s paper describes results from the author's dissertation research. The work presented here was supported by grant N0014-80-C-0296f rom the Office of Naval Research. an abstraction of the agent's beliefs, that is, a model of what the agent believes. This is the descriptive part of the research strategy. Among the most important properties of this model is an explicit representation of the deduction of the consequences of beliefs, and so we call the model one of Deductive Belief. It is assumed that the beliefs of the agent are about conditions that obtain in the planning domain, e.g., what (physical) objects there are, what properties they have, and what relations hold between them. Thus the descriptive model of Deductive Belief has an obvious shortcoming. Although agents can reason about the physical wor ld, they don't have any method for reasoning about the beliefs of other agents (or their own). By taking the descriptive model to be the way in which agents view other agents' beliefs, we can construct a more complex model of belief that lets agents reason about others' beliefs. This is the constructive part of the research strategy. There are two main sections to this paper. In the first, the concept of a belief subsystem is introduced, and its properties are defined by its relationship to the planning system as a whole. Here we discuss issues of deductive closure, completeness, and the resource l imitations of the belief subsystem. We also characterize the constructive part of the model by showing how to expand a belief subsystem to reason about the beliefs of other agents. In the second section, we formalize the Deductive Belief model for the propositional case by introducing the belief logic B, and compare it w i th other formalizations of knowledge and belief. Because the treatment here must be necessarily brief, throughout the paper proofs established by the author, but not yet published, are referenced. 2 . D e d u c t i v e B e l i e f What is an appropriate model of belief for robot problemsolving systems reasoning about the wor ld , which includes other robot problem-solving systems? In this section we discuss issues surrounding this question and propose a model of Deductive Belief as a suitable formal abstraction for this purpose. 2.1 P l a n n i n g a n d B e l i e f : B e l i e f S u b s y s t e m s A robot planning system, such as STRIPS, must represent knowledge about the world in order to plan actions that affect the world. Of course it is not possible to represent all the complexity of the real wor ld , so the planning system uses some abstraction of real-world properties that are important for its task, e.g., it might assume that there are objects that can be