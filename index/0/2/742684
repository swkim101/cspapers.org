A distribution <i>X</i> over binary strings of length <i>n</i> has min-entropy <i>k</i> if every string has probability at most 2<sup>-<i>k</i></sup> in <i>X</i>. We say that <i>X</i> is a δ-source if its rate <i>k</i>⁄<i>n</i> is at least δ.We give the following new explicit instructions (namely, poly(n)- time computable functions) of <i>deterministic</i>extractors, dispersers and related objects. All work for any fixed rate δ>0. No previous explicit construction was known for either of these, for any δ‹1⁄2. The first two constitute major progress to very long-standing open problems. <ol><li><b>Bipartite Ramsey</b> <i>f</i><inf>1</inf>: (0,1)<sup>n</sup>)<sup>2</sup> →0,1, such that for any two independent δ-sources <i>X</i><inf>1</inf>, <i>X</i><inf>2</inf> we have <i>f</i><inf>1</inf>(<i>X</i><inf>1</inf>,<i>X</i><inf>2</inf>) = 0,1 This implies a new explicit construction of <i>2N</i>-vertex bipartite graphs where no induced <i>N</i><sup>δ</sup> by <i>N</i><sup>δ</sup> subgraph is complete or empty.</li> <li><b>Multiple source extraction</b> <i>f</i><inf>2</inf>: (0,1<i>n</i>)<sup>3</sup>→0,1 such that for any three independent δ-sources <i>X</i><inf>1</inf>,<i>X</i><inf>2</inf>,<i>X</i><inf>3</inf> we have that <i>f</i><inf>2</inf>(<i>X</i><inf>1</inf>,<i>X</i><inf>2</inf>,<i>X</i><inf>3</inf>) is (<i>o</i>(1)-close to being) an unbiased random bit.</li> <li><b>Constant seed condenser</b><sup>2</sup> <i>f</i><inf>3</inf>: <i><sup>n</sup></i> →(0,1<sup><i>m</i></sup>)<sup><i>c</i></sup>, such that for any δ-source <i>X</i>, one of the <i>c</i> output distributions <i>f</i><inf>3</inf>(<i>X</i>)<inf><i>i</i></inf>, is a 0.9-source over 0,1<i><sup>m</sup></i>. Here <i>c</i> is a constant depending only on δ.</li><li><b>Subspace Ramsey</b> <i>f</i>4: 0,1<i><sup>n</sup></i>→0,1 such that for any <i>affine</i>-δ-source<sup>3</sup> <i>X</i> we have <i>f</i><inf>4</inf>(<i>X</i>)= 0,1.</li></ol>The constructions are quite involved and use as building blocks other new and known gadgets. But we can point out two important themes which recur in these constructions. One is that gadgets which were designed to work with independent inputs, sometimes perform well enough with correlated, high entropy inputs. The second is using the input to (introspectively) find high entropy regions within itself.