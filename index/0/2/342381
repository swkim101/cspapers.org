This session invites educators interested in sharing and/or learning about experiences with tools for automatic feedback on technical work: the "if", "why" and "how". This includes experiences with program testing, problem-solving exercises, or quizzes, generated or checked with engines with expert-level technical capabilities, to scale up feedback to cope with burgeoning enrollment in CS courses while maintaining or improving student learning outcomes. Commercial, free and open-source tools now exist to assist in this endeavor. The benefits of providing timely, detailed, and insightful feedback for student effort are well known. Yet as enrollment in CS courses increases, many are hard pressed to find the human resources to scale up their feedback efforts. TAs and instructors may struggle to deal with the increased evaluation load, resulting in inconsistent, untimely, or lessened insight in feedback. Automatic grading and feedback offer a way to scale detailed and individualized feedback and for instructors to write materials with enhanced shelf-life, but with additional courseware engineering and administrative cost. It raises pedagogical questions about good ways for machine-based feedback to be blended with other types of learning activities, both conventional and novel. It has significance and relevance to departments facing the "scaling up" situation when it is understood if, how and why it can help address the problem.