In the well known Prisoner’s Dilemma, two people that are following the only rational behavior end up in the worst possible outcome. Unfortunately, this example is a useful analogy for many situations in real life, where (individually) rational behavior leads to a disaster for the society. With the rapid delegation of decision making to automated agents, the role of game theory within artificial intelligence is becoming increasingly important. In particular, game-theoretical principles must be taken into account in the design of systems and environments in which agents operate (human and automated alike). My research focuses on mechanism design (see [Nisan and Ronen, 2001] for background). More specifically, on ways to incentivize self-interested agents to cooperate in a way that will benefit the entire society. This cooperation arises not by forcing them or by relying on their good intentions, but by changing the “rules of the game” so that the best individual decision would be to cooperate. The research is multi-disciplinary in nature, involving tools and ideas from economics, computer science, mathematics, artificial intelligence, and cognitive science. This proposal briefly describes my recent work on prompting cooperation in two related domains, and outlines some future directions. I will conclude with some remarks on the strong assumption of rationality that underlies standard gametheoretic analysis and how it can be relaxed in the quest for cooperation.