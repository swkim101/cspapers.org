There has been a growing concern about the potential impact of long-term correlations (second-order statistic) in variable-bit-rate (VBR) video traffic on ATM buffer dimensioning. Previous studies have shown that video traffic exhibits long-range dependence (LRD) (Hurst parameter large than 0.5). We investigate the practical implications of LRD in the context of realistic ATM traffic engineering by studying ATM multiplexers of VBR video sources over a range of desirable cell loss rates and buffer sizes (maximum delays). Using results based on large deviations theory, we introduce the notion of Critical Time Scale (CTS). For a given buffer size, link capacity, and the marginal distribution of frame size, the CTS of a VBR video source is defined as the number of frame correlations that contribute to the cell loss rate. In other words, second-order behavior at the time scale beyond the CTS does not significantly affect the network performance. We show that whether the video source model is Markov or has LRD, its CTS is finite, attains a small value for small buffer, and is a non-decreasing function of buffer size. Numerical results show that (i) even in the presence of LRD, long-term correlations do not have significant impact on the cell loss rate; and (ii) short-term correlations have dominant effect on cell loss rate, and therefore, well-designed Markov traffic models are effective for predicting Quality of Service (QOS) of LRD VBR video traffic. Therefore, we conclude that it is unnecessary to capture the long-term correlations of a real-time VBR video source under realistic ATM buffer dimensioning scenarios as far as the cell loss rates and maximum buffer delays are concerned.