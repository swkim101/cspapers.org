The problem of maximizing precision at the top of a ranked list, often dubbed [email protected] ([email protected]), finds relevance in myriad learning applications such as ranking, multi-label classification, and learning with severe label imbalance. However, despite its popularity, there exist significant gaps in our understanding of this problem and its associated performance measure. 
 
The most notable of these is the lack of a convex upper bounding surrogate for [email protected] We also lack scalable perceptron and stochastic gradient descent algorithms for optimizing this performance measure. In this paper we make key contributions in these directions. At the heart of our results is a family of truly upper bounding surrogates for [email protected] These surrogates are motivated in a principled manner and enjoy attractive properties such as consistency to [email protected] under various natural margin/noise conditions. 
 
These surrogates are then used to design a class of novel perceptron algorithms for optimizing [email protected] with provable mistake bounds. We also devise scalable stochastic gradient descent style methods for this problem with provable convergence bounds. Our proofs rely on novel uniform convergence bounds which require an in-depth analysis of the structural properties of [email protected] and its surrogates. We conclude with experimental results comparing our algorithms with state-of-the-art cutting plane and stochastic gradient algorithms for maximizing [email protected]