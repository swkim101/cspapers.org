Consider two user populations, of which one is targeted and the other is not. Users in the targeted population follow a Markov chain on a space of n states. The untargeted population follows another Markov chain, also defined on the same set of n states. Each time a user arrives at a state, he/she is presented with information appropriate for the targeted population (an advertisement, or a recommendation) with some probability. Presenting the advertisement incurs a cost. Notice that while the revenue grows in proportion to the flow of targeted users through the state, the cost grows in proportion to the total flow (targeted and untargeted) through the state. How can we compute the best advertisement policy? The world-wide web is a natural setting for such a problem. Internet service providers have trail information for building such Markovian user models where states correspond to pages on the web. In this paper we study the simple problem above, as well as the variants with multiple targetable segments. In some settings the policy need not be a staticprobability distribution on states. Instead, we can dynamicallyvary the policy based on the userâ€™s path through the states. Computer Science Department, Stanford University, CA 9430 5. Supported by the Pierre and Christine Lamond Fellowship, NSF Gr ant IIS9811904 and NSF Award CCR-9357849, with matching funds from IBM, Mitsubishi, Schlumberger Foundation, Shell Foundation, a nd Xerox Corporation. Email:moses@theory.stanford.edu . This work was done while the author was visiting IBM Almaden Research Center. yIBM Almaden Research Center, 650 Harry Road, San Jose, CA 95120. Email: fravi, pragh, sridhar, tomkins g@almaden.ibm.com We provide characterizations which reveal interesting insights into the nature of optimal policies, and then, use these insights for algorithm design. Targeting problems do not seem amenable to solutions using methods from familiar fields such as Markov decision processes.