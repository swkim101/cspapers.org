The advances in memory technology concerning performance have not been able to keep pace with those in processor technology. Processors clocked with hundreds of megahertz exceed the speed of affordable memory by factors. Caches can decouple the speed of the processing unit from the speed of the memory system if applications show a high locality of reference. Unfortunately, operations on data streams – frequently found in soft real-time multimedia applications – do not show this benign behavior. Thus, applications working on data streams rely heavily upon a guaranteed memory bandwidth to meet specific timing requirements. In multiprocessor systems the available memory bandwidth is shared by all processing units and DMA devices. Consequently, the processing units can interfere and slow each other down when accessing memory. Up to now, this effect, called memory preemption, is not covered by realtime operating systems using timer and I/O related information. Our novel approach to resource management is based on knowledge derived from counters in the memory subsystem. We demonstrate that the use of information related to cacheand mainmemory access opens new dimensions of resource management in shared-memory architectures. The introduction of memory-bandwidth guarantees adds a further resource to capacityreservation models and therefore enhances the quality of service. Soft real-time processes can request memory bandwidth guarantees. Processes without guarantees are throttled, so that they do not withhold valuable main memory bandwidth from real-time applications. To dynamically slow down processes that exceed a certain number of main memory operations per time frame, the TLB-miss handler executes additional NOP instructions. Thus, the TLB-fill will be delayed, fewer memory pages can be touched and fewer cache misses per time frame will occur. This new mechanism, called Process Cruise Control, maintains the execution speed of soft real-time applications in a multiprocessor environment. Applications of other scheduling classes (e.g., Time-Sharing), which operate with low memory demands, run at full speed, whereas applications with high memory demands will be throttled in their speed of execution and executed with lower priority. Measurements conducted on a prototype implementation using the Solaris operating system clearly demonstrate the benefit of the memory throttle for a video conferencing application running in a multiprogrammed multiprocessor environment.