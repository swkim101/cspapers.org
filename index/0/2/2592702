Memory latency and lack of bandwidth are the main barriers to achieve high performance from current and future processors, specially in numeric applications. New organizations of the memory subsystem as well as hardware and software mechanisms to effectively exploit them are required. The paper presents a new compilation technique to pack several load/stores (that access consecutive memory locations) into a single wide load/store, so that the number of wide load/stores is maximized. It also evaluates the performance trade-offs of wide buses and the additional register pressure, showing that it is minimal and has negligible effects. Finally, the paper proposes a hardware mechanism to detect and group memory accesses into wide accesses at run time, so that binary compatibility is preserved. The evaluations are performed using 1243 loops that represent about 78% of the execution time of the Perfect Club. The results reveal that using wide buses is a cost-effective solution to improve the bandwidth between the processor and the first-level cache.