Our eminent researchers including John McCarthy, Allen Newell, Claude Shannon, Herb Simon, Ken Thompson and Alan Turing put significant effort into computer chess research. Now that computers have reached the grandmaster level, and are beginning to vie for the World Championship, the AI community should pause to evaluate the significance of chess in the evolving objectives of A I , evaluate the contributions made to date, and assess what can be expected in the future. Despite the general interest in chess amongst computer scientists and the significant progress in the last twenty years, there seems to be a Jack of appreciation for the field in the AI community. On one hand this is the frui t of success (brute force works, why study anything else?), but also the result of a focus on performance above all else in the chess community. Also, chess has proved to be too challenging for many of the AI techniques that have been thrown at i t . We wish to promote chess as the fundamental test bed recognized by our founding researchers and increase awareness of its contribution to date. Panel S u m m a r y The factors that make chess an excellent domain for AI research include: • Richness of the problem-solving domain. • Abi l i ty to monitor and record progress accurately through competition and rating, because of its welldefined structure. • Chess has been around for centuries the basics are well-understood internationally, expertise is readily available and is (generally!) beyond proprietary or nationalistic interests. Has been considered a "game of intelligence." Many players of the game feel mentally "stretched." • Detailed psychological studies of chess playing exist. These studies suggest that human players use different reasoning modes from those in current chess programs. Further, the reasoning modes are also used in many other problem-solving domains. • Excellent test bed for uncertainty management schemes the basis of most expert problem-solving. The well-definedness and discreteness of the game have led many to ignore this. Levinson, et at. 547 The above factors make chess a useful tool regardless of the strength of the current programs. Because of the success of the current methods there remains a vast arena of other methods that have not been explored. The most obvious lack is in the application and development of machine learning techniques to chess, but other areas, including knowledge representation and compilat ion, planning and control, also seem to be applicable. AI researchers should be encouraged to use chess as a test bed for their techniques, with the understanding that chess is not the end in itself. Chess may provide the avenue by which bridges may be buil t between cognitive science, AI and connectionist modelingWi t h the current and future battle for the World Human-Computer Championship the AI community should be made more sensitive to the issues involved and their bearing on intelligence research: Is search sufficient? How much detailed chess knowledge is required? How is this knowledge implemented and incorporated with search? We are fortunate to have a World Champion who promotes creativity over the chess board and is wil l ing to face the challenge from computers head-on. The members of the panel and the presentations have been designed to address these topics in a way that supports our objectives to make chess an important and respected AI tool in this new decade. Jonathan Schaeffer wi l l emphasize those areas of computer chess research that have been ignored, because the approach has been a competitive/engineering one instead of scientific. Feng-hsiung Esu of the Deep Thought team wil l discuss the role of knowledge in current chess programming and argue that more responsibility for the knowledge should be put on the machines themselves, Tony Marsland wi l l present specific open research issues in computer chess that wi l l require AI solutions. Robert Levinson wi l l describe an alternative model of chess computation, a self-learning pattern-oriented chess program ("Morph") whose knowledge is learned incrementally from experience, without many examples being stored (and wi th l i t t le guidance about relevant features). David Wilkins wi l l provide balance to the discussion by pointing out the l imitations of chess and claiming that Go is a better domain. He wi l l also describe a new type of games tournament that prevents the human tailoring of evaluation functions and encourages the use of learning and more robust approaches. The t iming for this panel is particularly good with the current World Championship having completed, a more powerful Deep Thought on the scene, a recent article in Scientific American [Hsu et al, 1990] and new books by Levy and Newborn [1991], and by Marsland and Schaeffer [1990]. P r e s e n t a t i o n s C o m p u t e r C h e s s : Sc ience o r E n g i n e e r i n g ? J o n a t h a n Schaeffer University of Alberta Research into artificial intelligence using chess as the application domain has produced several important contributions to A I : 548 Panels • The effectiveness of brute-force search. Chess has clearly demonstrated that simple, brute-force approaches should not be quickly discarded. • Iterative search. Some of the ideas developed for alpha-beta search, iterative deepening in particular, are applicable to other search domains. • The inadequacy of conventional AI techniques for realtime computation. No competitive computer chess program uses AI languages or knowledge representation methods. Why? They are too slow for a realtime, high performance application. Although these (and other, lesser contributions) have enhanced our knowledge, it is not clear whether the effort expended justifies the results obtained. It is easy to question the usefulness of computer chess research. It is important to distinguish between computer chess research and research using chess as a test bed. Unfortunately, the latter has evolved into the former. An entirely new field of "computer chess" has evolved, with the emphasis on chess performance and chess research not generally of much interest to the AI community. There is a much deserved credibility problem here. The unfortunate correlation between program speed and performance encourages short-term projects (speeding up a move generator 10%) at the sacrifice of long-term research projects (such as chess programs that learn). After over 30 years of work on chess programs, where are the scientific advances in : • knowledge-based search algorithms? There has been some good work in this area, but none has progressed enough to be used in competitive chess programs. Alpha-beta simplifies the programming task, but the exponential search l imits what can be achieved. • knowledge representation and acquisition? These areas are of considerable importance to chess programs, yet the computer chess community has done embarrassing l i t t le research in this area. • error analysis? While extensive error analysis has been done on search algorithms, l i t t le has been done to quantify errors in evaluation functions and how they interact with the search. • tool development? W i th the right tool, work that might take days could be done in minutes. No tools are being developed to help build chess programs. For example, why isn't someone working on tools for defining chess knowledge? If the community were committed to research, many of these problems would have been addressed by now. Sadly, much of the work currently being done on computer chess programs is engineering, not science. For example, the engineering of special-purpose VLSI chips to increase the speed of a chess program only underlines the importance chess programmers attach to speed. In my opinion, conventional computer-chess methods wil l yield l i t t le of further interest to the AI community. I believe they wi l l be inadequate to defeat the human World Champion in a match for a long time to come. It is sti l l very easy to set up a position for which the computer has no idea what is going on even if you speed up the machine 1000-fold. The current computer chess work wi l l only underscore the need for better ways of adding and manipulating knowledge reliably. The defeat of the human World Chess Champion sooner rather than later wi l l help artificial intelligence. This wi l l help to re-establish chess as an ideal problem domain for experimenting wi th the fundamental problems of artificial intelligence, as elaborated more fully by Donskoy and Schaeffer [1989]. " E x p e r t I n p u t s " are Somet imes H a r m f u l Feng-hsiung Hsu IBM T J . Watson Research Center Experience from the chess machine Deep Thought suggests that inputs from chess experts, while generally useful, cannot be trusted completely. A good example of this is Deep Thought's evaluation function. Several changes by capable human chess experts failed to produce significant improvements and occasionally even affected the machine's performance negatively. Here, human experts, along with their expertise, introduced their own prejudices into the program. One way of solving this problem is to l imi t the type and the amount of expert inputs allowed into the program; in other words, having an almost "knowledge-free" machine. The availability of on-line high quality chess game databases makes this an attractive approach. Instead of having the value of, say, an isolated pawn set by human experts either explicitly or in functional form, one can simply tell the program that isolated pawns are important features and statistical procedures, with some additional expert inputs, can then be used to decide the functional form and the proper weighting of the features in question. That more responsibility for knowledge should be placed on machines is consistent with recent efforts to handle the knowledge acquisition problem in expert systems and also in memory-based reasoning schemes where knowledge is generated stat