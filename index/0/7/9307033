We present new results from applying the assumptions of two-player game searches, namely limited search horizon and commitment to moves in constant time, to single-agent problem-solving searches. We show that the search depth achievable with alpha pruning for a given amount of computation actually increases with increasing branching factor. We prove that real-time-A* (RTA*) makes locally optimal decisions given the heuristic information available to it on a tree. We then modify RTA* to perform optimally on a graph as well. We also prove that RTA* is guaranteed to find a solution to any solvable problem regardless of the initial heuristic values. In addition, we develop a learning version of RTA* that improves its performance over multiple problem-solving trials, and prove convergence of the learned heuristic values to the exact values. Finally, we demonstrate that these algorithms effectively solve larger problems than have previously been solvable with heuristic search techniques.