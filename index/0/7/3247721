Event-driven vision sensors have the potential to support a new generation of efficient and robust robots. This requires the development of a new computational framework that exploits not only the spatial information, like in the traditional frame-based approach, but also the temporal content of the sensory data. We propose a method for unsupervised learning of filters for the processing of the visual signal from event-driven sensors. This method exploits the temporal coincidence of events generated by each object in a spatial location of the visual field. The approach is based on a modification of Spike Timing Dependent Plasticity that takes into account the specific implementation on the robot and the characteristics of the used sensor. It gives rise to oriented spatial filters that are very similar to the receptive fields observed in the primary visual cortex and traditionally used in bio-inspired hierarchical structures for object recognition, as well as to novel curved spatial structures. Using mutual information measure we provide a quantitative evidence that such curved spatial filters provide more information than equivalent oriented Gabor filters and can be an important aspect for object recognition in robotic applications.