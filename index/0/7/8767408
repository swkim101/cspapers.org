Eigendecomposition is a common technique that is performed on sets of correlated images in a number of computer vision and robotics applications. Unfortunately, the computation of an eigendecomposition can become prohibitively expensive when dealing with very high resolution images. While reducing the resolution of the images will reduce the computational expense, it is not known how this affects the quality of the resulting eigendecomposition. The work presented here proposes a framework for quantifying the effects of varying the resolution of images on the eigendecomposition that is computed from those images. Preliminary results show that an eigendecomposition from low-resolution images may be nearly as effective in some applications as those from high-resolution images.