We propose a method to build thesauri on the basis of grammatical relations The proposed method constructs thesaun by using a hierarchical clustering algorithm An important point in this paper is the claim that thesauri in order to be efficient need to take (surface) case information into account We refer to the thesauri as ' relation-based thesaurus (RBT) " In the experiment four RBTs of Japanese nouns were constructed from 26,023 verb-noun cooccurrences, and each RBT was evaluated fry objective criteria The experiment has shown that the RBTs have better properties for selectional restriction of case frames than conventional ones 1 I n t r o d u c t i o n For most natural language processing (NLP) systems thesaun are one of the basic ingredients In particular coupled with case frames, they are useful to guide corrert analysis [Allen, 1988] In the examplebased frameworks thesauri are also used to compensate for insufficient example data [Sato and Nagao, 1990, Nagao and Kurohashi 1992] Roget s International Thesaurus [Chapman, 1984] Bunruigoihyo [Hayashi 1966] and WordNet [Miller et al, 1993] are typical thesaun which have been used in the past NLP research Al l of them are handcrafted, machine-read able and have farely broad coverage However, since these thesaun are originally compiled for human use they are not always suitable for natural language processing by computers Their classification of words is sometimes too coarse and does not provide sufficient distinctions between words One of the reasons for this is that these thesauri aim for broad coverage, rather than for dealing wi th a par ticular domain Experience has shown that restricting the target domain appropriately is the key to building successful NLP systems This fact has been discussed by researchers working on "sublanguage" [Gnshman and Sterling, 1992, Sekine et al 1992] or "register" [Halliday and Hassan, 1985 Biber, 1993] Another problem with handcrafted thesauri is the fact that their classification is based on the intuition of lexicographers, with their cnteria of classification not being alwavs clear Furthermore crafting thesauri by hand is very expensive even in restricted domains Therefore building thesauri automatically from corpora has received a large attention in recent years [Hirschman et al 1975, Hindle 1990 Hatzivassiloglou and McKeown, 1993, Pereira et al 1993] These attempts basically take the following steps [Charniak 1993] (1) extract co-occurrences (2) define similarities (distance) between words on the basis of co-occurrence data (3) cluster words on the basis of similarity At each step, we have several options In this paper we wil l focus on step (1) the properties of co-occurences As for step (2) and (3) we wil l use the method proposed by Iwayama and Tokunaga [lwayama and Tokunaga, 1995], which is bnef l \ described in section 3 Co-occurrenres are usually gathered on the basis of some relations such as predicate-argument modifiermodified, adjacency or mixture of them For example Hmdle used verb-subject and verb-object relations to classify nouns [Hindle, 1990] Hirschman et al also used verb-subject and verb-object relations as well as prepositions and adjective-noun relations [Hirschman tt al, 1975] Hatzivassiloglon and McKeown suggested to use as many relations as possible in order to classify adjectives [Hatzivassiloglou and McKeown, 1993] Al l these attempts assume a distribution hypothesis that is words appearing in a similar context are similar hence they should be classified into the same class [Grishman et al, 1986, Hindle, 1990] As far as we concerned, we consider co-occurrences of words as a kind of context The more specific the context is, the more precise our classification will be In this respect we should use as specific relations as possible in order to obtain better thesauri Unlike previous research on this topic, we suggest to build a thesaurus for each grammatical relation In particular, we wil l use surface cases Therefore we would have a thesaurus for each surface case This is what we call ''relation-based thesaurus (RBT) " Another aspect that seems to be lacking in the past research is an objective evaluation of the automatically built thesauri All the previous attempts except [Pereira 1308 NATURAL LANGUAGE et al , 1993] evaluate their results on the basis of subjec tive cntena to what extent iS the result consistent with human intuit ion In this paper we propose an objective eva lua t ion method for automatically built thesauri In the following, we wil l introduce relation-based thesauri (section 2) and describe the clustering algorithm (section 3) Section 4 describes an experiment in which we compared with relation-based thesauri to conventional ones Finally section 5 concludes the paper and gives some future research directions 2 R e l a t i o n b a s e d t h e s a u r i This paper focuses on building thesauri of nones based on verb-noun relations Following the research mentioned in the previous section co-occurrence data is represented b\ tuples as shown in the left column of figure 1 where n1, and v3 denote nouns and verbs respectivelv while T1 denotes grammatical relations such as subject object and so forth F i g 1 Thesaurus construction from tuples Past research has not focused on using grammatical relations (T1) For example Hindle used subject and object relations but did not distinguish between them when calculating the distance between nouns [Hindle, 1990] Hirschman et al used other grammatical relations than subject and object in order to build word classes Actually they used various relations simulta neously [Hirschman tt al 1975] On the other hand Pereira et al used only the object relation [Pcreira et al 1993] Unlike all these attempts, we will focus on difference of relations and propose to build a thesaurus for each relation This approach is based on the fact that a noun behaves differently depending on its grammatical role Take the following examples (a) John studied English at the university (b) Mary worked ti l l late at her office (c) The university stated that they would raise the tu it ion fee (d) The mavor stated that he would raise taxes Wi th regard to taking a locatije role (derived from ' at" phrase in (a) and (b)), 'university" and 'office"" behave similarly, hence thev would be classified into the same word class On the other hand with regard to being a subject of verb "state" (in (c) and (d)), "university" behaves like "mayor" Wi th this respect, "university" and "mayor" would be classified into the same class It should be noted that the transitivity does not always hold beyond the relations In the above example, it is questionable if we could classify 'office" and "mavor" into the same class The bases of the similarity between 'university" and "office' and that between "university" and "mayor" are different In conventional thesauri "university and "mayor' would be placed in the different classes university" would be some kind of O R G A N I Z A T I O N and mayor" some kind of HUMAN However they could be put in the same class, namely as being a subject of a certain set of verbs Figure 2 shows our approach while figure 1 illustrates the conventional ones The tuples arc divided into the subsets with respect to their Tt latum \ thesaurus is built from each set of these tuples 3 H i e r a r c h i c a l B a y e s i a n C l u s t e r i n g Wc adopt a hierarchical clustering algorithm that attempts to maximize the Bavesian posterior probability at each step of merge This algorithm has been introduced b> Iyvayama and Toknnaga [iwavama and Toku naga 1995] and is referred to as Hi< rarchical Bay at an Clustering (HBC) In this set lion wc briefly icview the outline of the algorithm Given a set of training data D HBC constructs the set of clusters C that has the locally maximum value of the posterior probability P{C\D) This is a general form of the well known Maximum Likelihood estimation, estimating the most likely model (I e , set of clusters) for a given set of training data F ig 3 Hierarchical Bavesian Clustering Like most agglomerative clustering algorithms [Cor mack, 1971 Anderberg, 1973, Griffiths et al 1984 Willett 1988] HBC constructs a cluster hierarchy (also called ^ dendrogram') from bottom up by merging two clusters at a time At the beginning (the bottom level m a dendrogram) each datum belongs to a cluster whose only member is the datum itself For even pair of clusteis, HBC calculates the posterior probability after merging the pair, selecting the pair with the highest probability; To see the details of this merge process con sider a merge step k+1 (0 < k < V1) Bv the step k + TAKENOBU MAKOTO, AND H02UMI 1309 1310 NATURAL LANGUAGE