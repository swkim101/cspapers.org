My thesis aims to contribute towards building autonomous agents that are able to develop what White (1959) called competency over their environmentâ€”agents that are able to achieve mastery over their domain and are able to solve new problems as they arise using the knowledge and skills they acquired in the past. While the field of machine learning has made much progress in solving individual, isolated problems, progress has been slow in developing agents that are able to interact effectively with their environment and flexibly deal with new tasks. There is still a large gap between the abilities of humans in this respect and the current capabilities of autonomous agents. My thesis will propose a number of methods for building competence in autonomous agents using the reinforcement learning (RL) framework, a computational approach to learning from interaction that has proved effective in certain types of problems (Sutton & Barto 1998). I expect that the methods I propose will extend the capabilities of RL agents in ways that are more than incremental, essentially allowing an autonomous agent to operate at a qualitatively different level.