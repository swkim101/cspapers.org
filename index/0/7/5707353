In this paper we focus on a mobile platform which physically interacts with a human operator. We detect the contact gestures of a human operator in real-time using a labmade time-of-flight 3D scanner mounted on the platform as well as rotary torque sensors mounted along the drivetrain of its omni-directional wheels. Through the fusion of these two different sensors, touch gestures of an operator are processed inferring information about the body parts in contact and the applied forces. Behaviors that respond to touch-based gestures are programmed a priori, and with the previous sensor data we classify them into a set of known contact gestures that allow the platform to quickly react. We investigate these physical human-robot cooperative functions in a testbed consisting of a sensorized mobile platform and a human operator.