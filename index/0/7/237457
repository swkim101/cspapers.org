Statistical language modeling allows formal methods to be applied to information retrieval. As a result, such methods are preferred over their heuristic tf.idf -based counterparts. In language modeling, a statistical model is estimated for each document in the corpus. Documents are then scored by the likelihood the query was generated by the document’s model. Typically, the underlying model is assumed to be of a specific parametric form. In the past, a number of different assumptions have been made about this distribution. In [1], documents were modeled by a multiple-Bernoulli distribution. However, the estimation and smoothing techniques used to estimate the model were non-standard and somewhat heuristic. The predominant modeling assumption used today, as described in [2], is to model documents by a multinomial distribution. Such models may be smoothed in a number of ways [4]. Among these is Bayesian (Dirichlet) smoothing that takes a formal, Bayesian approach to smoothing by assuming a Dirichlet prior over the document model. Unlike Ponte and Croft’s multiple-Bernoulli estimation techniques, the multinomial assumption combined with Bayesian smoothing results in a completely formal statistical model. In this paper, we revisit the multiple-Bernoulli assumption and formalize it by taking a Bayesian approach to estimating smoothed document models.