We present a transcription system that takes a music signal as input and returns its musical score. Two stages of processing are used. The first employs a fundamental frequency detector and an onset detector to transform input signals into a sequence of sound events. The onset detection is inherently noisy. This paper focuses on the second stage, going from sound events to a notated score. We use a family of graphical models for this task. We allow the results of onset detection to be noisy, necessitating a search over possible segmentations of the sound events. We use a large corpus of monophonic vocal music to evaluate our system. Our results show that our approach is well-suited to the problem of music transcription. The initial onset detection reduces the number of observations and makes the system less instrument specific. The search over segmentations corrects the errors in the onset detection. Without such reasoning, these errors are magnified in subsequent rhythm transcription.