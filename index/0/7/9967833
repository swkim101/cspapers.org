This paper develops a novel methodology for using symbolic knowledge in deep learning. We deﬁne a semantic loss function that bridges between neural output vectors and logical constraints. This loss function captures how close the neural network is to satisfying the constraints on its output. An experimental evaluation shows that our semantic loss function effectively guides the learner to achieve (near-)state-of-the-art results on semi-supervised multi-class classiﬁcation. More-over, it signiﬁcantly increases the ability of the neural network to predict structured objects under weak supervision, such as rankings and shortest paths