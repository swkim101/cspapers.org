Online learning to rank for information retrieval has shown great promise in optimization of Web search results based on user interactions. However, online learning to rank has been used only in the monolingual setting where queries and documents are in the same language. In this work, we present the first empirical study of optimizing a model for Cross-Language Information Retrieval (CLIR) based on implicit feedback inferred from user interactions. We show that ranking models for CLIR with acceptable performance can be learned in an online setting although ranking features are noisy because of the language mismatch.