Stochastic optimization algorithms typically use learning rate schedules that behave asymptotically as µ(t) = µ0/t. The ensemble dynamics (Leen and Moody, 1993) for such algorithms provides an easy path to results on mean squared weight error and asymptotic normality. We apply this approach to stochastic gradient algorithms with momentum. We show that at late times, learning is governed by an effective learning rate µeff = µ0/(1 - β) where β is the momentum parameter. We describe the behavior of the asymptotic weight error and give conditions on µeff that insure optimal convergence speed. Finally, we use the results to develop an adaptive form of momentum that achieves optimal convergence speed independent of µ0.