In this paper we give approximation algorithms for several proximity problems in high dimensional spaces. In particular, we give the first Las Vegas data structure for (1 + e)nearest neighbor with polynomial space and query time polynomial in dimension d and log n, where n is the database size. We also give a deterministic 3-approximation algorithm with similar bounds; this is the first deterministic constant factor approximation algorithm (with polynomial space) for any norm. For the closest pair problem we give a roughly n I+p time Las Vegas algorithm with approximation factor O(1/plog l/p); this is the first Las Vegas algorithm for this problem. Finally, we show a general reduction from the furthest point problem to the nearest neighbor problem. As a corollary, we improve the running time for the (1 + e)approximate diameter problem from n 2-°(~2) to n 2-°(~). Our results are unified by the fact that their key component is a dimensionality reduction technique for Hamming spaces. 1 I n t r o d u c t i o n The proximity problems is a class of geometric problems which involve the notion of a distance between points in a d-dimensional space. For example, the closest pair problem, furthest pair (or diameter) problem and nearest neighbor search all belong to this class. If the dimension d is low, these problems have very efficient solutions [13, 3, 12]. However, the running time and/or space requirements of these algorithms grow exponentially with the dimension. This is unfortunate, since the high-dimensional versions of the above problems are of major and growing importance to a variety of applications, usually involving similarity search or clustering; some examples are information retrievM, image and video databases, vector quantization, data mining and pattern recognition. Therefore, a lot of recent ~ p o r t e d by Stanford Graduate Fellowship and NSF Grant IIS-9811904 research focused on approximate algorithms for these problems [4, 10, 7, 11, 6, 2]; this relaxation enables to overcome the "curse of dimensional i ty ' . In this paper we present several new results for the aforementioned proximity problems. They are unified by the fact that their key component is a dimensionality reduction technique for Hamming spaces (which we describe in more detail at the end of this Section). O u r r esu l t s . The first problem we address is the cnearest neighbor problem. In this problem the goal is to construct (for a given set P of n points from R d) a data structure, which given a query point q, finds any c-approximate nearest neighbor of q in P; the latter is defined as any point p/whose distance to q is bounded by c times the distance from q to its nearest neighbor in P. Several algorithms have been proposed recently for this problem for the cases when the similarity between points is measured by a dot-product [4], ll or 12 norm [10, 7, 11] and lc~ norm [6]. The intriguing common property of almost all of them [4, 10, 7, 11] is that they are randomized Monte Carlo, i.e. have small probability of returning an incorrect answer (the solution of [6] is deterministic, but unlike other solutions does not allow arbitrarily small approximation error). This situation stands in the contrast with the state of the art in lowdimensional Computat ion Geometry, where virtually almost all randomized algorithms are of Las Vegas type (i.e. whose correctness is always guaranteed). Thus one may ask if randomization, in particular of Monte Carlo type, is an inherent property of algorithms for high-dimensional proximity problems. We mention that from the practical perspective the Las Vegas algorithms offer a significant advantage over the Monte Carlo ones (if their running times is comparable) for the following reason: it was observed [1, 5] tha t the average error incurred by the algorithms is much lower than predicted by theoretical analysis. Therefore, one can significantly speed them up by setting artificiMly high upper bound for the error. However, a Monte Carlo algorithm canno~