Symmetrically connected recurrent networks have recently been used as models of a host of neural computations. However, biological neural networks have asymmetrical connections, at the very least because of the separation between excitatory and inhibitory neurons in the brain. We study characteristic differences between asymmetrical networks and their symmetrical counterparts in cases for which they act as selective amplifiers for particular classes of input patterns. We show that the dramatically different dynamical behaviours to which they have access, often make the asymmetrical networks computationally superior. We illustrate our results in networks that selectively amplify oriented bars and smooth contours in visual inputs.