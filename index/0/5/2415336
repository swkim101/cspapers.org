We present and analyze a sampling algorithm for the basic linear-algebraic problem of <i>l</i><inf>2</inf> regression. The <i>l</i><inf>2</inf> regression (or least-squares fit) problem takes as input a matrix <i>A</i> ∈ R<sup><i>n×d</i></sup> (where we assume <i>n</i> ≫ <i>d</i>) and a target vector <i>b</i> ∈ R<sup><i>n</i></sup>, and it returns as output <i>Z</i> = min<inf><i>x</i>∈R<sup><i>d</i></sup></inf> |<i>b - Ax</i>|<inf>2</inf>. Also of interest is <i>x</i><inf><i>opt</i></inf> = <i>A</i>+<i>b</i>, where <i>A</i><sup>+</sup> is the Moore-Penrose generalized inverse, which is the minimum-length vector achieving the minimum. Our algorithm randomly samples <i>r</i> rows from the matrix <i>A</i> and vector <i>b</i> to construct an induced <i>l</i><inf>2</inf> regression problem with many fewer rows, but with the same number of columns. A crucial feature of the algorithm is the nonuniform sampling probabilities. These probabilities depend in a sophisticated manner on the lengths, i.e., the Euclidean norms, of the rows of the left singular vectors of <i>A</i> and the manner in which <i>b</i> lies in the complement of the column space of <i>A</i>. Under appropriate assumptions, we show relative error approximations for both <i>Z</i> and <i>x</i><inf><i>opt</i></inf>. Applications of this sampling methodology are briefly discussed.