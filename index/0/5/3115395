Classification is a well-established operation in text mining. Given a set of labels <i>A</i> and a set <i>D<sub>A</sub></i> of training documents tagged with these labels, a classifier learns to assign labels to unlabeled test documents. Suppose we also had available a different set of labels <i>B</i>, together with a set of documents <i>D<sub>B</sub></i> marked with labels from <i>B</i>. If <i>A</i> and <i>B</i> have some semantic overlap, can the availability of <i>D<sub>B</sub></i> help us build a better classifier for <i>A</i>, and vice versa? We answer this question in the affirmative by proposing <i>cross-training</i>: a new approach to semi-supervised learning in presence of multiple label sets. We give distributional and discriminative algorithms for cross-training and show, through extensive experiments, that cross-training can discover and exploit probabilistic relations between two taxonomies for more accurate classification.