Named Entity Recognition is a well established information extraction task with many state of the art systems existing for a variety of languages. Most systems rely on language speciﬁc resources, large annotated corpora, gazetteers and feature engineering to perform well monolingually. In this paper, we introduce an attentional neural model which only uses language universal phonological character representations with word embeddings to achieve state of the art performance in a monolingual setting using super-vision and which can quickly adapt to a new language with minimal or no data. We demonstrate that phonological character representations facilitate cross-lingual transfer, out-perform orthographic representations and incorporating both attention and phonological features improves statistical efﬁciency of the model in 0-shot and low data transfer settings with no task speciﬁc feature engineering in the source or target language.