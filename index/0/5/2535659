In recent years, a rapidly increasing number of applications in practice requires optimizing non-convex objectives, like training neural networks, learning graphical models, maximum likelihood estimation. Though simple heuristics such as gradient descent with very few modifications tend to work well, theoretical understanding is very weak. 
 
We consider possibly the most natural class of non-convex functions where one could hope to obtain provable guarantees: functions that are "approximately convex", i.e. functions f : ℝd → ℝ for which there exists a convex function f such that for all x, |f (x) - f(x)| ≤ Δ for a fixed value Δ. We then want to minimize f, i.e. output a point x such that f(x) ≤ minx f(x) + e. 
 
It is quite natural to conjecture that for fixed e, the problem gets harder for larger Δ, however, the exact dependency of e and Δ is not known. In this paper, we significantly improve the known lower bound on Δ as a function of e and an algorithm matching this lower bound for a natural class of convex bodies. More precisely, we identify a function T : ℝ+ - ℝ+ such that when Δ = O(T(e)), we can give an algorithm that outputs a point x such that f (x) ≤ minx f (x) + e within time poly (d, 1/e). On the other hand, when Δ = Ω(T(e)), we also prove an information theoretic lower bound that any algorithm that outputs such a x must use super polynomial number of evaluations of f.