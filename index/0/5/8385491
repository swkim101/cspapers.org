As robots enter human spaces, unique perception challenges are emerging. Sensing human activity, adapting to highly dynamic environments, and acting coherently and contingently is challenging when robots transition from structured environments to human-centric ones. We approach this problem by employing context-based perception, a biologically-inspired, low-cost approach to sensing that leverages noisy, global features. Across several months, our mobile robot collected real-world, multimodal data from multi-use locations; where the same space might be used for many different activities. We then ran a series of unimodal and multimodal classification experiments. We successfully classified several aspects of situational context from noisy data, and, to our knowledge are the first group to do so. This work represents an important step toward enabling robots that can readily leverage context to solve perceptual tasks.