A dictionary data structure supports efficient <italic>search, insert,</italic> and <italic>delete</italic> operations on <italic>n</italic> keys from a totally ordered universe. Red-black trees, 2-3 trees, AVL trees, skip lists and other classic data structures facilitate <italic>O</italic>(log<italic>n</italic>) time search, insert and deletes, matching the information theoretic lower bound when access probabilities are uniform i.i.d. If access probabilities are non-uniform but still i.i.d., there are other <italic>weighted</italic> data structures such as D-trees, biased search trees, splay trees and treaps which can achieve optimality.
In many applications, however, the source of nonuniformity in access probabilities is <italic>locality of reference</italic>: examples include memory, cache, disk and buffer management and emerging applications in internetwork traffic management. In such applications, the access probability of any given key is not i.i.d., but decreases with idle time since the last access to the key.
It is possible to adjust the weighted dictionaries to achieve optimal search time even under time dependent distributions; however insert/delete times will be suboptimal at <italic>O</italic>(log<italic>n</italic>). In this paper, we present a lazy updating scheme which can be applied to weighted dictionaries to improve their amortized insert/delete performance when access probabilities decrease with time; optimality of search time is preserved. More speci%cally, let <italic>r(k)</italic> be the number of distinct keys accessed since the last access to key <italic>k</italic>- that is <italic>r(k)</italic> is the move-to-front rank of <italic>k</italic>. Let <italic>rmax(k)</italic> be the maximum rank of <italic>k</italic> during its lifetime.
 Then our lazy update scheme enables the abovementioned data structures to perform search in <italic>O</italic>(log <italic>r(k)</italic>) time and insert/delete in <italic>O</italic>(log <italic>rmax(k)</italic>) time. We illustrate our lazy update scheme in the context of a new Biased Skip List data structure and show that our bounds are optimal.