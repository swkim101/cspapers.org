Recent work has shown that inaudible signals (at ultrasound frequencies) can be designed in a way that they become audible to microphones. Designed well, this can empower an adversary to stand on the road and silently control Amazon Echo and Google Home-like devices in people’s homes. A voice command like “Alexa, open the garage door” can be a serious threat. While recent work has demonstrated feasibility, two issues remain open: (1) The attacks can only be launched from within 5 ft of Amazon Echo, and increasing this range makes the attack audible. (2) There is no clear so-lution against these ultrasound attacks, since they exploit a recently discovered loophole in hardware non-linearity. This paper is an attempt to close both these gaps. We begin by developing an attack that achieves 25 ft range, limited by the power of our ampliﬁer. We then develop a defense against this class of voice attacks that exploit non-linearity. Our core ideas emerge from a careful forensics on voice, i.e., ﬁnding indelible traces of non-linearity in recorded voice signals. Our system, LipRead , demonstrates the inaudible attack in various conditions, followed by defenses that only require software changes to the microphone.