Our understanding of several allocation algorithms basic to operating systems and to data base systems has improved substantially as a result of a number of research efforts within the past one or two years. The results have stirred considerable excitement in both theorists and practitioners. This is not only because of the inroads made into long-standing problems, but also because of the surprising nature of the results; in particular, we refer to proofs that certain classical algorithms described as approximate are in fact optimal in a strong probabilistic sense.
 The work discussed here will be classified according to the application areas, archival and dynamic storage allocation. In both cases we are concerned with the packing problems that arise in making efficient use of storage. Equivalents of the archival problems also have importance in scheduling applications [4]; however, we shall focus exclusively on the storage allocation setting.