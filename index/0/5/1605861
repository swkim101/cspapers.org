Sparsity is a desirable property in high dimensional learning. The <i>l</i><sub>1</sub>-norm regularization can lead to primal sparsity, while max-margin methods achieve dual sparsity. Combining these two methods, an <i>l</i><sub>1</sub>-norm max-margin Markov network (<i>l</i><sub>1</sub>-M<sup>3</sup>N) can achieve both types of sparsity. This paper analyzes its connections to the Laplace max-margin Markov network (LapM<sup>3</sup>N), which inherits the dual sparsity of max-margin models but is pseudo-primal sparse, and to a novel adaptive M<sup>3</sup>N (AdapM<sup>3</sup>N). We show that the <i>l</i><sub>1</sub>-M<sup>3</sup>N is an extreme case of the LapM<sup>3</sup>N, and the <i>l</i><sub>1</sub>-M<sup>3</sup>N is equivalent to an AdapM<sup>3</sup>N. Based on this equivalence we develop a robust EM-style algorithm for learning an <i>l</i><sub>1</sub>-M<sup>3</sup>N. We demonstrate the advantages of the simultaneously (pseudo-) primal and dual sparse models over the ones which enjoy either primal or dual sparsity on both synthetic and real data sets.