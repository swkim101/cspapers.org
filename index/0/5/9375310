Current inductive logic programming systems are limited in their handling of noise, as they employ a greedy covering approach to constructing the hypothesis one clause at a time. This approach also causes di(cid:14)culty in learning recursive predicates. Additionally, many current systems have an implicit expectation that the cardinality of the positive and negative examples re(cid:13)ect the \proportion" of the concept to the instance space. A framework for learning from noisy data and (cid:12)xed example size is presented. A Bayesian heuristic for (cid:12)nding the most probable hypothesis in this general framework is derived. This approach evaluates a hypothesis as a whole rather than one clause at a time. The heuristic, which has nice theoretical properties, is incorporated in an ILP system, Lime. Experimental results show that Lime handles noise better than FOIL and PROGOL. It is able to learn recursive de(cid:12)nitions from noisy data on which other systems do not perform well. Lime is also capable of learning from only positive data and also from only negative data.