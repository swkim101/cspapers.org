Reinforcement Learning (RL) problems with hidden state present signi(cid:12)cant obstacles to prevailing RL methods. In this paper, we present experiments conducted using a straightforward approach to solving such problems that trains arti(cid:12)cial neural networks with recurrent connections to represent action policies using evolutionary search. We apply this method to two di(cid:11)erent benchmark problems. The (cid:12)rst problem, involving maze navigation, has original features. Key among these features are that it is scalable, and thus provides a benchmark for investigating the performance of algorithms on problems with increasingly larger state spaces, and that it facilitates the study of inter-task transfer of search e(cid:11)ort by providing for the generation of multiple, related tasks. The second problem, New York Driving, was introduced by McCallum (1995). Previously reported results for this task provide a basis for comparison with the evolutionary approach. Singh et al. (1994) demonstrated that in RL problems with hidden state, the best memory-less policy may be a stochastic one. Of particular interest in this study is our (cid:12)nd-ing that in practice, the ability to represent stochastic policies can signi(cid:12)cantly enhance the performance of evolutionary search for policies with memory. We explore this phenomenon via the use of recurrent networks composed of stochastic units as a means for representing policies.