We give near-tight lower bounds for the sparsity required in several dimensionality reducing linear maps. First, consider the Johnson-Lindenstrauss (JL) lemma which states that for any set of <i>n</i> vectors in R<sup>d</sup> there is an A∈R<sup>m x d</sup> with m = O(ε<sup>-2</sup>log n) such that mapping by <i>A</i> preserves the pairwise Euclidean distances up to a 1 pm ε factor. We show there exists a set of n vectors such that any such A with at most s non-zero entries per column must have s = Ω(ε<sup>-1</sup>log n/log(1/ε)) if m < O(n/log(1/ε)). This improves the lower bound of Ω(min{ε<sup>-2</sup>, ε<sup>-1</sup>√(log<sub>m</sub> d)) by [Dasgupta-Kumar-Sarlos, STOC 2010], which only held against the stronger property of distributional JL, and only against a certain restricted class of distributions. Meanwhile our lower bound is against the JL lemma itself, with no restrictions. Our lower bound matches the sparse JL upper bound of [Kane-Nelson, SODA 2012] up to an O(log(1/ε)) factor. Next, we show that any m x n matrix with the <i>k</i>-restricted isometry property (RIP) with constant distortion must have Ω(k log(n/k)) non-zeroes per column if m=O(k log (n/k)), the optimal number of rows for RIP, and k < n/polylog <i>n</i>. This improves the previous lower bound of Ω(min{k, n/m}) by [Chandar, 2010] and shows that for most <i>k</i> it is impossible to have a sparse RIP matrix with an optimal number of rows.
 Both lower bounds above also offer a tradeoff between sparsity and the number of rows.
 Lastly, we show that any oblivious distribution over subspace embedding matrices with 1 non-zero per column and preserving distances in a d dimensional-subspace up to a constant factor must have at least Ω(d<sup>2</sup>) rows. This matches an upper bound in [Nelson-Nguyên, arXiv abs/1211.1002] and shows the impossibility of obtaining the best of both of constructions in that work, namely 1 non-zero per column and d ⋅ polylog <i>d</i> rows.