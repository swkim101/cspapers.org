We study the problem of finding an outlier-free subset of a set of points (or a probability distribution) in <italic>n</italic>-dimensional Euclidean space. A point <italic>x</italic> is defined to be a β-outlier if there exists some direction <italic>w</italic> in which its squared distance from the mean along <italic>w</italic> is greater than β times the average squared distance from the mean along <italic>w</italic> [1]. Our main theorem is that for any ε>0, there exists a (1-ε) fraction of the original distribution that has no <italic>O</italic>(\frac{<italic>n</italic>}{ε}(<italic>b</italic>+log \frac{<italic>n</italic>}{ε))-outliers, improving on the previous bound of <italic>O(n</italic>^7<italic>b</italic>/ε). This bound is shown to be nearly the best possible. The theorem is constructive, and results in a \frac{1}{1-ε} approximation to the following optimization problem: given a distribution μ (i.e. the ability to sample from it), and a parameter ε>0, find the minimum β for which there exists a subset of probability at least (1-ε) with no β-outliers.