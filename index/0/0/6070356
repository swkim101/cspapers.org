In unsupervised semantic role labeling, identifying the role of an argument is usually informed by its dependency relation with the predicate. In this work, we pro-pose a neural model to learn argument embeddings from the context by explicitly incorporating dependency relations as multiplicative factors, which bias argu-ment embeddings according to their dependency roles. Our model outperforms existing state-of-the-art embeddings in un-supervised semantic role induction on the CoNLL 2008 dataset and the SimLex999 word similarity task. Qualitative results demonstrate our model can effectively bias argument embeddings based on their dependency role.