Neural network-based encoder-decoder models are among recent attractive methodologies for tackling natural language generation tasks. This paper investigates the usefulness of structural syntactic and semantic information additionally incorporated in a baseline neural attention-based model. We encode results obtained from an abstract meaning representation (AMR) parser using a modiÔ¨Åed version of Tree-LSTM. Our proposed attention-based AMR encoder-decoder model improves head-line generation benchmarks compared with the baseline neural attention-based model.