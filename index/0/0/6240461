Many important classification problems are polychotomies, i.e. the data are organized into $K$ classes with $K>2$. Given an unknown function $F : Omega \to {1, \dots, K}$ representing a polychotomy, an algorithm aimed at ``learning'' this polychotomy will produce an approximation of $F$, based on a set of pairs ${(\mathbf{x}^p, F(\mathbf{x}^p))}_{p=1}^P$. Although in the wide variety of learning tools, there exist some learning algorithms capable of handling polychotomies, many of the interesting tools were designed by nature for dichotomies ($K=2$). Therefore, many researchers are compelled to use techniques to decompose a polychotomy into a series of dichotomies and thus to apply their favorite algorithms to the resolution of a general problem. A decomposition method based on error-correcting codes has been lately proposed and shown to be very efficient. However, this decomposition is designed only on the basis of $K$ without taking the data into account. In this paper, we explore alternatives to this method, still based on the fruitful idea of error-correcting codes, but where the decomposition is inspired by the data at hand. The efficiency of this approach, both for the simplicity of the model and for the generalization, is illustrated by some numerical experiments.