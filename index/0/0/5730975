For smooth and strongly convex optimizations, the optimal iteration complexity of the gradient-based algorithm is O(âˆšk log 1/e), where k is the condition number. In the case that the optimization problem is ill-conditioned, we need to evaluate a large number of full gradients, which could be computationally expensive. In this paper, we propose to remove the dependence on the condition number by allowing the algorithm to access stochastic gradients of the objective function. To this end, we present a novel algorithm named Epoch Mixed Gradient Descent (EMGD) that is able to utilize two kinds of gradients. A distinctive step in EMGD is the mixed gradient descent, where we use a combination of the full and stochastic gradients to update the intermediate solution. Theoretical analysis shows that EMGD is able to find an e-optimal solution by computing O(log 1/e) full gradients and O(k2 log 1/e) stochastic gradients.