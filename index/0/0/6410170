We consider the problem of approximating the entropy of a discrete distribution <i>P</i> on a domain of size <i>q</i>, given access to <i>n</i> independent samples from the distribution. It is known that <i>n</i> ≥ <i>q</i> is necessary, in general, for a good additive estimate of the entropy. A problem of multiplicative entropy estimate was recently addressed by Batu, Dasgupta, Kumar, and Rubinfeld. They show that <i>n</i> = <i>q</i>α suffices for a factor-α approximation, α > 1.
 We introduce a new parameter of a distribution--its <i>effective alphabet size q<inf>ef</inf></i>(<i>P</i>). This is a more intrinsic property of the distribution depending only on its entropy moments. We show <i>q<inf>ef</inf></i> ≤ Õ(<i>q</i>). When the distribution <i>P</i> is essentially concentrated on a small part of the domain <i>q<inf>ef</inf></i> ≪ <i>q</i>. We strengthen the result of Batu et al. by showing it holds with <i>q<inf>ef</inf></i> replacing <i>q</i>.
 This has several implications. In particular the rate of convergence of the maximum-likelihood entropy estimator (the empirical entropy) for both finite and infinite alphabets is shown to be dictated by the effective alphabet size of the distribution. Several new, and some known, facts about this estimator follow easily.
 Our main result is algorithmic. Though the effective alphabet size is, in general, an unknown parameter of the distribution, we give an efficient procedure, with access to the alphabet size only, that achieves a factor-α approximation of the entropy with <i>n</i> = Õ (exp {α <sup>1/4</sup> · log<sup>3/4</sup> <i>q</i> · log<sup>1/4</sup> <i>q<inf>ef</inf></i>}). Assuming (for instance) log <i>q<inf>ef</inf></i> ≪ log <i>q</i> this is smaller than any power of <i>q</i>. Taking α → 1 leads in this case to efficient <i>additive</i> estimates for the entropy as well. In particular, this result shows that for many natural scenarios, a tight estimation of the entorpy may be achieved using a sub-linear sample.
 Several extensions of the results above are discussed.