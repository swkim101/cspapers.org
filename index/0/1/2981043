We revisit stereo matching functions, a topic that is considered well understood, from a different angle. Our goal is to discover a transformation that operates on the cost or similarity measures between pixels in binocular stereo. This transformation should produce a new matching curve that results in higher matching accuracy. The desired transformation must have no additional parameters over those of the original matching function and must result in a new matching function that can be used by existing local, global and semi-local stereo algorithms without having to modify the algorithms. We propose a transformation that meets these requirements, taking advantage of information derived from matching the input images against themselves. We analyze the behavior of this transformation, which we call Self-Aware Matching Measure (SAMM), on a diverse set of experiments on data with ground truth. Our results show that the SAMM improves the performance of dense and semi-dense stereo. Moreover, as opposed to the current state of the art, it does not require distinctiveness to match pixels reliably.