We introduce a computationally efficient methodology for generating and recognizing free-space movements for humanoids robots. This methodology operates on exemplar-based representations of behaviors. Our method for actuating humanoid robots allows us to perform variations on a given behavior, resulting in a very human like movement appearance. Besides control, this method also facilitates classification of perceived human and humanoid robot movement. We demonstrate the method on a physically-simulated humanoid robot with 132 degrees-of-freedom and evaluate our movement classification methodology on two data sets: human motion-capture data and joint-angle data sampled from the simulated robot.