Recognizing objects from the point cloud captured by modern 3D sensors is an important task for robots operating autonomously in real-world environments. However, the existing well-performing approaches typically suffer from a trade-off between resolution of representation and computational efficiency. In this paper, raw point cloud normals are fed into the Point Convolution Network (PCN) without any other representation converts. The point cloud set disordered and unstructured problems are tackled by Kd-tree-based local permutation and spatial commutative pooling strategies proposed in this paper. Experiments on ModelNet illustrate that our method has two orders of magnitude less floating point computation in each non-linear mapping layer while it contributes to significant classification accuracy improvement. Compared to some of the state-of-the-art methods using the 3D volumetric image convolution, the PCN method also yields comparable classification accuracy.