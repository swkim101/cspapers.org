This paper considers the Valiant framework as it is applied to the task of learning logical concepts from random examples. It is argued that the current interpretation of this Valiant model departs from common sense and practical experience in a number of ways: it does not allow sample dependent bounds, it uses a worst case rather than an average case analysis, and it does not accommodate preferences about hypotheses. It is claimed that as a result, the current model can produce overlyconservative estimates of confidence and can fail to model the logical induction process as it is often implemented. A Bayesian approach is developed, based on the sample dependent notion of disagreement between consistent hypotheses. This approach seems to overcome the indicated problems.