In this poster we describe experiments in information retrieval using a new method for scoring correlated features. This method uses information about word co-occurrences in the documents ranking high after the initial scoring to reduce combined scores of correlated words. We have experimented with this technique in conjunction with both simple Okapi scoring and a query expansion method using a probabilistic model, improving system performance in the context of TREC standardized tasks. 1 Correlated Features When combining the scores of the features (e. g. words or n-grams) contained in the intersection of a query and a document to obtain the document’s total score, it is useful to apply some mechanism to reduce the contribution . of highly correlated features. This issue becomes even more important in systems where query expansion is applied and the expanded query contains a large number of often highly correlated features and has been addressed in the past using various approaches (e.g. vector retrieval methods [7], [3]). We experimented with a new method for scoring correlated features adjustment in conjunction a query expansion method using probabilistic model [2], 151. In our experiments we tried to obtain the information about the correlated features from the results of first pass scoring. To estimate the closeness of the features z and y, we started with the average mutual information between two binary sources, X and Y We used the actual counts of feature Z, feature y and simultaneous occurrence of z and y in top N documents after the first pass to estimate the probabilities P(z), P(y), and P(z, y), respectively. We defined the closeness of a given feature pair as Permission to make digital/hard copy of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication and its date appear, and notice is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. SIGIR’BE, Melbourne, Australia @ 1998 ACM l-58113-015-5 S/98 $5.00. Salim Roukos IBM T. J. Watson Research Center POB 718, Yorktown Heights, NY 10598 roukosOwatson.ibm.com C(x, Y) = 4x1 I Yl )b Nc(xl, ~1) + c(xo, yoyog Nc(xo, YO) 444Yl) C(~O)C(YO) --(Xl, yo)log Nc(xl, YO) c(+(Yo) 4x0, Yl )b where ~(21, yi) is number of documents in top N containing both feature x and feature y, C(Q) is number of documents in top N containing feature 2, ~(21,~s) is number of documents in top N containing feature z and not containing feature y etc. We have found that using the negative signs preceding the terms when only one of the features is present leads to a better performance than when the mutual information criterion is used. Closeness of the feature pairs may be applied in many different ways to penalize the combined scores of highly correlated features. The results shown in Table 2 were obtained the following way: the features contributing to the score of given query/document combination were ranked according to their scores. Each feature’s score was then adjusted by using the formula: 4x1 s’(x) = ac(;e,Y,,,) where s(x) and S’(Z) are the original and adjusted scores of feature 2, ynaZ is the feature with maximum C(z,y) and a is a constant. No adjustment was performed in the cases where C(z, y) was smaller than 0. Table 1 shows the top ten correlated morph pairs, using the top 1000 documents, in TREC-5 topic 267, Should U.S. firefighters incorporate training in procedures and equipment utilized by foreign firefighters to improve capability of coping with ever-changingfirejighting conditions? 1 1 I burn 2 firefighter * 3 burn 4 fire * 5 procedure * fire *