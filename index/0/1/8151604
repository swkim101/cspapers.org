Extreme resource constraints in web application development environments often lead to minimal formal testing. This lack of testing compounds the problem of low consumer retention. In this paper, we analyze an existing study of the consumer-perceived severities of 400 realworld web application faults. We provide concrete guidelines to increase the perceived return-on-investment of testing. We show that even coarse-grained, automated test cases can detect high severity faults. Developers can also mitigate consumer perceptions of fault severity by presenting faults using specific idioms that minimize disruptions to application interaction. Finally, we examine the trade offs between various user-session-based test suite reduction approaches for web applications and the severity of uncovered faults. Overall, we show that even modest testing approaches are able to provide significant predicted gains in consumer satisfaction by focusing on flagging and preventing high severity faults.