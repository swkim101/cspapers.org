A new class of simple, adaptive, under-actuated and compliant robot hands has recently attracted the interest of the robotics community. The under-actuated mechanisms and the structural compliance used in these hands facilitate and robustify not only grasping but also the execution of dexterous, in-hand manipulation tasks. Another significant characteristic of the particular hands is that they are able to efficiently grasp a wide range of everyday life objects even under significant object pose uncertainties. However, these hands, are difficult to model due to kinematic constraints introduced by the underactuation and the use of complex flexure joints. Moreover, adaptive hands tend to reconfigure upon contact with the object surface, imposing certain parasitic object motions. In this paper, we propose a learning scheme that uses the contact force measurements collected from tactile sensors to estimate the post-contact reconfiguration of the hand-object system and the imposed parasitic object motion. The learning scheme's estimates are compared with “ground truth” data that describe the actual motion of the object and that are collected using a vision based motion capture system. The proposed learning scheme can be used with any type of adaptive robot hand and its efficiency is experimentally validated using extensive paradigms involving different hand designs and various everyday life objects.