This paper introduces a new sampling strategy and shows that superior performance can be obtained for a range of sampling based robotic motion planners, used in scenarios with low task variance, as found in many vision guided pick and place operations. The strategy uses kernel density estimation to identify regions with high probability of containing configurations being part of feasible solutions, and use the estimation to bias sampling. The kernel densities are initialized with a uniform distribution and are continuously updated, whenever paths are successfully planned and optimized. The system is thereby self-learning and improves performance over time. The sampler is tested on a variety of planners and against other sampling methods in two different scenarios containing robotic arms with 6 degrees of freedom and compared to a state-of-the-art optimization based planning algorithm. Tests show that the sampler learns fast and improves both time taken for solving problems and the quality of the resulting paths compared to other samplers.