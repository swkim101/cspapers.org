We show that a randomly selected N-tuple x→ of points of Rn with probability > 0 is such that any multi-layer percept ron with the first hidden layer composed of h1 threshold logic units can implement exactly 2 Σi=0h1n (N-1 i) different dichotomies of x→. If N > h1n then such a perceptron must have all units of the first hidden layer fully connected to inputs. This implies the maximal capacities (in the sense of Cover) of 2n input patterns per hidden unit and 2 input patterns per synaptic weight of such networks (both capacities are achieved by networks with single hidden layer and are the same as for a single neuron). Comparing these results with recent estimates of VC-dimension we find that in contrast to the single neuron case, for sufficiently large n, and h1 the VC-dimension exceeds Cover's capacity.