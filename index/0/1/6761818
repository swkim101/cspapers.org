Anticipatory   systems   have   been   shown   to   be   useful   in discrete,   symbolic   systems.   However,   non­symbolic anticipatory systems are less well understood. In this paper, we explore the use of anticipation within the framework of connectionist   networks   to   bootstrap   from   an   innate behavior;   to drive a reinforcement  signal;  and to provide feedback on the learnability of a task. Developmental Robotics Developmental   robotics   is   an   approach   to   artificial intelligence   that   focuses   on   the   autonomous   self­ organization of a general­purpose control  system. Rather than being programmed to solve a particular task, a robot in the developmental robotics paradigm is programmed to develop  sophisticated,  self­organized  representations  and self­motivated behaviors  over  time.  Such a robot  begins with nothing but a “seed” program consisting of a simple innate   behavior,   a   motivational   system,   and   a   learning system. There is no a priori task to master, only a general goal to develop “mentally” and behaviorally. Developmental   robotics   brings   together   several paradigms   falling   under   a   range   of   rubrics,   including embodied cognition, biologically­based robotics, artificial animals   (animats),   reinforcement   learning,   evolutionary computation,   and   machine   learning.   However, developmental robotics is unique in its insistence on goal non­specificity   for   systems.   Therefore,   central   issues   in this  field  are  those  at   the heart  of  artificial   intelligence: What feature detectors and concepts should be built  into the   system?  How  does   the   system “decide”  what   to  do next? What drives the system to “want” to do anything? Indeed, perceiving the world and deciding which actions to   perform   are   two   of   the   hardest   problems   of   AI. However,   developmental   robotics   advocates   combining these two problems into one and letting a solution grow through the experience of the robot. Many   learning   mechanisms   that   are   based   on Copyright © 2005, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. optimizing a fitness measure are not useful in this domain because there is no task by which to measure performance. However,   the  idea of  anticipation  has been  found to  be useful   in   multiple   ways   in   the   learning   and   motivation processes. This paper explores three roles of anticipation in a connectionist developmental robotics context. Roles of Anticipation Recently,   anticipatory   learning   systems   have   gained increasing attention in the field. Butz, Sigaud, and Gerard (2002)   give   an   overview   of   several   such   systems. However,   their  summary is   limited to those models  that can   be   framed   in   a   Markovian   paradigm.   That   is, anticipation is explored in a  partially observable Markov decision process (POMDP). However, to have anticipation work in such a framework requires a system to have three properties. First, the Markovian assumption must hold: the next state of the system depends only on the current state. Therefore, the next state cannot be determined by context, unless context becomes part of the previous state. Second, perceptions and actions must be appropriately discretized. Third,   these  discrete   representations  cannot  change  over the   course   of   learning.   However,   we   view   these representations  of  perceptions   and  actions  as   something that should be created by the system itself. How can the system's representations of its perceptions be based on the actions made by the system, while the representations of the actions are in turn based on the perceptions? We believe that  the answer  to this chicken­and­egg dilemma   is   to   start   a   system   with   a   set   of   innate representations of perceptions and actions,  and allow the system  to  gradually  modify   them over   time.    Thus,  we believe   that   the   space   of   representations   should   be continuous   and   mutable.   This   leads   us   in   a   different direction   than,   say,   Witkowski's   Dynamic   Expectancy Model (2002). Rather than creating a “hypothesis engine” and   a   logic   for   determining   when   and   how   to   create hypotheses,   we   have   implemented   our   system   as   an artificial   neural   network.   The   main   difference   between such   a   Markovian   system   and   a   neural   network (connectionist) system is the form of the hypotheses and actions.  As  noted,   the  Markov  process  requires  discrete symbols   and  actions,   and   therefore  discrete  hypotheses. However,   artificial   neural   networks   can   utilize   non­ symbolic, distributed representations, and thus implement a  continuum of  hypotheses  and  actions.   In   this  manner, distributed,   connectionist   representations   allow   many hypotheses   to  be   active   and   tested   at   any  one   time.   In addition, such a system also allows for a system to deal naturally with probabilistic and noisy environments. Connectionist   systems   offer   the   ability   to   exploit information without having a full­fledged hypothesis. We have found that the question of what can be learned from a prediction task is much subtler in a connectionist network than in a POMDP, and potentially much more powerful. In the following sections we present three different methods that can be used in self­organizing anticipatory systems. Anticipation for Bootstrap Learning In   a   series   of   now­classic   experiments,   Elman   (1990) showed  that  a connectionist  network that  was trained to predict the next word when given a random sentence could not, of course, guess the next word exactly. However, the prediction   task   did   force   the   network   to   develop representations that reflected the syntax and semantics of the grammar of the sentences. His experiments show that no   innate   knowledge   is   needed   in   order   to   develop grammatical concepts such as noun and verb. One   can   ask   a   similar   question   in   robotics:   How much   innate   information  does   a  developmental   robotics system need to know about its own sensors and actuators? Pierce and Kuipers (1997) demonstrated that a simulated robot armed with not much more than the ability to make correlations and predictions can learn most everything that it  needs to know about itself,  including its  sensor types, sensor positions, and its degrees of freedom of movement. Their   system   first   gathered   information   from   its sensors as the simulated robot made random movements in a room. At first, the robot does not know if a particular sensor   reading   is   a   camera   pixel,   a   sonar   reading,   or something   else   entirely.   However,   after   analyzing   the qualitative properties of sensor readings over time, sensors could then be clustered into similar groups. For example, each sonar sensor varies over a similar range of values as the  others,   as  do   each  camera   pixel,   laser   sensor,   joint reading, etc. Each sensor in a cluster is then arbitrarily assigned a position in a low­dimensional (often 2D) space (Pierce and Kuipers 1997). These positions are adjusted so as to reflect associated pairwise distances between the sensor readings in a given cluster, in a manner similar to learning in a self­ organizing map (Kohonen, 2001). Starting with no knowledge about its sensor types or sensor topology, such a system now has a foundation to take   advantage   of   higher­order   analysis,   such   as prediction.     First,   higher­order   features   (such   as discontinuities,   local   minima,   and   local   maxima)     are proposed.   Proposed   features   are   evaluated   based   on stability,  predictive  power,  and  extensibility   (Pierce  and Kuipers, 1997). Evidence is collected for motor commands that cause sensor reading changes in predictable ways. In such systems, anticipation and correlation are tools used   to   bootstrap   a   robot   from   having   zero  knowledge about itself,  to being able to understand its  body, sensor topology, and environment. Anticipation as a Reinforcement Signal As we have seen,  anticipation can be used to create  the foundations of basic concepts of self. We now examine a system that uses anticipation at a higher level.  Marshall, Blank, and Meeden (2004) used anticipation as the basis for generating an internal reinforcement signal for training a   simulated   robot.     In   this   system,   a   simple   recurrent network (SRN) was used to generate motor actions to be performed by the robot,  together with predictions  of  the robot's next sensory state.   Both sensory states and motor actions   were   represented   as   continuous,   distributed patterns of network activation values. The network architecture used is shown in Figure 1. The robot's predicted next sensory state,  P, is determined by its current motor action Min, its current sensory state S, and   its   past   experience,   represented   by   the   recurrent context layer  C  of the network.   The robot's  next motor action to perform, Mout, is determined by S and C, with the Min  layer   temporarily  disabled.    The robot's  anticipatory subsystem comprises the Min,  S,  C, and P portions of the network, while its control subsystem comprises the  S,  C, and Mout portions.  The weights from the S and C banks of units   to   the   hidden   layer   are   shared   between   both subsystems.   Training proceeds in an interleaved fashion, with the robot learning to anticipate while simultaneously learning to act in its environment. Sensory states consisted of a two­dimensional spatial representation of the robot's visual field.  On each training step, the robot's observed sensory state was compared to the state predicted by the network, and a spatial map of the predic