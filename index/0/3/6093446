Experiments are presented which measure the perplexity reduction derived from incorporating into the predictive model utilised in a standard tag-n-gram part-of-speech tagger, contextual information from previous sentences of a document. The tagset employed is the roughly-3000-tag ATR General English Tagset, whose tags are both syntactic and semantic in nature. The kind of extrasentential information provided to the tagger is semantic, and consists in the occurrence or non-occurrence, within the past 6 sentences of the document being tagged, of words tagged with particular tags from the tagset, and of boolean combinations of such conditions. In some cases, these conditions are combined with the requirement that the word being tagged belong to a particular set of words thought most likely to benefit from the extrasentential information they are being conjoined with. The baseline model utilized is a maximum entropy-based t ag -n gram tagging model, embodying a standard tag-n-gram approach to tagging: i.e. constraints for tag trigrams, bigrams, and and the word-tag occurrence frequency of the specific word being tagged, form the basis of prediction. Added into to this baseline tagging model is the extrasentential semantic information just indicated. The performance of the tagging model with and without the added contextual knowledge is contrasted, training from the 850,000word ATR General English Treebank, and testing on the accompanying 53,000-word test treebank. Results are that a significant reduction in testset perplexity is achieved via the added semantic extrasentential information of the richer model. The model with both long-range tag triggers and more complex linguistic constraints achieved a perplexity reduction of 21.4%. 1 I n t r o d u c t i o n It appears intuitively that information from earlier sentences in a document ought to help reduce uncertMnty as to a word's correct par t of-speech tag. This is especially so for a large semantic and syntactic tagset such as the roughly-3000-tag ATR General English Tagset (Black et al., 1996; Black et al., 1998). And in fact, (Black et al., 1998) demonstrate a significant "tag trigger-pair" effect. That is, given that certain "triggering" tags have already occurred in a document, the probability of occurrence of specific "triggered" tags is raised significantly--with respect to the unigram tag probability model. Table 1, taken from (Black et al., 1998), provides examples of the tag trigger-pair effect. Yet, it is one thing to show that extrasentential context yields a gain in information with respect to a unigram tag probability model. But it is another thing to demonstrate that extrasentential context supports an improvement in perplexity vis-a-vis a part-of-speech tagging model which employs s ta te-ofthe-ar t techniques: such as, for instance, the tagging model of a maximum entropy t ag -n -g rambased tagger. The present paper undertakes just such a demonstration. Both the model underlying a standard tag-n-gram-based tagger, and the same model augmented with extrasentential contextual information, are trMned on the 850,000-word ATR General English Treebank (Black et al., 1996), and then tested on the accompanying 53,000-word test treebank. Performance differences are measured, with the result that semantic information from previous sentences within a document is shown to help significantly in improving the perplexity of tagging