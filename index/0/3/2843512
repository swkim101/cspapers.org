In this paper we present an average case anal ysis of the naive Bayesian classi er a sim ple induction algorithm that performs well in many domains Our analysis assumes a monotone M of N target concept and train ing data that consists of independent Boolean attributes The analysis supposes a known target concept and distribution of instances but includes parameters for the number of training cases the number of irrelevant rel evant and necessary attributes the proba bility of each attribute and the amount of class noise Our approach di ers from most previous average case analyses by introduc ing approximations to achieve computational tractability This lets us explore the behav ioral implications for larger training and at tribute sets than the earlier exact analyses and experimental studies show that the anal ysis makes very accurate predictions despite its use of approximations In closing we sug gest promising directions for future research on the average case analysis of induction Introduction and Motivation Typical theoretical analyses of machine learning focus on worst case results including those in the probably approximately correct framework Haussler Although this approach lets analysts obtain quite gen eral distribution free results it also means their pre dictions of learning rate are much slower than those ob Also a liated with the DaimlerChrysler Research Technology Center Palo Alto and the Center for the Study of Language and Information at Stanford University served in practice As a result the link between theory and experiment in machine learning has become tenu ous leading some researchers to explore other paths An alternative approach involves the average case analysis of speci c induction algorithms on domains with known characteristics For example Pazzani and Sarrett report early results of this sort for a conjunctive learning method and similar studies have been done for decision stumps Iba Langley the naive Bayesian classi er Langley Iba Thomp son nearest neighbor Langley Iba and k nearest neighbor Okamoto Nobuhiro Each analysis produced predictions about the e ect of domain characteristics averaged over di erent train ing sets that t experimental data very closely However this theoretical accuracy came with a price For even simple methods like naive Bayes and nearest neighbor the calculations needed to predict behavior could take drastically longer than actually running ex periments with synthetic data even when the latter averaged over many training sets The di culty re sulted from the analyses reliance on the exact calcu lation of probabilities for all possible combinations of events For many induction methods the number of such events grows exponentially with the number of at tributes and size of the training set This meant that theoretical predictions were only possible for small do mains and early parts of the learning curve In this paper we present a more tractable approach to the average case analysis of induction algorithms We demonstrate the framework with a new treatment of the naive Bayesian classi er both because of the grow ing interest with this simple yet powerful method e g Domingos Pazzani and because our earlier results were especially problematic in computational terms The new analysis uses many of the same tech niques as the previous one but it introduces approx Average Case Analysis of Naive Bayes imations based on the normal distribution that let us calculate means and variances for the sums and dif ferences of quantities rather than reasoning about the probabilities of their explicit combinations The re sult is a tractable average case analysis of naive Bayes that as experimental studies reveal remains accurate despite its use of approximations A Brief Review of Naive Bayes Although it has a long history in pattern recognition Duda Hart the naive Bayesian classi er rst appeared in the machine learning literature as a straw man against which to compare more sophisti cated methods e g Cestnik Konenenko Bratko Only gradually did researchers become aware of its potential but now it is widely recognized as a viable and robust approach to supervised induction Before beginning the analysis we should review the manner in which naive Bayes operates The method represents each class with a single probabilistic sum mary each having an associated class probability or base rate p C which speci es the probability that one will observe a member of class C Every descrip tion also includes an associated conditional probability distribution for each attribute For symbolic domains on which we will focus here one typically stores a dis crete distribution for each attribute in a description with each p vjC term specifying the probability of value v given an instance of class C To classify a new instance I which is simply a conjunc tion of attribute values V vj the naive Bayesian clas si er applies Bayes theorem to determine the proba bility of each description given the instance giving P CijI P Ci P I jCi P I P Ci QJ j P vj jCi PK k P Ck QJ j P vj jCk where K is the number of classes J is the number of attributes and P vj jCk is the conditional probability for the observed value of attribute j given the class Ck The product of conditional probabilities comes from the assumption that attributes are independent given the class which greatly simpli es the computation of Golea and Marchand report an average case analysis of perceptron learning that also incorporates nor mal approximations but their work includes other ideas from statistical mechanics that make it inaccessible to the machine learning community the class scores and eases the induction process After calculating P CijI for each class the algorithm as signs the instance to the class with the highest overall score or probability Although the above formulation of naive Bayes is the traditional one we can express the score for each class in another form that is more tractable for analyti cal purposes The basic idea is that if we are con cerned only with predictive accuracy we can invoke any monotonic transformation that does not a ect the ordering on class scores One transformation involves removing the denominator which is the same for each class and another involves taking the logarithm of the numerator Together these produce a new score