We present a visually guided, dual-arm, industrial robot system that is capable of autonomously flattening garments by means of a novel visual perception pipeline that fully interprets high-quality RGB-D images of a clothing scene based on an active stereo robot head. A segmented clothing range map is B-Spline smoothed prior to being parsed by means of shape and topology analysis into `wrinkle' structures. The length, width and height of each wrinkle is used to quantify the topology of each wrinkle and thereby rank wrinkles by size such that a greedy algorithm can identify the largest wrinkle present. A flattening plan optimised for the largest detected wrinkle is formulated based on dual-arm manipulation. We report the validation of our autonomous flattening behaviour and observe that dual-arm flattening requires significantly fewer manipulation iterations than single-arm flattening. Our experimental results also reveal that the flattening process is heavily influenced by the quality of the RGB-D sensor: use of a custom off-the-shelf high-resolution stereo-based sensor system outperformed a commercial low-resolution kinect-like camera in terms of required flattening iterations.