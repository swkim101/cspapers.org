A word in natural language can be polysemous, having multiple meanings, as well as synonymous, meaning the same thing as other words. Word sense induction attempts to ﬁnd the senses of polysemous words. Synonymy detection attempts to ﬁnd when two words are interchangeable. We combine these tasks, ﬁrst inducing word senses and then detecting similar senses to form word-sense synonym sets ( synsets ) in an unsupervised fashion. Given pairs of images and text with noun phrase labels, we perform synset induction to produce collections of underlying concepts described by one or more noun phrases. We ﬁnd that considering multi-modal features from both visual and textual context yields better induced synsets than using either context alone. Human evaluations show that our unsupervised, multi-modally induced synsets are comparable in quality to annotation-assisted ImageNet synsets, achieving about 84% of ImageNet synsets’ approval.