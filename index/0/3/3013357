The transmission capacity of today's high-speed networks is often greater than the capacity of an end-system (such as a server or a remote client) to consume the incoming data. The mismatch between the network and the end-system, which can be exacerbated by high end-system workloads, will result in incoming packets being dropped at different points in the packet receiving process. In particular, a packet may be dropped in the NIC, in the kernel ring buffer, and (for rate based protocols) in the socket buffer. To provide reliable data transfers, these losses require retransmissions, and if the loss rate is high enough result in longer download times. In this paper, we focus on UDP-like rate based transport protocols, and address the question of how best to estimate the rate at which the end-system can consume data which minimizes the overall transfer time of a file.
 We propose a novel queueing network model of the end-system, which consists of a model of the NIC, a model of the kernel ring buffer and the protocol processing, and a model of the socket buffer from which the application process reads the data. We show that using simple and approximate queueing models, we can accurately predict the effective end-system bottleneck rate that minimizes the file transfer time. We compare our protocol with PA-UDP, an end-system aware rate based transport protocol, and show that our approach performs better, particularly when the packet losses in the NIC and/or the kernel ring buffer are high. We also compare our approach to TCP. Unlike in our rate based scheme, TCP invokes the congestion control algorithm when there are losses in the NIC and the ring buffer. With higher end-to-end delay, this results in significant performance degradation compared to our reliable end-system aware rate based protocol.