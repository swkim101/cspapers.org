It is a well-known intuition that human sentence understanding works in an incremental fashion, with a seemingly constant update of the interpretation through the left-to-right processing of a string. Such intuitions are backed up by experimental evidence dating from at least as far back as Marslen-Wilson (1973), showing that under many circumstances, interpretations are indeed updated very quickly. 
 
From a parsing point of view it is interesting to consider the structure-building processes that might underlie incremental interpretation---what kinds of partial structures are built during sentence processing, and with what time-course? 
 
In this talk I will give an overview of the state-of-the-art of experimental psycholinguistic research, paying particular attention to the time-course of structure-building. The discussion will focus on a new line of research (some as yet unpublished) in which syntactic phenomena such as binding relations (e.g., Sturt, 2003) and unbounded dependencies (e.g., Aoshima, Phillips, & Weinberg, in press) are exploited to make a very direct test of the availability of syntactic structure over time. 
 
The experimental research will be viewed from the perspective of a space of computational models, which make different predictions about time-course of structure building. One dimension in this space is represented by the parsing algorithm used: For example, within the framework of Generalized Left Corner Parsing (Demers, 1977), algorithms can be characterized in terms of the point at which a context-free rule is recognized, in relation to the recognition-point of the symbols on its right-hand side. Another relevant dimension is represented by the type of grammar formalism that is assumed. For example, with bottom-up parsing algorithms, the degree to which structure-building is delayed in right-branching structures depends heavily on whether we employ a traditional phrase-structure formalism with rigid constituency, or a cateogorial formalism with flexible constituency (e.g., Steedman, 2000). 
 
I will argue that the evidence is incompatible with models which predict systematic delays in the construction of syntactic structure. In particular, I will argue against both head-driven strategies (e.g., Mulders, 2002), and purely bottom-up parsing strategies, even when flexible constituency is employed. Instead, I will argue that to capture the data in the most parsimonious way, we should turn our attention to those models in which a fully connected syntactic structure is maintained throughout the processing of a string.