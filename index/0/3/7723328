We describe a neural shift-reduce parsing model for CCG , factored into four unidirectional LSTMs and one bidirectional LSTM. This factorization allows the linearization of the complete parsing history, and results in a highly accurate greedy parser that outperforms all previous beam-search shift-reduce parsers for CCG . By further deriving a globally optimized model using a task-based loss, we improve over the state of the art by up to 2 . 67% labeled F1.