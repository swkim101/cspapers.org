During most of the last two decades, computational linguists and AI researchers working on natural language have assumed that phrase structure grammars, despite their computational tractability, were unsatisfactory devices for expressing the syntax of natural languages, however, during the same period, they have come to realize that transformational grammars, whatever their linguistic merits, are computationally intractable as they stand. The assumption, unchallenged for many years, that PSG's were inadequate for natural languages is based on arguments originally advanced by transformational linguists in the late 1950''s and early 1960's. but recent work has shown that none of those arguments were valid. The present paper draws on that work to argue that (i) there is no reason, at the present time, to think that natural languages are not context-free languages, (ii) there are good reasons to think that the notations needed to capture significant syntactic generalizations will characterize phrase structure grammars or some minor generalization of them, and (iii) there are good reasons for believing that such grammars, and the monostratal representations they induce, provide the necessary basis for the semantic interpretation of natural languages. If these arguments are valid, then the prospects for a fruitful interaction between theoretical linguistics and AI are much brighter than they would otherwise be.