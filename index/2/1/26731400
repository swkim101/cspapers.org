The selection of kernel parameters is an open problem in the training of nonlinear support vector machines. The usual selection criterion is the quotient of the radius of the smallest sphere enclosing the training features and the margin width. Empirical studies on real-world data using Gaussian and polynomial kernels show that the test error due to this criterion is often much larger than the minimum test error. In other words, this criterion can be suboptimal or inadequate. Hence, we propose augmenting the usual criterion with a traditional measure of class separability in statistical feature selection. This measure employs the within-class and betweenclass scatter in feature space, which is equivalent to computing the pooled covariance matrix trace and the distance between class means. We show empirically that the new criterion results in improved generalization.