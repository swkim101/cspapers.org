Mining knowledge from textual data has traditionally been applied on web-based publicly available resources, such as Wikipedia, online news, scientific publications and social medial such as Facebook and Twitter. In recent years, the increased maturity and accessibility of natural language processing and text mining algorithms have now raised serious interests beyond academia to uncover the chunk of corporate knowledge buried in hard-to-process natural language descriptions and reports. Such textual data are often in larger volumes and contain more valuable than numerical data. To make unstructured textual data written in natural languages ready for use in downstream data mining tasks, a typical workflow consists of anonymisation, text normalisation, domain entity and relation recognition. Despite intensive research efforts in the past decade, this seemingly simple workflow remains challenging when facing real-world text. The dominant issues are but not limited to: Lack of labelled training data: despite the fact that supervised learning based algorithms are more effective than unsupervised ones, it is often difficult if not impossible to obtain large amount of labelled documents. Not-so-reproducible results: the results of current algorithms are highly dependent on the chosen combination of the pre-processing steps and their ordering. The performance are sensitive to the statistical and linguistic features chosen. Lack of domain-independent representations: there is the lack of assurance that algorithms performing well for one text will also work for another. In this invited talk, we will take constructing knowledge nets as the objective, looking into the methods and techniques ranging from traditional feature-based distributional representation of words and phrases to the more recent deep learning enabled distributed vector representations. A demo Figure 1: A Knowlege Net Constructed from Textual Records of a prototype system (as shown in Figure 1) will showcase how the knowledge net construction workflow could be realised using textual records in real world databases