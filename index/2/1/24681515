We consider the problem of policy learning in aMarkov Decision Process (MDP) where only a restricted, limited subset of the full policy space can be used. A MDP consists of a state space S, a set of actions A, a transition probability function t(s, a, s′) and a reward function R : S → R. Also there is the discount factor γ. The problem is to find a policy, a mapping from states to actions π : S → A, which gives the highest discounted return IE ∑∞ i=1 γ R(s) (where s represents the state encountered at time step i) for every possible start state. However, we are not interested in any possible policy, only in a restricted, limited subsetΠ of the full policy space. The assumption will be made that there exists a policy π which is best for every state s ∈ S, compared to the other policies in Π. It is not required that the true optimal policy for the MDP belongs to Π. In some settings we can also consider stochastic policies, which map states to a probability distribution over the action set. This greatly increases the size of the policy search space.