In recent years, advances in machine learning and related fields have led to significant advances in a range of user-interface technologies, including audio processing, speech recognition, and natural language processing. These advances in turn have enabled speech-based digital assistants and speech-to-speech translation systems to become practical to deploy on a large scale. In essence, machines are becoming capable of hearing what we are saying. But will they understand what we want them to do when we talk to them? What are the prospects for getting useful work done in essence, by synthesizing programs -- through the act of having a conversation with a computer? In this lecture, I will speculate on the central role that programming-language design and program synthesis may have in this possible -- and I will argue, likely -- future of computing, one in which every user writes programs, every day, by conversing with a computing system.