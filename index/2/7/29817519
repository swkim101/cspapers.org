Dataflow languages and processors are currently being extensively studied because they provide for the specification and realization of processes exhibiting a high degree of parallel and/or asynchronous activity [12, 8]. Several researchers have developed simulators for specific candidate dataflow architectures in which there are essentially an infinite number of resources available to the nost machine [9, 1]. This is done to study the degree of parallelism which is achievable with a given version of an algorithm. However, it is an equally important (and neglected) area to study the behavior of programs executing in candidate computer systems having a finite amount of resources. This paper presents results which have been obtained from such modeling. It is shown that in such a system certain “critical nodes” must be given priority of execution when competing with other nodes for the same resources in order to achieve the maximum system throughput. It is suggested that the abstract dataflow model be modified to accommodate such situations. Various design trade-offs associated with the implementation of the simulator are discussed along with a description of available features. A companion paper [6] describes the general dataflow simulation facility which provided the basis of this work.