Abuse on a large-scale website such as YouTube comes in various forms — scraping, email spam, hate speech, or “black-hat” search engine optimization to name a few — and must be fought accordingly. The detection of abusive behaviors uses supervised (machine learning) as well as unsupervised algorithms to mine billions of requests and predict the likelihood of abuse of each. We describe here an unsupervised system scoring incoming traffic for spam, more specifically bot activity, through some simple statistical anomaly detection rules we found to be quite effective. Each incoming record includes a number of characteristics or entities, such as user id or IP address, used to group the traffic into slices; for each entity type we have a number of metrics known to correlate with anomalous behavior in that large values, observed over large slices, reliably indicate anomalous behaviour. Each metric, for each slice, produces a score β ∈ [0, 1] which represents a conservative estimate (0 for most slices), based on that metric alone, of the proportion of undesirable (automated) traffic. We map metric values to scores using the key fact that each metric is a log-ratio; thus the most standard outlier detection method — flagging as suspicious any value x of metricX more than α standard deviations σ above the mean μX — naturally extends into a scoring method by yielding a goodness score γ = exp(−max(0, x− μX − ασ)), then a badness (or spam) score β = 1 − γ. The quantity σ = √ σ2 X + σ 2 x combines two very different sources of noise : the (weighted) standard deviation σX of the metric across entities, and the sampling error σx on its measurement x.