Research into mobile robot navigation has concentrated on the problem of finding optimal paths through known environments. For a mobile robot to operate in a partially known or a completely unknown environment the robot must be able to learn the cnvironmcnt. In this paper a mobile robot cnvironment learning algorithm is presented. The robot learns the environment by sensing, while it is executing paths produced by a path planner. A s obstacles are encountered en route to a goal, the model of the environment is updated and a new path to the goal is planned to avoid the obstacles. The paths generated by the algorithm will initially be locally optimal, but as the knowledge of the environment structure increases the optimality of the solution paths will improve. for the optimisation of distance to the goal combined will obstacle clearance information. The algorithm uses uses a quadtree representation to model environments. Initially the robot treats the unknown world as free space, until it learns information to the contrary. The path planner uses the distance transform methodology to generate all the solution paths from the goal back to the start location. This method not only yields a solution path to the goal, but it can also detect whether a goal is reachable. Once a path has been planned to the goal, the robot can execute it in a "safe" or "optimal" way. This paper will present the results of a computer simulation of the learning algorithm. Introduction The algorithm allows A great deal of the research concerning the problem of finding optimal paths for robot manipulators and autonomous mobile robots has concentrated on situations where the environment in which the robot operates in, is completely known and supplied -0 the path planner partially known or completely unknown environment, these path planning techniques are not directly applicable or extendable. Difficulties arise in deciding how to treat unexplored regions as obstacles, and only proceed into the unexplored regions if the goal lies there [ 41 [ 61 . 11 131 . For a robot navigating in a Research efforts into environment learning with mobile robots have followed a variety of approaches I41 111 . These can be broadly classified into two groupings; adaptive models and rigid models. Adaptive models use the nature and clutter of the environment to drive a representation of the environment. Typically adaptive modeis represent the environment as a network of free space regions [51 [ 7 ] 191 or as a graph of obstacle vertices [4]. environment learning methods require accurate sensor information. Rigid models impose a structure, such as a grid onto the environment without any regard to the nature and clutter of the environment [ 61 81 [ 111 [ 1 2 1 . Adaptive model environment learning methods offer elegant solutions, but in practise are difficult to implement on a real robot. The converse can be said of rigid model environment learning methods. Environment learning by a mobile robot csn be Adaptive model accomplished into one of two ways; either by operating in'mapping' or 'learning' modes. When a robot is operating in 'mapping' mode [ 81 [ 91, it traverses the entire environment in 2 systematic manner, scanning with onboard sensors and updating a map. The map i s then used for a11 subsequent path planning cxcrciscs. Difficultics n r i s c if thc cnvironment is updated and a new path to the goal is planned to avoid the obstacles 101. The paths generated by this system will initially be locally optimal, but as knowledge of environment increases the optimality of the paths generated, improves until eventually global optimality is attained. Environment learning methods of this type have difficulty in detecting that a goal is not reachable since they use heuristics when planning locally optimal paths. restrictions that the robot can recognise line of sight distances to obstacles and can detect obstacle. edges without imprecision, makes it difficult to implement this learning algorithm with current sensor technology. Since sensing line of sight to obstacles cannot be guaranteed an arrangement of obstacles in an environment can always be found where a heuristic path planner will fail [ 131. The The Jarvis e t a l . [ 121 approach to environment learning covers the enviroqment with a grid, and generates all the paths from the goal location back to the start locatio.. using distance transforms. To find the path of minimum length from the start to the goal, the grid neighbours of the start location are examined to find the grid with the lowest distance transform. This grid cell represents the start of a chain of cells of least distance to the goal. This method uses a rigid environment model and uses a 'learning' mode to map the environment. This method can also deduce that goals are not reachable. However, this method does have drawbacks, namely the inefficiencies of a grid representation, the costly computation of the distance transform and the ambiguity of optimal minimum length paths. A good compromise between adaptive free space models and rigid grid models are hierarchical planning methods. One such approach has been to use q-adtrees 131, this approach assumes that map of the environment is supplied to the path planner. Quadtrees have the advantage of having a rigid structure, but are adaptive to the clutter of the environment. This paper will present an environment learning algorithm using quadtrees to model the environment and a modification of the distance transform to generate paths of navigation for the robot. Most patn pianners approximate the robot to De a cylinder, then shrink the robot to point and expand all the objects in the environment by the robots radius. This strategy is useful in known environments, but in unexplored environments this represents unnecessary extra processin?, and important information is discarded. Namely the volumein which the robot is lOCatPd in and the volumes of the paths swepr; by the robot through the environment are definitely free space in a static environment and are probably free space in a dynamic environment.