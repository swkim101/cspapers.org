This paper describes the mathematical basis for a computer language which can be used for representing natural human reasoning with imprecise linguistic information. The approach to doing this employs a collection of abstraction mechanisms which are based on the concept of a linguistic variable first introduced by Zadeh [1975]. The present semantics differs from that of Zadeh, however, in that (i) it does not require the use of fuzzy sets for the interpretation of linguistic terms, and (ii) the meanings of logical inferences are given as algorithms which act directly on linguistic terms themselves, rather than on their underlying interpretations. Two distinct types of deduction algorithm are proposed. The overall objective is to devise a reasoning system having sufficient generality that it can conveniently employ these plus others in a unified frame.