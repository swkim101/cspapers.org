Parallel computing technology offers the opportunity for one basic computer architecture that achieves a wide performance range, at cost nearly linear in performance: A family of products, ranging from a workstation to a massively parallel processor with thousands of nodes, built from the same basic hardware components; the same languages, the same user interfaces used over all the range; the same application codes running efficiently on all these machines. ‘Ilk is the vision of scalable parallel computing. Is this dream, marketing hyperbole, or can it be reality? We examine, in our presentation, some of the underpinnings of this technology, We shall focus on parallel machines built of standard RISC microprocessor nodes, and consider the likely evolution of such systems in coming 3-5 years. The effective performance of a parallel machine is a function of many parameters: number of nodes, node compute rate, memory, internode communication bandwidth and latency, external 1/0 bandwidth and latency, and more. An intuitive requirement for scalable performance is that memory per node, internal and external bandwidth per node, and internal and external latency, stay nearly constant, as the number of nodes increase. Available technology can achieve these goals, over the range of interest. An examination of algorithmic performance show that these requirements for scalable performance have no better theoretical justification than the usual rules of thumb for uniprocessors (one byte of memory and one bit of 1/0 per flop/s). However, like these rules of thumb, they can provide a convention on what is a balanced parallel architecture. Such convention, even if arbitrary, simplifies the design of scalable software, since it leaves us with only two free performance parameters: number of nodes, and node performance. Internode communication latency is the performance parameter where current parallel architectures show the widest spread. Currently, shared memory machines achieve a much lower latency than distributed memory machines. An examination of the likely evolution of both architectures indicates that the gap will narrow, with increasingly similar communication mechanisms used on both. We shall examine the pros and cons of communication baaed on memory to cache transfers as compared to communication based on memory to memory tranafers. Parallel codes are written with various levels of explicit parallelism. At the most basic level, user explicitly control both data and control partition – this is the case for message passing. The compiler can still play an important role in optimizing communication by hiding la. tency. At the next level, languages are designed either to express control partition (control parallelism) or to express data partition (data parallelism). In the first case, the compiler derives an efficient data layout and communication pattern, from the partition of control; in the second case, it derives an efhcient assignment of computation to processors and an efficient communication pattern, from the partition of data. Both approaches give larger scope for compiler optimization. Finally there is the Holy Grail of implicit parallelism, all dealt by the compiler. Both distribution of computation and minimization of communication are, oftentimes, essential parts of parallel algorithm design. Therefore, I expect languages with explicit parallelism to be essential for the success of scalable parallel computing. Languages, like High Performance Fortran, that offer a choice of levels of control (explicit data and control parallelism, explicit data parallelism, “no explicit parallelism), may be the right approach: the user specify data or control partition only when the compiler fails to derive an efficient partition on its own. Such strategy can work only if the compiler ceases to be a black box, and provides to the user a clear model of the control and data partition derived from the source code,