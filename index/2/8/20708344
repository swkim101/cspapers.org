Affine policies (or control) are widely used as a solution approach in dynamic optimization where computing an optimal adjustable solution is usually intractable. While the worst case performance of affine policies can be significantly bad, the empirical performance is observed to be near-optimal for a large class of problem instances. For instance, in the two-stage dynamic robust optimization problem with linear covering constraints and uncertain right hand side, the worst-case approximation bound for affine policies is $O(\sqrt m)$ that is also tight (see Bertsimas and Goyal (2012)), whereas observed empirical performance is near-optimal. In this paper, we aim to address this stark-contrast between the worst-case and the empirical performance of affine policies. In particular, we show that with high probability affine policies give a good approximation for two-stage dynamic robust optimization problems on random instances generated from a large class of distributions; thereby, providing a theoretical justification of the observed empirical performance. The approximation bound depends on the distribution, but it is significantly better than the worst-case bound for a large class of distributions.