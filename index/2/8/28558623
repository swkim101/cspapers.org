In this work, we propose an inverted hidden Markov model (HMM) approach to automatic speech and handwriting recognition that naturally incorporates discriminative, artificial neural network based label distributions. Instead of aligning each input frame to a state label as in the standard HMM derivation, we propose to inversely align each element of an HMM state label sequence to a single input frame. This enables an integrated discriminative model that may be trained end-to-end from scratch or starting from an existing alignment path. The approach does not assume the usual decomposition into a separate (generative) acoustic model and a language model, and allows for a variety of model assumptions, incl. statistical variants of attention. Here, an initial proof-of-concept with experiments on the RIMES handwritten word recognition task is provided. For this initial experiment, model assumptions for the inverted HMM were chosen that are similar to those for a standard (hybrid) HMM and a similar LSTM-based network design was used. The experimental results show that this initial approach already performs similar to hybrid HMM, CTC, and a sequence-to-sequence approach using attention.