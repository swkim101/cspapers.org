Despite the multifaceted benefits of stochastic computing (SC) such as low cost, low power, and flexible precision, SC-based deep neural networks (DNNs) still suffer from the long-latency problem, especially for those with high precision requirements. While log quantization can be of help, it has its own accuracy-saturation problem due to uneven precision distribution. In this paper we propose successive log quantization (SLQ), which extends log quantization with significant improvements in precision and accuracy, and apply it to state-of-the-art SC-DNNs. SLQ reuses the existing datapath of log quantization, and thus retains its advantages such as simple multiplier hardware. Our experimental results demonstrate that our SLQ can significantly extend both the accuracy and efficiency of SCDNNs over the state-of-the-art solutions, including linear-quantized and log-quantized SC-DNNs, achieving less than $1\sim 1.5$% p accuracy drop for AlexNet, SqueezeNet, and VGG-S at mere 4$\sim$5-bit weight resolution.Acm Reference Format:Sugil Lee, Hyeonuk Sim, Jooyeon Choi, and Jongeun Lee. 2019. Successive log Quantization for Cost-Efficient Neural Networks Using Stochastic Computing. In The 56th Annual Design Automation Conference 2019 (DACâ€™19), June 2-6, 2019, Las Vegas, NV USA. ACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/3316781.3317916