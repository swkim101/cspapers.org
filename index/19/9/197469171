In the load balancing problem, there is a set of servers, and jobs arrive sequentially. Each job can be run on some subset of the servers, and must be assigned to one of them in an online fashion. Tradi- tionally, the assignment of jobs to servers is measured by the L, norm; in other words, an assignment of jobs to servers is quantified by the maximum load as- signed to any server. In this measure the performance of the greedy load balancing algorithm may be a loga- rithmic factor higher than the ofline optimal (3). In many applications, the L, norm is not a suitable way to measure how well the jobs are balanced. If each job sees a delay that is proportional to the number of jobs on its server, then the average delay among all jobs is proportional to the sum of the squares of the numbers of jobs assigned to the servers. Minimizing the average delay is equivalent to minimizing the Eu- clidean (or Lz) norm. For any fixed p, 1 < p < M, we show that the greedy algorithm performs within a con- stant factor of the ofline optimal with respect to the L, norm. The constant grows linearly with p, which is best possible, but does not depend on the number of servers and jobs.