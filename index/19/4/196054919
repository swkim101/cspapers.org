Learning to walk over a graph towards a target node for a given input query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem that has a known state transition model, but with sparse reward. To overcome the challenge, we develop a graph walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and a Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walking path) and map it into the Q-value, the policy and the state value. In order to effectively train the agent from sparse reward, we combine MCTS with the neural policy to generate trajectories with more positive rewards. From these trajectories, we improve the network in an off-policy manner using Q-learning, which indirectly improves the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy improvement step to learn the entire model. At testing stage, the MCTS is also applied with the neural policy to predict the target node. Experiment results on several graph-walking benchmarks show that we are able to learn better policies compared to other RL-based baseline methods, which are mainly based on policy gradient method. It also outperforms traditional KBC baselines.