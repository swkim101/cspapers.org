We present theoretical guarantees for an alternating minimization algorithm for the dictionary learning/sparse coding problem. The dictionary learning problem is to factorize samples $y$ into an appropriate basis (dictionary) $A^*$ times a sparse vector $x^*$. Our algorithm is a simple alternating minimization procedure switching between gradient descent and $\ell_1$ minimization at every step. Dictionary learning and specifically alternating minimization algorithms for dictionary learning are well studied both theoretically and empirically. However, in contrast to previous theoretical analysis for this problem we replace the condition on the operator norm of the true underlying dictionary ($A^*$) with a condition on the matrix infinity norm. This allows us to not only get convergence rates in terms of the error in estimated dictionary in the infinity norm but, also allows us to initialize randomly and converge to the globally optimum. Our guarantees under a reasonable generative model allow for dictionaries with growing operator norms, can handle an arbitrary level of overcompleteness while having sparsity which is information theoretically optimal for incoherent dictionaries. We also present statistical guarantees and present sample complexity guarantees for our algorithm.