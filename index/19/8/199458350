We present a feedback motion planning algorithm, Bounded-Error LQR-Trees, that leverages reinforcement learning theory to find a policy with a bounded amount of error. The algorithm composes locally valid linear-quadratic regulators (LQR) into a nonlinear controller, similar to how LQR-Trees constructs its policy, but minimizes the cost of the constructed policy by minimizing the Bellman Residual, which is estimated in the overlapping regions of LQR controllers. We prove a sample-based upper bound on the true Bellman Residual, and demonstrate a five-fold reduction in cost over previous methods on a simple underactuated nonlinear system.