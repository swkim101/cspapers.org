Visual analytics applications often rely on target tracking across a network of cameras for inference and prediction. A network of cameras generates immense amount of video data and processing it for tracking a target is highly computationally expensive. Related works typically use data association and visual re-identification techniques to match target templates across multiple cameras. In this thesis, I propose to formulate this scheduling problem as a Markov Decision Process (MDP) and present a reinforcement learning based solution to schedule cameras by selecting one where the target is most likely to appear next. The proposed approach can be learned directly from data and doesn't require any information of the camera network topology. NLPR MCT and DukeMTMC datasets are used to show that the proposed policy significantly reduces the number of frames to be processed for tracking and identifies the camera schedule with high accuracy as compared to the related approaches. Finally, I will be formulating an end-to-end pipeline for target tracking that will learn a policy to find the camera schedule and to track the target in the individual camera frames of the schedule.