Model interpretability is an increasingly important component of practical machine learning systems, with example-based, local, and global explanations representing some of the most common forms of explanations. We present a novel explanation system called SLIM that leverages favorable properties of all three of these explanation types. By combining local linear modeling techniques with dual interpretations of random forests (as a supervised neighborhood approach and as a feature selection method), we present a novel local explanation system with several favorable properties. First, SLIM sidesteps the typical accuracy-interpretability trade-off, as it is highly accurate while also providing both example-based and local explanations. Second, while SLIM does not provide global explanations, it can detect global patterns and thus diagnose limitations in its local explanations. 
Third, SLIM can select an appropriate explanation for a new test point when restricted to an existing set of exemplar explanations. Finally, in addition to providing faithful self-explanations, SLIM can be deployed as a black-box explanation system.