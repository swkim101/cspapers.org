Federated learning is an approach for privacy preserving machine learning. It is increasingly being used in a number of classification as well as ranking tasks. Protocols for federated learning involve model update at the edge devices and aggregation at the central servers over multiple rounds. In practice, most deep learning models deployed on the edge are already trained and in-use. Federated learning protocols lead to an oscillation in the performance of these local models over the epochs. The drop in accuracy is more prominent in the early phases. In this article, we study such effects for the popular FedAvg federated learning algorithm and suggest the modified HBIAS FedAvg algorithm. The algorithm proposes a heuristic based initialization adoption strategy for this purpose. We find that this protocol leads to smoother performance variation for experiments on benchmark datasets.