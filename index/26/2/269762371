One of the byproducts of message passing neural networks (MPNNs) is their potential bias towards weakly connected nodes, which can result in degraded performance. This paper confirms that as the number of layers increases, this bias becomes more closely associated with an imbalance in the distribution of eigenvector centrality, known as localization, which further amplifies the discrepancy in label influence on nodes, resulting in a performance gap. Therefore, we explore the effectiveness of non-backtracking centrality and PageRank centrality in mitigating this bias in MPNNs.