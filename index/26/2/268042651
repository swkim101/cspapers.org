Heterogeneous federated learning without assuming any structure is challenging due to the conflicts among non-identical data distributions of clients. In practice, clients often comprise near-homogeneous clusters so training a server-side model per cluster mitigates the conflicts. However, FL with client clustering often suffers from “clustering collapse”, i.e., one cluster’s model excels on increasing clients, and reduces to single-model FL. Moreover, cluster-wise models hinder knowledge sharing between clusters and each model depends on fewer clients. Furthermore, the static clustering assumption on data may not hold for dynamically changing models, which are sensitive to cluster imbalance/initialization or outliers. To address these challenges, we propose “ C lustered A dditive M odeling ( CAM )”, which applies a globally shared model Θ g on top of the cluster-wise models Θ 1: K , i.e., y = h ( x ; Θ g )+ f ( x ; Θ k ) for clients of cluster-k . The global model captures the features shared by all clusters so Θ 1: K are enforced to focus on the difference among clusters. To train CAM, we develop a novel Fed-CAM algorithm that alternates be-tween client clustering and training global/cluster models to predict the residual of each other. We can easily modify any existing clustered FL methods by CAM and significantly improve their performance without “clustering collapse” in different non-IID settings. We also provide a convergence analysis of Fed-CAM algorithm.