Our approach to automatically summarizing online mental health posts could help counselors by reducing their reading time, enabling quicker and more effective support for individuals seeking mental health assistance. Neural text summarization methods demonstrate promising performance owing to their strong pre-training procedure. Random token/span masking technique is often relied upon by existing pre-trained language models; an approach that overlooks the importance of content when learning word representations. In an attempt to rectify this, we propose using source and summary alignments as a saliency signal to enhance the pre-training strategy of language model for better representation learning of important content, paving the way for a positive impact on the model fine-tuning phase. Our experiments on a mental health-related dataset for user post summarization MentSum reveal improved performance, as evidenced by human evaluation metrics, surpassing the current state-of-the-art system.