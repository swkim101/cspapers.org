Visual Place Recognition (VPR) is essential for autonomous robots and unmanned vehicles, as an accurate identification of visited places can trigger a loop closure to optimize the built map. The most prevalent methods tackle VPR as a single-frame retrieval task, which uses a CNN-based encoder to describe and compare each individual frame. These methods, however, overlook the temporal information between frames. Other methods improve this by searching the database with consecutive frames, which can greatly reduce false positives. Nevertheless, current sequence-based methods typically assume the consecutive image frames to be captured at an approximately constant speed, which is not always the case in practice. Therefore, we propose an adaptive sequence search strategy (AdaptSeq), which can dynamically alter the step size of adjacent frames in the retrieved sequence trajectory. Furthermore, to address false positive retrieval of input frames, we propose a CNN-based discriminator named DDsNet. It can determine whether the top retrieved candidates are true positives based on the learned statistics rather than an artificial threshold. Overall, we construct a novel sequence-based VPR pipeline named AdaptSeqVPR. It utilizes a CNN-based encoder for frame descriptions, and encompasses AdaptSeq and DDsNet for sequence matching. The experimental results indicate that our AdaptSeqVPR exhibits superior performance compared to the baseline SeqSLAM and SeqVLAD. Notably, our method can robustly handle the sequence-based VPR for vehicles traveling at non-uniform speeds in changing environments.