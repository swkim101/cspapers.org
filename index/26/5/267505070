Graph Neural Networks have achieved notable success, yet explaining their rationales remains a challenging problem. Existing methods, including post-hoc and interpretable approaches, have numerous limitations. Post-hoc methods treat models as black boxes and can mislead users, while interpretable models often overlook user-centric explanations. Furthermore, most existing methods do not carefully consider the userâ€™s perception of explanations, potentially resulting in explanation-user mismatches. To address these problems, we propose a novel interpretable concept-matching model to enhance GNN interpretability and prediction accuracy. The proposed model extracts frequent concepts from input graphs using the graph information bottleneck theory and modified constraints. These concepts are managed in an in-memory concept corpus for efficient inference lookups and explanation generation. Various explanation construction features are implemented based on the concept corpus and the discovery module, aiming to fulfill diverse user preferences. Extensive experiments and a user study validate the performance of the proposed approach, showcasing its potential for improving model accuracy and interpretability.