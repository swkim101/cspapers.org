Sign Stochastic Gradient Descent ( SIGN SGD) is a communication-efficient stochastic algorithm that only uses the sign information of the stochastic gradient to update the modelâ€™s weights. However, the existing convergence theory of SIGN SGD either requires increasing batch sizes during training or assumes the gradient noise is symmetric and unimodal. Error feedback has been used to guarantee the convergence of SIGN SGD under weaker assumptions at the cost of communication overhead. This paper revisits the convergence of SIGN SGD and proves that momentum can remedy SIGN SGD under weaker assumptions than previous techniques; in particular, our convergence theory does not require the assumption of bounded stochastic gradient or increased batch size. Our results resonate with echoes of previous empirical results where, unlike SIGN SGD, SIGN SGD with momentum maintains good performance even with small batch sizes. Another new result is that SIGN SGD with momentum can achieve an improved convergence rate when the objective function is second-order smooth. We further extend our theory to SIGN SGD with major vote and federated learning.