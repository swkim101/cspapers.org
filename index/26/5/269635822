In this paper, we demonstrate the feasibility of alterfactual explanations for black box image classifiers.

Traditional explanation mechanisms from the field of Counterfactual Thinking are a widely-used paradigm for Explainable Artificial Intelligence (XAI), as they follow a natural way of reasoning that humans are familiar with. However, most common approaches from this field are based on communicating information about features or characteristics that are especially important for an AI's decision. 

However, to fully understand a decision, not only knowledge about relevant features is needed, but the awareness of irrelevant information also highly contributes to the creation of a user's mental model of an AI system. 

To this end, a novel approach for explaining AI systems called alterfactual explanations was recently proposed on a conceptual level. 

It is based on showing an alternative reality where irrelevant features of an AI's input are altered.

By doing so, the user directly sees which input data characteristics can change arbitrarily without influencing the AI's decision. 

In this paper, we show for the first time that it is possible to apply this idea to black box models based on neural networks.

To this end, we present a GAN-based approach to generate these alterfactual explanations for binary image classifiers.

Further, we present a user study that gives interesting insights on how alterfactual explanations can complement counterfactual explanations.