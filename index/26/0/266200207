We present One-Shot Affordance Learning (OSAL): a unified pipeline that learns manipulation for articulated objects by observing human demonstration only once. The key idea of our method is to embody affordance of articulated objects with an open-loop trajectory conditioned on a certain area of the object's surface. It serves as a simplified object-centric manipulation representation, which can be easily transferred into robot motion, while traditional methods fail to deal with the configuration difference between human hands and robot end effectors. Our system extracts the embodied affordance by focusing on hand action's effect on the object, and further grounds such affordance into object visual features through self-supervised learning for novel object configurations. We evaluated our method on a collection of real-life objects and furniture and demonstrated high success rates. With our system, humans only need to manipulate a novel object once with any gesture to transfer that manipulation skill to the robot, which we believe to be a highly efficient and user-friendly paradigm oriented for future real-life robots.