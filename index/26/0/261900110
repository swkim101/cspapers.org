Conventional AI accelerators have been bottle-necked by high volumes of data movement and accesses required between memory and compute units. A transformative approach that has emerged to address this in compute-in-memory (CIM) architectures, which perform computation in-place inside the volatile or non-volatile memory in an analog or digital manner, greatly reducing the data transfers and memory accesses. This paper presents recent advances and trends on CIM macros and CIM-based accelerator designs.