In many applications of machine learning, a large number of variables are consid-1 ered. Motivated by machine learning of interacting particle systems, we consider 2 the situation when the number of input variables goes to infinity. First, we continue 3 the recent investigation of the mean field limit of kernels and their reproducing 4 kernel Hilbert spaces, completing the existing theory. Next, we provide results 5 relevant for approximation with such kernels in the mean field limit, including 6 a representer theorem. Finally, we use these kernels in the context of statistical 7 learning in the mean field limit, focusing on Support Vector Machines. In particu-8 lar, we show mean field convergence of empirical and infinite-sample solutions as 9 well as the convergence of the corresponding risks. On the one hand, our results 10 establish rigorous mean field limits in the context of kernel methods, providing 11 new theoretical tools and insights for large-scale problems. On the other hand, our 12 setting corresponds to a new form of limit of learning problems, which seems to 13 have not been investigated yet in the statistical learning theory literature.