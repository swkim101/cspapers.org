Cross-lingual named entity recognition (CrossNER) faces challenges stemming from uneven performance due to the scarcity of multilingual corpora, especially for non-English
data. While prior efforts mainly focus on data-driven transfer methods, a significant aspect that has not been fully explored is aligning both semantic and token-level representations across diverse languages. In this paper, we propose Multi-view Contrastive Learning for Cross-lingual Named
Entity Recognition (MCL-NER). Specifically, we reframe the CrossNER task into a problem of recognizing relationships between pairs of tokens. This approach taps into the
inherent contextual nuances of token-to-token connections within entities, allowing us to align representations across
different languages. A multi-view contrastive learning framework is introduced to encompass semantic contrasts between
source, codeswitched, and target sentences, as well as contrasts among token-to-token relations. By enforcing agreement within both semantic and relational spaces, we minimize the gap between source sentences and their counterparts of both codeswitched and target sentences. This alignment
extends to the relationships between diverse tokens, enhancing the projection of entities across languages. We further
augment CrossNER by combining self-training with labeled source data and unlabeled target data. Our experiments on
the XTREME benchmark, spanning 40 languages, demonstrate the superiority of MCL-NER over prior data-driven
and model-based approaches. It achieves a substantial increase of nearly +2.0 F1 scores across a broad spectrum and
establishes itself as the new state-of-the-art performer.