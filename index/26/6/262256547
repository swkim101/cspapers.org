Motivation The scientific community is working towards the creation of fully autonomous mobile robots capable of interacting with the proverbial real world. Autonomous robots are predominantly dependent on sensory information but the ability to accurately sense the complex world is still missing. Visual input, in the form of color images from a camera, should be an excellent and rich source of such information, considering the significant amount of progress made in machine vision. But color, and images in general, have been used sparingly on mobile robots, where people have mostly focussed their attention on other sensors such as sonar and laser. There are three main reasons for this reliance on other relatively low-fidelity sensors. First, most sophisticated vision algorithms require substantial amount of computational and/or memory resources making them infeasible to use on mobile robotic systems that typically have constrained memory and computational resources but demand real-time processing. Second, most vision algorithms assume a stationary or slowly moving camera and hence cannot account for the rapid non-linear camera motion that is characteristic of mobile robot domains. Third, the variation of illumination over the operating environment causes a nonlinear shift in color distributions that is difficult to model; mobile robots, while moving around the world, often go into places with changing illumination. Even in the face of