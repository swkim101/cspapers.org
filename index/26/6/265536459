Securing the Internet of Things is critical for its successful deployment in various industries. While Machine Learning techniques have shown promise for intrusion detection in the Internet of Things, existing methods require large amounts of labeled training data; moreover, they encounter challenges with the presence of extreme class imbalance, i.e., some classes are underrepresented in the datasets used. Supervised methods rely on extensive labeled data, which can be costly and time-consuming to obtain. Class imbalance in datasets further exacerbates the challenge by skewing the modelâ€™s learning process toward the majority classes, leading to poor detection of attacks belonging to minority classes. This issue is particularly pronounced in the Internet of Things environments due to diverse devices and the varying frequency of intrusions targeting them. To overcome these challenges, we present a Few-Shot and Self-Supervised framework, called , for detecting intrusions in IoT networks. works in three phases. The first phase employs self-supervised learning to learn latent patterns and robust representations from unlabeled data. The second phase introduces Few-shot learning with contrastive training. Few-shot learning enables the model to learn from a few labeled examples, thereby eliminating the dependency on a large amount of labeled data. Contrastive Training addresses the class imbalance issue by improving the discriminative power of the model. The third phase introduces a novel K-Nearest neighbor algorithm that sub-samples the majority class instances to further reduce imbalance and improve overall performance. Experimental results based on three publicly available benchmark datasets demonstrate the efficacy of in addressing the challenges posed by the limited availability of labeled data as well as class imbalance in datasets. Our proposed framework , utilizing only of labeled data, outperforms fully supervised state-of-the-art models by up to and with respect to the metrics precision and F1 score, respectively.