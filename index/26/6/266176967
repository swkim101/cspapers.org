Most of the recent work in leveraging Large 001 Language Models (LLMs) such as GPT-3 for 002 Machine Translation (MT) has focused on se-003 lecting the few-shot samples for prompting. In 004 this work, we try to better understand the role 005 of demonstration attributes for the in-context 006 learning of translations through perturbations 007 of high-quality, in-domain demonstrations. We 008 find that asymmetric perturbation of the source-009 target mappings yield vastly different results. 010 We show that the perturbation of the source 011 side has surprisingly little impact, while tar-012 get perturbation can drastically reduce transla-013 tion quality, suggesting that it is the output text 014 distribution that provides the most important 015 learning signal during in-context learning of 016 translations. We propose a method named Zero-017 Shot-Context to add this signal automatically 018 in Zero-Shot prompting. Our proposed method 019 greatly improves upon the zero-shot translation 020 performance of GPT-3, thereby making it com-021 petitive with few-shot prompted translations. 022