Humans effortlessly grasp the connection be-tween sketches and real-world objects, even when these sketches are far from realistic. Moreover, human sketch understanding goes beyond categorization â€” critically, it also entails understanding how individual elements within a sketch correspond to parts of the physical world it represents. What are the computational ingredients needed to support this ability? Towards answering this question, we make two contributions: first, we introduce a new sketch-photo correspondence benchmark, PSC6k , containing 150K annotations of 6250 sketch-photo pairs across 125 object categories, augmenting the existing Sketchy dataset (Sangkloy et al., 2016) with fine-grained