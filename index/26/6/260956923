Deep reinforcement learning (RL) is a powerful approach for solving optimal control problems. However, RL-trained policies often suffer from the action ﬂuctuation problem, where the consecutive actions signiﬁcantly differ despite only slight state variations. This problem results in mechanical components’ wear and tear and poses safety hazards. The action ﬂuctuation is caused by the high Lipschitz constant of actor networks. To address this problem, we pro-pose a neural network named LipsNet. We pro-pose the Multi-dimensional Gradient Normalization (MGN) method, to constrain the Lipschitz constant of networks with multi-dimensional input and output. Beneﬁting from MGN, LipsNet achieves Lipschitz continuity, allowing smooth actions while preserving control performance by adjusting Lipschitz constant. LipsNet addresses the action ﬂuctuation problem at network level rather than algorithm level, which can serve as actor networks in most RL algorithms, making it more ﬂexible and user-friendly than previous works. Experiments demonstrate that LipsNet has good landscape smoothness and noise robustness, resulting in signiﬁcantly smoother action compared to the Multilayer Perceptron.