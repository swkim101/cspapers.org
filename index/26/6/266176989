Neural code generation models are nowadays widely adopted to generate code from natural language descriptions automatically. Recently, pre-trained neural models equipped with token-level retrieval capabilities have exhibited great potentials in neural machine translation. However, applying them directly to code generation experience challenges: the use of the retrieval-based mechanism inevitably introduces extra-neous noise to the generation process, resulting in even syntactically incorrect code. Com-putationally, such models necessitate frequent searches of the cached datastore, which turns out to be time-consuming. To address these issues, we propose k NN-TRANX, a token-level retrieval augmented code generation method. k NN-TRANX allows for searches in smaller datastores tailored for the code generation task. It leverages syntax constraints for the retrieval of datastores, which reduces the impact of retrieve noise. We evaluate k NN-TRANX on two public datasets and the experimental re-sults confirm the effectiveness of our approach.