Asking for help (help-seeking) is a recognized and effective problem-solving strategy. This study investigates students' interaction with on-demand hints (automated hints requested by students) and assesses their impact on learning progress. We conducted an A/B experiment in a third-year computer science database course, offering hints for selected SQL problems with different hint designs. We collected data on students' code submissions, grades, and hint requests, and we administered a survey to gather feedback and gauge student perception of the hints. Many students accessed hints immediately without attempting the problem first, often requesting multiple hints in quick succession. While students perceived the hints to be valuable, we did not detect an impact on student problem-solving. These insights could inform future studies on the possible impact of students' attitudes toward hints, and how different types of hints might impact uptake and perception of hints.