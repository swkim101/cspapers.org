Modern data center applications require servers to respond to requests from thousands of clients while maintaining micro-second-scale SLOs. Efficient operation of the data center infrastructure requires assigning the exact amount of resources needed by the application, no more and no less. A burst of incoming traffic that exceeds allocated capacity can cause long queues, requiring efficient and responsive overload control schemes. A sudden drop in demand requires re-allocation of resources to other applications. Within a single host, several mechanisms have been proposed for overload control [1, 3, 8, 9] and dynamic core allocation [2, 4, 6, 7]. The state of the art in both control loops is designed to react to microsecond-level changes in load. We present a simulation-based study of the interaction between Overload Controllers and Core Allocators at microsecond timescales, examining their macroscopic implications at larger timescales. Core Allocators and Overload Controllers strive for the same delicate balance between high utilization and low latency, employing complementary approaches. In particular, Core Allocators adjust the provisioned capacity to an application, assuming a fixed load. Overload Controllers adjust the admitted load, assuming fixed capacity. Moreover, both control loops typically make their decision based on the amount of queueing in the system, adding more resources or admitting less load as queueing delay increases, targeting a specific level of queueing delay. The potential for interference between the two control loops arises because the quantity that one controller assumes to be fixed is changed by the other controller (i.e., load and capacity). Further, both controllers use the same signal to make their decision.