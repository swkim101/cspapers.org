Vision Transformers (ViTs) have continuously achieved new milestones in object detection. However, the considerable computation and memory burden compromise their efficiency and generalization of deployment on resource-constraint devices. Besides, efficient transformer-based detectors designed by existing works can hardly achieve a realistic speedup, especially on multi-core processors (e.g., GPUs). The main issue is that the current literature solely concentrates on building algorithms with minimal computation, oblivious that the practical latency can also be affected by the memory access cost and the degree of parallelism . Therefore, we propose SpeedDETR, a novel speed-aware transformer for end-to-end object detectors, achieving high-speed inference on multiple devices. Specifically, we de-sign a latency prediction model which can directly and accurately estimate the network latency by analyzing network properties , hardware memory access pattern , and degree of parallelism . Following the effective local-to-global visual modeling process and the guidance of the latency prediction model, we build our hardware-oriented architecture design and develop a new family of SpeedDETR. Experiments on the MS COCO dataset show SpeedDETR outperforms current DETR-based methods by 1.5% ∼ 9.2% AP with 1.09 ×∼ 3.6 × speedup on Tesla V100. Even acceptable speed inference can be achieved on edge GPUs, i.e., 4 FPS for NVIDIA JETSON TX2 (1.4 ×∼ 4 × faster than other counterparts), 1 FPS for NVIDIA NANO (1.5 ×∼ 6.7 × faster). Codes release SpeedDETR.