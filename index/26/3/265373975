
 In the past decades, neural networks have rapidly grown in popularity as a way to model complex non-linear relationships. The computational efficiently and flexibility of neural networks has made them popular for machine learning-based optimization methods. As such the derivative of a neural network’s output is required for gradient-based optimization algorithms. Recently, there have been several works towards improving derivatives of neural network targets, however there is yet to be done a comparative study on the different derivation methods for the derivative of a neural network’s targets with respect to its input features. Consequently, this paper’s objective is to implement and compare common methods for obtaining or approximating the derivative of neural network targets with respect to their inputs. The methods studied include analytical derivatives, finite differences, complex step approximation, and automatic differentiation. The methods are tested by training deep multilayer perceptrons for regression with several analytical functions. The derivatives of the neural network-derived methods are evaluated against the exact derivative of the test functions. Results show that all of the derivation methods provide the same derivative approximation to near working precision of the computer. Implementation of the study is done using the TensorFlow library in a provided Python code.