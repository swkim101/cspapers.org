Multi-domain generalization $(mDG)$ is universally aimed to minimize the discrepancy between training and testing distributions to enhance marginal-to-label distribution mapping. However, existing $mDG$ literature lacks a general learning objective paradigm and often imposes constraints on static target marginal distributions. In this paper, we propose to leverage a Y-mapping to relax the constraint. We rethink the learning objective for $mDG$ and design a new general learning objective to interpret and analyze most existing $mDG$ wisdom. This general objective is bifurcated into two synergistic amis: learning domain-independent conditional features and maximizing a posterior. Explorations also extend to two effective regularization terms that incorporate prior information and suppress invalid causality, alleviating the issues that come with relaxed constraints. We theoretically contribute an upper bound for the domain alignment of domain-independent conditional features, disclosing that many previous $mDG$ endeavors actually optimize partially the objective and thus lead to limited performance. As such, our study distills a general learning objective into four practical components, providing a general, robust, and flexible mechanism to handle complex domain shifts. Extensive empirical results indicate that the proposed objective with Y -mapping leads to substantially better $mDG$ performance in various downstream tasks, including regression, segmentation, and classification. Code is available at htttps://github.com/zhaorui-t.an/GMDG/tree/main.