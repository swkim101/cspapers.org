Since multiple flows (users) compete for a single, shared solid state drives (SSDs) concurrently in cloud environments, fairness has drawn great interest in recent years. Although many fair schemes are proposed on different modules of SSDs, such as resource allocation for the cache and chip queue reordering for the transaction scheduling unit (TSU), they fail to continue to provide fairness for the worse locality of I/O requests due to a deeper cloud storage stack that makes cache performance poor. Moreover, existing methods only treat back-end flash as a black box and do not exploit the differences in flash features across requests. This paper proposes CoFS, a novel coordinated SSD cache and TSU light-weight learning-based fair scheme to achieve complete fairness considering cache and bandwidth resources which are significant for fairness control. The key idea is to make the front-end SSD cache achieves fairness at the workload-level by recognizing dynamic I/O changes, while the back-end TSU achieves fairness at the flash-level by awareness of flash page-types, and finally CoFS collaborates with them to achieve fairness at the SSD device-level. Experimental results show that CoFS achieves significant fairness and performance gain of a wide range of the latest cloud block storage workloads, with an average improvement of 46.3% and 94.5% compared to state-of-the-art fairness policies, respectively.