While traffic signals allow competing flows of traffic to safely cross busy intersections, they inevitably enforce a stop-and-go movement pattern. This stop-and-go movement pattern increases fuel consumption by 17%, CO2 emissions by 15%, and reduces vehicle flow aggravating congestion and driver frustration.
 The stop-and-go movement pattern can be alleviated by utilizing information about the future schedule of the traffic signals ahead. Based on when the signal ahead will turn green, onboard computational devices (e.g., smartphones) can advice the drivers on the optimal speed they should maintain so that they can cruise through an intersection and avoid coming to complete halt. Alternatively, efficient detours may be suggested to the drivers to avoid waiting for a long time at a red light.
 Our MobiSys'11 paper proposes SignalGuru, a novel software service that relies solely on collaborating windshield-mounted mobile phones to provide information about the schedule of traffic signals and enable a set of novel driver-assistance applications.
 The SignalGuru service consists of four main modules: First, video frames are captured with the mobile phone cameras and processed to detect the color (status) transitions (e.g., RED to GREEN) of the traffic signal ahead (detection module). Then, information across multiple consecutive frames is used to filter away erroneous traffic signal transition detections (transition filtering module). Third, SignalGuru-enabled phones collaborate by sharing their databases of detected traffic signal transitions with other phones within communication range (collaboration module). Finally, the merged data- base of traffic signal transitions is fed into a customized machine learning-based model to predict the future schedule of the traffic signal ahead (prediction module).
 Our demo will present a SignalGuru system, in which two SignalGuru-enabled iPhone 4 devices (iPhones A and B) will be collaborating to predict the schedule of the two pairs of mock traffic signals of the two intersecting roads of an intersection. The pictures of the intersection (taken with actual windshield-mounted iPhone devices) will be printed on posterboards A and B. The camera of SignalGuru-enabled iPhone A will be looking at posterboard A and the camera of SignalGuru-enabled iPhone B at posterboard B. While the iPhone B device will be fixed, the iPhone A device will be mounted on a lego vehicle so that it can move closer to or farther away from posterboard A. The mock traffic signals will be implemented with electronic displays (iPhone/iPad screen) and will be switching to red/green/ yellow based on a defined schedule.
 For the demo, we will be be bringing all the required electronic equipment and the two posterboards. However, we will also need two holders for the posterboards and two small tables for Signal-Guru-enabled devices A and B to stand.
 Demo attendants will be able to test the real time operation of SignalGuru and interact with it. More specifically, demo attendants will be able to see how the detection window gets dynamically adapted as they change the device's orientation and/or its distance from the traffic signal, and how SignalGuru's detection and filtering modules react to real-world events like fully or partially occluded traffic signals and false positive/negative signal detections.
 We believe that our SignalGuru demo will complement nicely our paper presentation as it will offer a good opportunity to discuss with conference participants about our system, demonstrate to them the challenges that our system poses as well as how it tackles them.