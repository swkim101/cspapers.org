Robotic grasping is a fundamental skill for robots, but it is quite challenging in cluttered scenes. In cluttered scenes, the precise prediction of high-quality grasp configurations such as rotation and grasping width while avoiding collisions is essential. To accomplish this, the grasp detection models require the capabilities of stronger fine-grained information extracted around the grasp points. However, due to the computational resource restriction, point clouds are usually downsampled in existing networks, which inevitably make some potentially important points discarded. To overcome this problem, we propose a Grasp Region Exploration module to explore the area covered by high-quality grasps. Based on the grasp region, we enhance the point density around the grasp points to mitigate the loss of information caused by downsampling. Furthermore, we devise the Grasp Region Attention module to dynamically aggregate features of various points within the grasp region, such as the grasp point and contact points. The proposed method achieves state-of-the-art performance on the large-scale GraspNet-1Billion dataset. We also conduct real-world experiments on a Franka Emika Panda robot and show that the robot can grasp objects in cluttered scenes with a high success rate.