With modeling on triangle similarity, margin-based triplet (MT) loss functions have been the paradigm of choice for robust deep metric learning (DML). However, they require carefully tuning a violation margin being the task-specific decision boundary. A globally invariant violation margin is irrational when performing online triplet mining on each mini-batch. To address this issue, we propose a novel yet efficient concordance-induced triplet (CIT) loss function for DML training to specify an individualized violation margin for each triplet. Concordance expects the predicted ordering of intra-class and inter-class similarities to be correct. Building on the concordance of triangle similarity, our CIT loss maximizes an intra-class similarity relative to two inter-class similarities. In addition, due to the high training complexity on triplets, we propose a regularizer for our CIT loss, called spectral decay (SD), to enhance data-driven DML model training in a parameter-driven manner. SD exploits the rank-one approximation of the spectral norm direction in the weight matrix to impose varying weight decay, thus collaborating with CIT loss to affect weight variance shifts to achieve fast convergence. Extensive experiments on various DML tasks, including face recognition, person re-identification, and image retrieval, demonstrate the elegance and superiority of our CIT against its counterparts.