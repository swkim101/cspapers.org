This research explores a novel approach to enhance the accuracy of static American Sign Language (ASL) sign classification by employing hand landmark extraction through computer vision. Traditional Convolutional Neural Networks (CNN) often operate on raw pixel data, leading to high-dimensional feature spaces and extensive training times. In contrast, our method extracts key features as data points, significantly reducing the dimensionality of the data. We demonstrate that this hand landmark detection-based approach yields promising accuracy with shallower models such as k-Nearest Neighbors (kNN) and Logistic Regression, due to the inherently lower dimensionality of the data and the elimination of background clutter. Importantly, our approach excels particularly when dealing with relatively small datasets, a critical advantage considering the limited availability of public data in this field. Our research highlights the efficiency and effectiveness of hand landmark extraction as a preprocessing technique for ASL sign classification using the Google MediaPipe hand landmark model.