In this paper, we look at cross-domain few-shot clas-sification which presents the challenging task of learning new classes in previously unseen domains with few la-belled examples. Existing methods, though somewhat ef-fective, encounter several limitations, which we alleviate through two significant improvements. First, we introduce a lightweight parameter-efficient adaptation strategy to ad-dress overfitting associated with fine-tuning a large number of parameters on small datasets. This strategy em-ploys a linear transformation of pre-trained features, sig-nificantly reducing the trainable parameter count. Second, we replace the traditional nearest centroid classifier with a discriminative sample-aware loss function, enhancing the model's sensitivity to the inter- and intra-class variances within the training set for improved clustering in feature space. Empirical evaluations on the Meta-Dataset bench-mark showcase that our approach not only improves accu-racy up to 7.7% and 5.3% on previously seen and unseen datasets, respectively, but also achieves the above performance while being at least IV 3 x more parameter-efficient than existing methods, establishing a new state-of-the-art in cross-domain few-shot learning. Our code is available at https://github.com/rashindrie/DIPA.