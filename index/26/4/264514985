Given the challenges of evaluating the effect of task allocation mechanisms and their associated incentives on the behavior of participants in spatial crowdsourcing, there is a need for an evaluation framework that simplifies the process of benchmarking such mechanisms with real crowds. In this demo, we present Snap’N’Go, a spatial crowdsourcing application that is modeled as a scavenger-hunt game, which is integrated within a larger modular framework that can be easily extended to evaluate various task allocation mechanisms and incentive models, without affecting the user-experience. Moreover, we present the details of the initial launch of the framework, in which we benchmark multiple mechanisms, and discuss how it can be easily extended for various experimental settings.