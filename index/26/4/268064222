In this work, we revisit the generalization error of stochastic mirror descent for 1 quadratically bounded losses studied in Telgarsky (2022). Quadratically bounded 2 losses is a broad class of loss functions, capturing both Lipschitz and smooth 3 functions, for both regression and classification problems. We study the high 4 probability generalization for this class of losses on linear predictors in both 5 realizable and non-realizable cases when the data are sampled IID or from a 6 Markov chain. The prior work relies on an intricate coupling argument between 7 the iterates of the original problem and those projected onto a bounded domain. 8 This approach enables blackbox application of concentration inequalities, but 9 also leads to suboptimal guarantees due in part to the use of a union bound 10 across all iterations. In this work, we depart significantly from the prior work of 11 Telgarsky (2022), and introduce a novel approach for establishing high probability 12 generalization guarantees. In contrast to the prior work, our work directly analyzes 13 the moment generating function of a novel supermartingale sequence and leverages 14 the structure of stochastic mirror descent. As a result, we obtain improved bounds 15 in all aforementioned settings. Specifically, in the realizable case and non-realizable 16 case with light-tailed sub-Gaussian data, we improve the bounds by a log T factor, 17 matching the correct rates of 1 /T and 1 / âˆš T , respectively. In the more challenging 18 case of heavy-tailed polynomial data, we improve the existing bound by a poly T 19 factor. 20