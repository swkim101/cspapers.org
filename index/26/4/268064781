Deep learning models are emerging to summarize source code to comment for code documentation and program comprehension. We can achieve good performance by training the model on large training corpus. However, in practice, the code samples from different projects can have contradictory training signal for learning a deep comment generator, making the model struggled to fit all the training samples. In this work, we introduce a novel approach, AdaCom, to improve the performance of comment generators by on-the-fly model adaptation. This research is motivated by the observation that deep comment generators often need to strike a balance as they need to fit all the training samples. Specifically, for one certain target code c , some training samples S p could have made more contributions while other samples S o could have counter effects. However, the traditional fine-tuned models need to fit both S p and S o from a global perspective, leading to compromised performance for one certain target code c . In this context, we design AdaCom to (1) detect whether the model might have a compromised performance on a target code c and (2) retrieve a few helpful training samples S p that have contradictory samples in the training dataset and, (3) adapt the model on the fly by re-training the S p to strengthen the helpful samples and unlearn the harmful samples. Our extensive experiments on 7 comment generators and 4 public datasets show that (1) AdaCom can significantly boost the performance of comment generation (BLEU4