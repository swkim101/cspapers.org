Deductive reasoning is a crucial cognitive ability of humanity, allowing us to derive valid conclusions from premises and observations. However, existing works mainly focus on language-based premises and generally neglect deductive reasoning from visual observations. In this work, we introduce rule b A sed futu R e-inference deduc T ion ( ART ), which aims at de-ducing the correct future event based on the visual phenomenon (a video) and the rule-based premises, along with an explanation of the reasoning process. To advance this field, we construct a large-scale densely annotated dataset ( Video-ART ), where the premises, future event candidates, the reasoning process explanation, and auxiliary commonsense knowledge ( e.g. , actions and appearance) are annotated by anno-tators. Upon Video-ART, we develop a strong baseline named ARTNet . In essence, guided by commonsense knowledge, ARTNet learns to identify the target video character and per-ceives its visual clues related to the future event. Then, ARTNet rigorously applies the given premises to conduct reasoning from the identified information to future events, through a non-parametric rule reasoning network and a reasoning-path review module. Empirical studies validate the rationality of ARTNet in deductive reasoning upon visual observations and the effectiveness over existing works.