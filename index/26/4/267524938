The latency of fetching data from memory is tens of that from last level cache. So cache hit rate is an important factor to the performance of services, especially for packets forwarding service running in network equipment, which need to process hundreds of giga-packets per second. Different user's connection to the network represent a flow in the network equipment, forwarding service need to match the flow's packet header information to rule tables, then decide the next hop or edit the packet header. The random incoming of different flows may lead to random memory access by forwarding services in the network equipment, inappropriate instructions layout on memory will result in high cache miss rate. In order to search for the optimal instruction blocks orchestration on memory, we propose a design-time system optimization solution, called DSO. Our main contributions are as follows: (i) We first formulate the forwarding service instruction blocks orchestration optimization problem; (ii) DSO implements four static memory allocation policies in the platform to find the important factors that has positive influence on cache performance; (iii) Based on the chosen feasible static policy, DSO further proposes a self-adaptive policy to adjust memory allocation of conflict instruction blocks during run time. We apply DSO on the real-world network services. Our experiments show that flow block locality is the more important factor than block visit frequencies, which has 7% difference on cache miss rate. In comparison to the baseline policy in our real system, experiments show that DSO offers 2% improvement on cache miss rate and more stable performance when network traffic grows.