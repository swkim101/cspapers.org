
 In the past year there have been paradigm shifting developments in the feasibility and availability of machine learning tools for the creation of visual and textual works. Two of the most prominent examples of this has been Large-Language models like chatGPT and methods like stable diffusion for generating art from text prompts. Both visual and language arts are often thought of as human activities, so exploring the possibilities and limitations of these tools is important for both understanding automation and improving our understanding of human cognition. In this paper I use a Large-Language Model and stable diffusion in tandem to develop an understanding of what new possibilities exist in computational cognition and design automation through their application. While no single model can recreate the complexities of a biological brain at this time, they can be thought of as analogous to individual neurological structures. For example, a Large-Language Model that is able to reason out and communicate the solutions to simple logic puzzles could recreate some of the functionality of the frontal lobe of the cerebrum. Additionally approaches like stable diffusion can recreate some of the functions of the occipital and parietal lobes. By combining them more complex behaviors and capabilities can be achieved than are possible from the individual parts. This work is in its early stages but is foundational for later developments in design automation, robotics, and computational cognition.