Many existing learning methods use incre-mental algorithms that construct a generalization in one pass through a set of training data and modify it in subsequent passes (e.g., perceptrons, neural nets, and decision trees). Most of these methods do not store the entire training set, in essence employing a limited storage requirement that abstracts the notion of a compressed representation. The question we address is, how much additional processing time is required for methods with limited stor-age? Processing time for learning algorithms is equated in this paper with the number of passes necessary through a data set to obtain a correct generalization. For instance, neural nets require many passes through a data set before converging. Decision trees require fewer passes, but precise bounds are unknown. We consider limited storage algorithms for a particular concept class, nested hyperrect-angles. We prove bounds that illustrate the fundamental trade-off between storage requirements and processing time required to learn an optimal structure. It turns out that our lower bounds apply to other algorithms and concept classes (e.g., decision trees) as well. Notably, imposing storage limitations on the learning task forces one to devise a completely different algorithm to reduce the number of passes. We also briefly discuss parallel learning algorithms. 1 Introduction Many existing learning methods attempt to create concise generalizations from a set of examples. Besides saving storage, small generalizations are easier to summarize and communicate to others. A common learning method will construct a generalization after one pass through a set of training data, and modify it in subsequent passes to make it smaller or more accurate. Per-ceptron methods, neural nets, and decision tree techniques all fit this paradigm. Most of these methods do not store the entire set of training data. When processing is completed, the only thing they store is a generalized data structure such as a tree, a matrix of weights, or a set of geometric clusters. The issue of limiting the storage of a learning algorithm is one abstraction of the notion of a compressed representation. The question we address is how many passes through a data set arc required to obtain a "correct" (i.e., accurate but minimum in size) generalization if we are only allowed to store the generalization. This issue is equivalent to analyzing an algorithm that has a limited storage requirement. Fixed storage is an important consideration for several reasons. First of all, some learning models always â€¦