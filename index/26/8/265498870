We propose SceneTex, a novel method for effectively gen-erating high-quality and style-consistent textures for indoor scenes using depth-to-image diffusion priors. Unlike pre-vious methods that either iteratively warp 2D views onto a mesh surface or distillate diffusion latent features with-out accurate geometric and style cues, SceneTexformulates the texture synthesis task as an optimization problem in the RGB space where style and geometry consistency are prop-erly reflected. At its core, SceneTex proposes a multires-olution texture field to implicitly encode the mesh appear-ance. We optimize the target texture via a score-distillation-based objective function in respective RGB renderings. To further secure the style consistency across views, we introduce a cross-attention decoder to predict the RGB values by cross-attending to the pre-sampled reference locations in each instance. SceneTex enables various and accurate texture synthesis for 3D-FRONT scenes, demonstrating sig-nificant improvements in visual quality and prompt fidelity over the prior texture generation methods.