As a paradigm that preserves privacy, Federated Learning (FL) enables distributed clients to cooperatively train global models using local datasets. However, this approach also provides opportunities for adversaries to compromise system stability by contaminating local data, such as through Label-Flipping Attacks (LFAs). In addressing these security challenges, most existing defense strategies presume the presence of an independent and identically distributed (IID) environment, resulting in suboptimal performance under Non-IID conditions. This paper introduces RSim-FL, a novel and pragmatic defense mechanism that incorporates Representational Similarity Analysis (RSA) into the detection of malevolent updates. This is achieved by calculating the similarity between uploaded local models and the global model. The evaluation, conducted against five state-of-the-art baselines, demonstrates that RSim-FL can accurately identify malicious local models and effectively mitigate divergent Label-Flipping Attacks (LFAs) in a Non-IID setting.