Large-scale Text-to-Image (T2I) models have rapidly gained prominence across creative fields, generating visu- ally compelling outputs from textual prompts. However, controlling these models to ensure consistent style remains challenging, with existing methods necessitating fine-tuning and manual intervention to disentangle content and style. In this paper, we introduce StyleAligned, a novel technique de- signed to establish style alignment among a series of gener- ated images. By employing minimal ‘attention sharing’ during the diffusion process, our method maintains style con- sistency across images within T2I models. This approach allows for the creation of style-consistent images using a reference style through a straightforward inversion operation. Our method's evaluation across diverse styles and text prompts demonstrates high-quality synthesis and fidelity, underscoring its efficacy in achieving consistent style across various inputs.