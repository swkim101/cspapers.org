This paper aims for a potential architectural breakthrough for multilingual learning and asks: could different tasks from different languages be modeled in a monolithic framework (without any task/language-speciﬁc module) ? The beneﬁt of achieving this is not only that systems trained on low resources scenario can be assisted by more other languages and tasks, but opening new doors for future multilingual research. We approach this goal by developing a learning framework Polyglot Prompt , where prompting methods are introduced to learn a uniﬁed semantic space for different languages and tasks after proper multilingual prompt engineering . Experimentally, we perform a comprehensive evaluation on 6 tasks (topic classi-ﬁcation, sentiment classiﬁcation, named entity recognition, question answering, natural language inference, summarization), 24 datasets, and 49 languages, which shows the efﬁcacy of multilingual multitask prompting training and suggests several interesting observations. e.g., English prompts are polyglots since directly applying them to task samples in other languages could result in a better improvement. We also present an interpretable multi-lingual evaluation methodology and show how the proposed framework, multilingual multi-task prompt training, works. We release all datasets prompted in the best setting 1 and will release our code soon. 2