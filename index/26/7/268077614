In order to combine the benefits of dataflow and control-flow computation while avoiding the pitfalls of both, the authors propose a two-level model of large-grain dataflow computation, called LGDG computation. A formalism has been provided in a previous paper to prove the determinism of parallel program execution under this model. The current paper presents the basic LGDG computer architecture which is organized at two levels, called the graph level and the node level. The kernel of the node-level architecture is characterized by its non-branch RISC (N-RISC) feature, which ensures an optimal utilization of pipeline processing. This kernel is supported by various co-processors based on the principle of function migration. We will show in this paper how the graph-level architecture can be reduced to a matching unit, extended by a writable control store, through node migration and node aggregation. As a result, the matching overhead is drastically reduced, thus eliminating the most severe bottleneck of existing 'fine-grain' dataflow computers.