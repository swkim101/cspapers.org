Nystr¨om low-rank approximation has shown great potential in processing large-scale kernel matrix and neural networks. However, there lacks a uniﬁed analysis for Nystr¨om approximation, and the asymptotical minimax optimality for Nystr¨om methods usually require a strict condition, assuming that the target regression lies exactly in the hypothesis space. In this paper, to tackle these problems, we provide a reﬁned generalization analysis for Nystr ¨ om approximation in the agnostic setting, where the target regression may be out of the hypothesis space. Speciﬁcally, we show Nystr ¨ om approximation can still achieve the capacity-dependent optimal rates in the agnostic setting. To this end, we ﬁrst prove the capacity-dependent optimal guarantees of Nystr ¨ om approximation with the standard uniform sampling, which covers both loss functions and applies to some agnostic settings. Then, using data-dependent sampling, for example, leverage scores sampling, we derive the capacity-dependent optimal rates that apply to the whole range of the agnostic setting. To our best knowledge, the capacity-dependent optimality for the whole range of the agnostic setting is ﬁrst achieved and novel in Nystr ¨ om approximation.