Federated machine unlearning (FMU) aims to remove the inﬂuence of a speciﬁed subset of training data upon request from a trained federated learning model. Despite achieving remarkable performance, existing FMU techniques suffer from inefﬁciency due to two sequential operations of training and retraining/unlearning on large-scale datasets. Our prior study, PCMU, was proposed to improve the efﬁciency of centralized machine unlearning (CMU) with certiﬁed guarantees, by simultaneously executing the training and unlearning operations. This paper proposes a fast FMU algorithm, FFMU, for improving the FMU efﬁciency while maintaining the unlearning quality. The PCMU method is leveraged to train a local machine learning (MU) model on each edge device. We propose to employ nonlinear functional analysis techniques to reﬁn the local MU models as output functions of a Nemytskii operator. We conduct theoretical analysis to derive that the Nemytskii operator has a global Lipschitz constant, which allows us to bound the difference between two MU models regarding the distance between their gradients. Based on the Nemyt-skii operator and average smooth local gradients, the global MU model on the server is guaranteed to achieve close performance to each local MU model with the certiﬁed guarantees.