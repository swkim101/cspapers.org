Enforcing orthogonality in convolutional neural networks is a remedy for gradient vanishing/exploding problems and sensitivity to perturbation. Many previous approaches for orthogonal convolutions enforce orthogonality on its ﬂattened kernel, which, however, do not lead to the orthogonality of the operation. Some recent approaches consider orthogonality for standard convolutional layers and propose speciﬁc classes of their realizations. In this work, we propose a theoretical framework that establishes the equivalence between diverse orthogonal convolutional layers in the spatial domain and the paraunitary systems in the spectral domain. Since 1D parau-nitary systems admit a complete factorization, we can parameterize any separable orthogonal convolution as a composition of spatial ﬁlters. As a result, our framework endows high expressive power to various convolutional layers while maintaining their exact orthogonality. Furthermore, our layers are memory and computationally ef-ﬁcient for deep networks compared to previous designs. Our versatile framework, for the ﬁrst time, enables the study of architectural designs for deep orthogonal networks, such as choices of skip connection, initialization, stride, and dilation. Consequently, we scale up orthogonal networks to deep architectures, including ResNet and ShufﬂeNet, substantially outperforming their shallower counterparts. Finally, we show how to construct residual ﬂows, a ﬂow-based generative model that requires strict Lipschitzness, us-ing our orthogonal networks. Our code will be publicly available at https://github.com/ umd-huang-lab/ortho-conv .