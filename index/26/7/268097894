In many online learning paradigms, convexity plays a central role in the derivation and analysis of online learning algorithms. The results, however, fail to be extended to the non-convex settings, which are necessitated by tons of recent applications. The Online Non-Convex Learning problem generalizes the classic Online Convex Optimization framework by relaxing the convexity assumption on the cost function (to a Lipschitz continuous function) and the decision set. The state-of-the-art result for ønco demonstrates that the classic Hedge algorithm attains a sublinear regret of O(√T log T). The regret lower bound for øco, however, is Omega(√T), and to the best of our knowledge, there is no result in the context of the ønco problem achieving the same bound. This paper proposes the Online Recursive Weighting algorithm with regret of O(√T), matching the tight regret lower bound for the øco problem, and fills the regret gap between the state-of-the-art results in the online convex and non-convex optimization problems.