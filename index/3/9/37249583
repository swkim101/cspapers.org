The theme was the effect of perturbations of the defining parameters of a neural network due to: 1) measurements (particularly with analog networks); 2) discretization due to a) digital implementation of analog nets; b) bounded-precision implementation of digital networks; or c) inaccurate evaluation of the transfer function(s); 3) noise in or incomplete input and/or output of the net or individual cells (particularly with analog networks). 
 
The workshop presentations address these problems in various ways. Some develop models to understand the influence of errors/perturbation in the output, learning and general behavior of the net (probabilistic in Piche and Tresp; optimization in Rojas; dynamical systems in Botelho & Garzon). Others attempt to identify desirable properties that are to be preserved by neural network solutions (equilibria under faster convergence in Peterfreund & Baram; decision regions in Cohen). Of particular interest is to develop networks that compute robustly, in the sense that small perturbations of their parameters do not affect their dynamical and observable behavior (stability in biological networks in Chauvet & Chauvet; oscillation stability in learning in Rojas; hysterectic finite-state machine simulation in Casey). In particular, understand how biological networks cope with uncertainty and errors (Chauvet & Chauvet) through the type of stability that they exhibit.