We propose a framework towards the integration of information sensors based on the idea that the stimulus perceived through different sensors are spatial-time correlated for a short time period. Applications in robotics need to be able to process information from multiple sensors, for instance, in the case of a visible talking person. How can we relate this kind of information in a simple way, without making use of high level representation? This is the question that we want to address. A new framework based on a correlation measure of low level data information is proposed. This low level correlation measure can be used as an integration data engine to support high level task description. In the paper a coherent approach from sensor level to task level for developing a robot which can handle a large number of sensors and actuators is developed. An example how this approach can be used for a visual-sound integration task is also presented.