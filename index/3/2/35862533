The support vector machine (SVM) has played an important role in bringing certain themes to the fore in computationally oriented statistics. However, it is important to place the SVM in context as but one member of a class of closely related algorithms for nonlinear classification. As we discuss, several of the “open problems” identified by the authors have in fact been the subject of a significant literature, a literature that may have been missed because it has been aimed not only at the SVM but at a broader family of algorithms. Keeping the broader class of algorithms in mind also helps to make clear that the SVM involves certain specific algorithmic choices, some of which have favorable consequences and others of which have unfavorable consequences—both in theory and in practice. The broader context helps to clarify the ties of the SVM to the surrounding statistical literature. We have at least two broader contexts in mind for the SVM. The first is the family of “large-margin” classification algorithms—a class that includes boosting and logistic regression. All of these algorithms involve the minimization of a convex contrast or loss function that upper bounds the 0–1 loss function. The SVM makes a specific choice of convex loss function—the so-called hinge loss. Hinge loss has some potentially desirable properties (e.g., sparseness) and some potentially undesirable properties (e.g., lack of calibration to posterior probabilities). As we discuss, much of the theoretical analysis of the SVM is best carried out by focusing on convexity and abstracting away from the details of specific loss functions. Second, as the authors note, the SVM is an instance of the broader family of statistical procedures based on