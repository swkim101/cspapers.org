Activity recognition along with device position recognition can provide contextual cues suitable to infer user interruptibility and device accessibility. Our system fuses data from accelerometer and multiple light sensors to classify activities and device positions. Previously published results achieve robust activity recognition performance with multiple sensors attached to fixed body positions, a model suitable for use cases such as healthcare and fitness. We achieve comparable activity recognition performance using smartphones placed in unknown on-body positions including pocket, holster and hand. Results obtained from a diverse data set show that motion state and device position are classified with macro-averaged f-scores 92.6% and 66.8% respectively, over six activities and seven device positions. We demonstrate the performance of our classifier with an implementation running on the Android platform, that visitors can try out.