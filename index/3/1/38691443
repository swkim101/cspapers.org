A wide variety of evaluation techniques are presented in the literature of computer science education. Many papers only present observations and anecdotal evidence. Others may present results from an examination or an attitude survey. A few papers report on studies that use the classical educational evaluation model of a hypothesis being tested with a control group and one or more treatment groups. Sometimes the widespread use of a particular approach can indicate educational effectiveness, as evidenced by the switch horn programming languages like Basic and Fortran to block structured languages in the late 1970s. Formative evaluation of a particular approach as it is being developed can help provide mid-course corrections that result in a better final product. Summative evaluation can be used to convince others that this particular approach has proven to be effective. The necessary evidence may depend on the person considering adoption of the proposed approach. Although anecdotal evidence may be sufficient to convince a close colleague, more formal techniques are necessary for more general audiences. This panel will explore various approaches to evaluation in computer science education and the need for measurable objectives that go beyond simplistic statements such as “the students liked it.” Jerry Engel is currently working at the National Science Foundation where he considers evaluation plans in grant proposals and reports of project results. Jim Miller is the editor of the SIGCSE Bulletin and Keith Barker is the editor of Computer Science Education. Nell Dale and Barry Kurtz have been authors of introducto~ textbooks and are active researchers in computer science education. Harriet Taylor is using formal evaluation techniques to determine the long range effects of the choice of the introducto~ programming language. A lively exchange of viewpoints can be expected.