Pseudo-relevance feedback (PRF) typically adds T new terms to the initial query by assuming that the top R documents in the initial ranked output are relevant. Although PRF is widely used in laboratory experiments for improving the average performance, it actually hurts performance for approximately one-third of a given set of search requests [2, 3]. One problem is that PRF uses the same values of R and T for all test requests once they have been optimized with a set of training requests. To enhance the reliability of PRF, Flexible PRF (FPRF) varies R and/or T across requests. An overview of FPRF approaches is given in [3]. This paper re-examines and extends one of these approaches, namely, the use of optimization tables [2]: Firstly, training requests are divided into several groups based on a measure such as the document scores in the initial ranked output, and the PRF parameters are optimized for each group. Then, for each test request, its matching group is determined using the same measure, and the optimal parameter values for that particular group is reused with the test request. In previous work on optimization tables using Japanese test collections [2], neither FPRF nor traditional PRF was successful, probably because: (1) Their relevance data for training covered only 2% (5,080 documents) of the document collection, (2) Although it is not clear how best to estimate the optimal values of R and T individually, they varied both at the same time; (3) Although a large set of training requests is desirable for reliable parameter estimation, they only had 50 training requests. In contrast, this paper uses English test collections, with better experimental settings: (1) We use TREC test collections with \complete" relevance data, and a variant of the NTCIR-2 English test collection whose relevance data for training covers 58% of the document collection [3]; (2) We initially treat the problems of varying R and T separately; (3) We use up to 100 requests for training.