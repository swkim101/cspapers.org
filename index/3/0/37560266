Our willingness to deliberately trade accuracy of computing systems for significant resource savings, notably energy consumption, got a boost from two directions. First, energy (or power, the more popularly used measure) consumption started emerging as a serious hurdle to our ability to continue scaling the complexity of processors, and thus enable ever richer computing applications. This "energy hurdle" spanned the gamut from large data-centers to portable embedded computing systems. Second, many believed that an engine of growth that supported scaling, captured by Gordon Moore's remarkable prophecy (Moore's law), was headed towards an irrevocable cliff edge - when this happens, our ability to produce computing systems whose hardware would support precise or exact computing would diminish greatly. In this talk which emphasizes the physical and hardware layers of abstraction where all of these troubles start (after all energy is rooted in thermodynamics), I will first review reasons that compelled and encouraged us to consider trading accuracy for energy savings deliberately resulting in inexact computing.