We study a transfer learning framework where source and target datasets are heterogeneous in both feature and label spaces. Speciﬁcally, we do not assume explicit relations between source and target tasks a priori , and thus it is crucial to deter-mine what and what not to transfer from source knowledge. Towards this goal, we deﬁne a new heterogeneous transfer learning approach that (1) selects and attends to an optimized subset of source samples to transfer knowledge from, and (2) builds a uniﬁed transfer network that learns from both source and target knowledge. This method, termed “Attentional Heterogeneous Transfer”, along with a newly proposed unsupervised transfer loss, improve upon the previous state-of-the-art approaches on extensive simulations as well as a challenging hetero-lingual text classiﬁcation task.