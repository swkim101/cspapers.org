â€”Language is a powerful tool that enables humans and robots to interact without the need for complex graphical interfaces. Statistical techniques for interpreting the meaning of utterances in complex domestic environments, however, remain computationally intensive and are prone to error. Herein we present a model for language understanding that uses parse trees and environment models to infer both the structure and the meaning of probabilistic graphical models for symbol grounding. This model, called the Hierarchical Distributed Correspondence Graph (HDCG), exploits information about symbols that are expressed in the corpus to learn rules that describe how to construct graphical models that are faster to search. In a comparative experiment, we observe an order of magnitude improvement in the speed of probabilistic inference over the Distributed Correspondence Graph (DCG) model. We conclude with a discussion of potential applications in rehabilitation and assistive robotics and future directions of research.