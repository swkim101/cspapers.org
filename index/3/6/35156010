Word-position-independent and word-position dependent n-gram probabilities were estimated from a large English language corpus. A text-recognition problem was simulated, and using the estimated n-gram probabilities, four experiments were conducted by the following methods of classification: without contextual information, Raviv's recursive Bayes algorithm, the modified Viterbi algorithm, and a proposed heuristic approximation to Raviv's algorithm. Based on the estimates of the probabilities of misclassification observed in the four experiments, the above methods are compared. The heuristic approximation of Raviv's algorithm performed just as well as Raviv's and required far less computation.