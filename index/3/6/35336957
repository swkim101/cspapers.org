Path planning for mobile robots requires rapidly finding collision-free trajectories in an uncertain and changing environment. Full collision checking with detailed, online-revised representations of the robot and world imposes a delay that undermines reactive obstacle avoidance. As a result, reactive vision-based approaches make various assumptions to arrive at simplified representations, such as circular or spherical robot shapes reducible to point masses, or obstacles that always rise from the ground. We seek to avoid these problems by modeling the robot directly in perception space so that collisionfree trajectories can be sought in a consistent representation with minimal processing needs. Here perception space refers to the depth space image measurements available by modern consumer range sensors. We hallucinate a robot navigating through the world and synthesize depth images of its path for comparison against the directly sensed depth images of the local world. The approach performs collision checking in a 3D volume but only requires 2D image comparisons. Experiments show that an implementation is able to negotiate an obstacle course consisting of miscellaneous objects in real-time.