Inferring the goals, preferences and restrictions of strategically behaving agents is a common goal in many situations, and an important requirement for enabling computer systems to better model and understand human users. Inverse reinforcement learning (IRL) is one method for performing this kind of inference based on observations of the agent’s behavior. However, traditional IRL methods are only applicable when the observations are in the form of state-action paths – an assumption which does not hold in many real-world modelling settings. This paper demonstrates that inference is possible even with an arbitrary observation noise model.