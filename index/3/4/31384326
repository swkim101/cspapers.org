High performance computing (HPC) is a general term used to describe the wide range of hardware, programming techniques and applications that have played an important role in American researchers' pursuit of Grand Challenge Problems. Solid understanding of these ever-evolving technologies, their potential and effectiveness, should be an integral part of undergraduate curricula preparing learners for the work and research environment of the 21st century. As with any curriculum transformation, the incorporation of HPC technologies into contemporary undergraduate teaching faces a series of adjustment problems, in addition to making more visible the traditional pedagogical issues. The new model of conducting scientific investigation calls for multi-disciplinary, data- and computationally-intensive, collaborative research involving teams of scientists with diverse backgrounds and residing in different geographic locations, who use supercomputers and work cooperatively over the Internet. While this model becomes ubiquitous in leading edge research, its replication in education faces several challenges. In this paper, we identify and explore ten such "Grand Challenges," of pedagogical, psychological, organizational, and technical origins.Promoting the incorporation of high performance computing technologies into undergraduate curricula is the charge of the NPACI (National Partnership for Advanced Computational Infrastructure) Education Center on Computational Science and Engineering (EC/CSE), created on the campus of San Diego State University in October 1997. This paper reports on the Center's first year experience in tackling this challenging mission, when we experimented with various ways and formats of technology outreach. In this paper, we show that supporting HPC technologies in undergraduate teaching is a multi-faceted effort requiring a specially constructed comprehensive educational infrastructure.