An MPI collective operation involves multiple processes that may arrive at the operation at different times. We call such arrival timing the process arrival pattern. The process arrival pattern can significantly affect the performance of a collective operation since it decides the time when each process starts participating in the operation. We characterize the process arrival pattern in a set of MPI programs on two common cluster platforms, study it in a micro-benchmark with balanced loads, and evaluate its impacts on some collective communication algorithms. The results indicate that application developers cannot effectively control the process arrival patterns in cluster environments. Furthermore, the performance of communication algorithms is sensitive to process arrival patterns. Based on these observations, we conclude that, to develop MPI collective routines that can achieve high performance for practical applications in cluster environments, MPI developers must take the process arrival pattern into consideration.