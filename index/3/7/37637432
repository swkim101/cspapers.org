We present a new model for the perceptual reasoning involved in hand/eye coordination, and we show how this model can be developed into a control mechanism for a robot manipUlator with a visual sensor. This new approach overcomes the hig,h computational cost, the lack of robustness, and the need for precise calibration that plague traditional approaches. At the heart of our model is the Perceptual Kinematic Map (PKM), a direct mapping from the control space of the manipulator onto a space defined by a set of measurable image parameters. By exploring its workspace, the robot learns, qualitatively, the topology of its PKM and thus acquires the dexterity for future tasks, in a striking parallel to biological systems.