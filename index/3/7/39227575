Bruner, Goodnow and Austin (1956) describe a “focussing strategy” for learning simple conjunctive concepts in a situation where the learner is shown a sequence of instances one at a time, each being an example or non-example of the concept to be learned. We consider the extension of this strategy to concepts in which the individual features are hierarchically structured, as is the case, for example, for the materials used by Winston (1975). The essence of the extended strategy is to represent an hypothesis by a pair of nodes in the hierarchy, instead of just a single node. The “lower”, more specialised node indicates the set of instances that have already been inferred to be examples of the concept. The “upper”, more abstract node indicates an inferred limit on how general the concept can be. Thus the nodes act as bounds between which the concept must lie, and represent respectively sufficient and necessary conditions for an instance to be an example of the concept. We generalise this notion of an hierarchicallystructured feature by defining a description space (d-space) as an upper semilattice (D, , satisfying a certain finiteness condition. We show that the Cartesian product of a number of these d-spaces is itself a d-space. This provides us with a simple, uniform notation for referring to concepts, instances and hypotheses. We describe a simple nondeterministic algorithm which implements the “focussing strategy” for such a conjunctive description space, and discuss aspects of its behaviour on the concept learning task, for example: