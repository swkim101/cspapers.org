Unsupervised domain adaptation has attracted appealing academic attentions by transferring knowledge from labeled source domain to unlabeled target domain. However, most existing methods assume the source data are drawn from a single domain, which cannot be successfully applied to explore complementarily trans-ferable knowledge from multiple source domains with large distribution discrepancies. Moreover, they require access to source data during training, which are inefﬁcient and unpractical due to privacy preservation and memory storage. To address these challenges, we develop a novel C onﬁdent-Anchor-induced multi-source-free D omain Adaptation (CAiDA) model, which is a pioneer exploration of knowledge adaptation from multiple source domains to the unlabeled target domain without any source data, but with only pre-trained source models . Specifically, a source-speciﬁc transferable perception module is proposed to automatically quantify the contributions of the complementary knowledge transferred from multi-source domains to the target domain. To generate pseudo labels for the target domain without access to the source data, we develop a conﬁdent-anchor-induced pseudo label generator by constructing a conﬁdent anchor group and assigning each unconﬁdent target sample with a semantic-nearest conﬁdent anchor. Furthermore, a class-relationship-aware consistency loss is proposed to preserve consistent inter-class relationships by aligning soft confusion matrices across domains. Theoretical analysis answers why multi-source domains are better than a single source domain, and establishes a novel learning bound to show the effectiveness of exploiting multi-source domains. Experiments on several representative datasets illustrate the superiority of our proposed CAiDA model. The code is available at https://github.com/Learning-group123/CAiDA .