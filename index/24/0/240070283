A multiclass classiﬁer is said to be top-label calibrated if the reported probability for the predicted class—the top-label—is calibrated, conditioned on the top-label. This conditioning on the top-label is ab-sent in the closely related and popular notion of conﬁdence calibration, which we argue makes conﬁdence calibration diﬃcult to interpret for decision-making. We propose top-label calibration as a rectiﬁcation of conﬁdence calibration. Further, we outline a multiclass-to-binary (M2B) reduction framework that uniﬁes conﬁdence, top-label, and class-wise calibration, among others. As its name suggests, M2B works by reducing multiclass calibration to numerous binary calibration problems, each of which can be solved using simple binary calibration routines. We instantiate the M2B framework with the well-studied histogram binning (HB) binary calibrator, and prove that the overall procedure is multiclass calibrated without making any assumptions on the underlying data distribution. In an empirical evaluation with four deep net architectures on CIFAR-10 and CIFAR-100, we ﬁnd that the M2B + HB procedure achieves lower top-label and class-wise calibration error than other approaches such as temperature scaling. Code for this work is available at https://github.com/aigen/df-posthoc-calibration .