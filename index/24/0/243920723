In this work, we propose a new counter-based stochastic-behaving approximate integer unsigned multiplier, called COSAIM, for many emerging error tolerant application workloads such as deep neural networks. Unlike existing approximate multipliers, which are based on some deterministic ad-hoc methods or mathematical formula, the new design is an improved stochastic multiplier, which performs improved sequential counting for multiplication operation in a deterministic way. In this work, we further improve the counting efficiency by introducing approximate schemes to significantly speed up the counting process, which leads to significant clock cycle reduction with no accuracy loss. COSAIM bears all the advantages of stochastic computing such as built-in configurability for progressive performance-accuracy trade-off. At the same time, it shows very small latency and high energy efficiency. Our evaluation shows that the COSAIM with error improvement operation can achieve very low error bias (0.06%), along with lower mean error (0.30% to 3.49%), and low peak errors (around 1.81%) with variance of 1. $47\times 10^{-4}$ %. Experimental results obtained from Xilinx ISE show that compared with the 8-bit exact multiplier baseline, COSAIM can save up to 53.95%, 32.84%, 52.24%, 21.05% in area, power, energy and the product Area. 1/Throughput, respectively. Furthermore, by doing shared parallel design, COSAIM can further lead to improvements in area, power and energy reduction by 60.44%, 53.33% and 68.54%, respectively compared to the baseline. We also implement COSAIM in a Convolution Neural Network (CNN) and test it on CIFAR10 dataset and find that CNN with COSAIM delivers similar inference accuracy compared to some state of art approximate multipliers.