Knowledge graph (KG) reasoning is a significant method for KG completion. To enhance the explainability of KG reasoning, some studies adopt reinforcement learning (RL) to complete the multi-hop reasoning. However, RL-based reasoning methods are severely limited by few-shot relations (only contain few triplets). To tackle the problem, recent studies introduce meta-learning into RL-based methods to improve reasoning performance. However, the generalization abilities of their models are limited due to the problem of low reasoning accuracies over hard relations (e.g., language and title). To overcome this problem, we propose a novel model called THML (Two-level Hardness-aware Meta-reinforcement Learning). Specifically, the model contains the following two components: (1) A hardness-aware meta-reinforcement learning method is proposed to predict the missing element by training hardness-aware batches. (2) A two-level hardness-aware sampling is proposed to effectively generate new hardness-aware batches from relation level and relation-cluster level. The generalization ability of our model is significantly improved by repeating the process of these two components in an alternate way. The experimental results demonstrate that THML notably outperforms the state-of-the-art approaches in few-shot scenarios.