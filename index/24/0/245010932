Active learning sequentially selects the best instance for labeling by optimizing an acquisition function to enhance data/label efﬁciency. The selection can be either from a discrete instance set (pool-based scenario) or a continuous instance space (query synthesis scenario). In this work, we study both active learning scenarios for Gaussian Process Classiﬁcation (GPC). The existing active learning strategies that maximize the Estimated Error Reduction (EER) aim at reducing the classiﬁcation error after training with the new acquired instance in a one-step-look-ahead manner. The computation of EER-based acquisition functions is typically prohibitive as it requires retraining the GPC with every new query. Moreover, as the EER is not smooth, it can not be combined with gradient-based optimization techniques to efﬁciently explore the continuous instance space for query synthesis. To overcome these critical limitations, we develop computationally efﬁcient algorithms for EER-based active learning with GPC. We derive the joint predictive distribution of label pairs as a one-dimensional integral, as a result of which the computation of the acquisition function avoids retraining the GPC for each query, remarkably reducing the computational overhead. We also derive the gradient chain rule to efﬁciently calculate the gradient of the acquisition function, which leads to the ﬁrst query synthesis active learning algorithm implementing EER-based strategies. Our experiments clearly demonstrate the computational efﬁciency of the proposed algorithms. We also benchmark our algorithms on both synthetic and real-world datasets, which show superior performance in terms of sampling efﬁciency compared to the existing state-of-the-art algorithms.