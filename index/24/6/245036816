We consider the problem of preference bandits in the contextual setting. At each round, the learner is presented with a context set of K items, chosen randomly from a potentially inﬁnite set of arms D ⊆ R d . However, unlike classical contextual bandits, our framework only allows the learner to receive feedback in terms of item preferences: At each round, the learner is allowed to play a subset of size q (any q ∈ { 2 , . . . , K } ) upon which only a (noisy) winner of the subset is revealed. Yet, same as the classical setup, the goal is still to compete against the best context arm at each round. The problem is relevant in various online decision-making scenarios, including recommender systems, information retrieval, tournament ranking–typically any application where it’s easier to elicit the items’ relative strength instead of their absolute scores. To the best of our knowledge, this work is the ﬁrst to consider preference-based stochastic contextual bandits for potentially inﬁnite decision spaces. We start with presenting two algorithms for the special case of pairwise preferences ( q = 2) : The ﬁrst algorithm is simple and easy to implement with an ˜ O ( d √ T ) regret guarantee, while the second algorithm is shown to achieve the optimal ˜ O ( √ dT ) regret, as follows from our Ω( √ dT ) matching lower bound analysis. We then proceed to analyze the problem for any general q -subsetwise preferences ( q ≥ 2 ), where surprisingly, our lower bound proves the fundamental performance limit to be Ω( √ dT ) yet again, independent of the subsetsize q . Following this, we propose a matching upper bound algorithm justifying the tightness of our results. This implies having access to subsetwise preferences does not help in faster information aggregation for our feedback model. All the results are corroborated empirically against existing baselines.