Resistive computing systems (RCSs) promise exascale computing capabilities to inference engines for deep learning. However, the classification accuracy of the accelerated neural networks may be degraded by defects. While hardware-aware training schemes can restore the accuracy of convolutional neural networks (CNNs) with low throughput, the schemes are rendered futile when weights are replicated to improve throughput. On the other hand, we discover that weight replication provides new opportunities for data layout organization. In this paper, we propose a framework for resilient deployment of high throughput CNNs to RCSs. The framework is based on integrating a data layout organization step and a distribution guided training step into the flow for mapping CNNs to RCSs. The data layout organization step involves modifying the weight matrix to crossbar assignments using channel, pixel, and hybrid channel-pixel data layout transformations. The distribution guided training is focused on training CNNs with weights that are amenable for data layout organization. The experimental results demonstrate that the proposed techniques expand the average solution space for data layout organization with 1.4Ã— 1014X. This translates into that CNNs with high throughput can be deployed onto RCS with up to 10% defects and still attain high classification accuracy.