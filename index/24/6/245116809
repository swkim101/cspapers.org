We study the problem of properly learning linear threshold functions (LTFs) in the learning from label proportions (LLP) framework. In this, the learning is on a collection of bags of feature-vectors with only the proportion of labels available for each bag. First, we provide an algorithm that, given a collection of such bags each of size at most two whose label proportions are consistent with (i.e., the bags are satisﬁed by) an unknown LTF, efﬁciently produces an LTF that satisﬁes at least (2 / 5) -fraction of the bags. If all the bags are non-monochromatic (i.e., bags of size two with differently labeled feature-vectors) the algorithm satisﬁes at least (1 / 2) -fraction of them. For the special case of OR over the d -dimensional boolean vectors, we give an algorithm which computes an LTF achieving an additional Ω(1 /d ) in accuracy for the two cases. Our main result provides evidence that these algorithmic bounds cannot be signiﬁ-cantly improved, even for learning monotone ORs using LTFs. We prove that it is NP -hard, given a collection of non-monochromatic bags which are all satisﬁed by some monotone OR, to compute any function of constantly many LTFs that satisﬁes (1 / 2 + ε ) -fraction of the bags for any constant ε > 0 . This bound is tight for the non-monochromatic bags case. The above is in contrast to the usual supervised learning setup (i.e., unit-sized bags) in which LTFs are efﬁciently learnable to arbitrary accuracy using linear programming, and even a trivial algorithm (any LTF or its complement) achieves an accuracy of 1 / 2 . These techniques however, fail in the LLP setting. Indeed, we show that the LLP learning of LTFs (even for the special case of monotone ORs) using LTFs dramatically increases in complexity as soon as bags of size two are allowed. Our work gives the ﬁrst inapproximability for LLP learning LTFs