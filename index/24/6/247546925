Hyperbolic space is particularly useful for embedding data with hierarchical structure; however, representing hyperbolic space with ordinary ﬂoating-point numbers greatly affects the performance due to its ineluctable numerical errors. Simply increasing the precision of ﬂoats fails to solve the problem and incurs a high computation cost for simulating greater-than-double-precision ﬂoats on hardware such as GPUs, which does not support them. In this paper, we propose a simple, feasible-on-GPUs, and easy-to-understand solution for numerically accurate learning on hyperbolic space. We do this with a new approach to represent hyperbolic space using multi-component ﬂoating-point (MCF) in the Poincaré upper-half space model. Theoretically and experimentally we show our model has small numerical error, and on embedding tasks across various datasets, models represented by multi-component ﬂoating-points gain more capacity and run signiﬁcantly faster on GPUs than prior work.