In two-player zero-sum extensive-form games, Nash equilibrium prescribes optimal strategies against perfectly rational opponents. However, it does not guarantee rational play in parts of the game tree that can only be reached by the players making mistakes. This can be problematic when operationalizing equilibria in the real world among imperfect players. Trembling-hand reﬁnements are a sound remedy to this issue, and are subsets of Nash equilibria that are designed to handle the possibility that any of the players may make mistakes. In this paper, we initiate the study of equilibrium reﬁnements for settings where one of the players is perfectly rational (the “machine”) and the other may make mistakes. As we show, this endeavor has many pitfalls: many intuitively appealing approaches to reﬁnement fail in various ways. On the positive side, we introduce a modiﬁcation of the classical quasi-perfect equilibrium (QPE) reﬁnement, which we call the one-sided quasi-perfect equilibrium . Unlike QPE, one-sided QPE only accounts for mistakes from one player and assumes that no mistakes will be made by the machine. We present experiments on standard benchmark games and an endgame from the famous man-machine match where the AI Libratus was the ﬁrst to beat top human specialist professionals in heads-up no-limit Texas hold’em poker. We show that one-sided QPE can be computed more efﬁciently than all known prior reﬁnements, paving the way to wider adoption of Nash equilibrium reﬁnements in settings with perfectly rational machines (or humans perfectly actuating machine-generated strategies) that interact with players prone to mistakes. We also show that one-sided QPE tends to play better than a Nash equilibrium strategy against imperfect opponents.