Sequential recommendation basically aims to capture user evolving preference. Intuitively, a user interacts with an item usually because of some specific feature, and user evolving preference is essentially determined by a series of important features along the time line. However, existing sequential models usually represent each item by a unified embedding, which fails to distinguish item features, let along modeling the feature sequences. To bridge this gap, in this paper, we propose a novel sequential recommender model by learning the key item feature sequences underlying user behaviors, which facilitates more focused model optimization and better recommendation performance. To achieve this goal, we firstly represent each item by explicit or latent features, and then build both soft and hard models to route optimal feature sequences. More specifically, in the soft model, we design a 2D attention mechanism, which simultaneously distinguishes the importances of the items in a sequence and the features for the same item. For the hard model, we regard the feature routing problem as a Markov decision process, and propose a reinforcement learning method to generate feature sequences, which can lead to the lowered negative log-likelihood. In the experiments, we compare our model with the state-of-the-art methods based on real-world datasets, where we can empirically demonstrate 8.2 and 16.1 improvements of our model on NDCG and MRR, respectively.