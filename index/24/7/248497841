The only learnable parameters (117,963 in all) in our model are those of the autoencoder. The encoder is made of 6 blocks of 3× 3 convolutions with 16, 32, 64, 64, 32 and 8 channels followed by max-pooling, batch normalization and ReLu layers, except for the last block which does not have a ReLu layer. The decoder is a symmetric copy of the encoder. As our images are 64 × 64, the last convolutional block yields a feature map with 8 channels and 1 × 1 spatial dimension, which is reshaped into an 8 × 1 vector. The latent code we consider is thus directly the output of the convolutional encoder. Contrary to [1], we do not follow our encoder by fully-connected layers to obtain a compact code since the output of the convolutional encoder is alreay quite compact. Models without updates take 2.5 hours to train on a Tesla V100-SXM2 GPU, and models with updates take 4 hours to train. Models including control take longer to train (4 hours without updates and 6 hours with partial online updates) since the video sequences considered are longer. All models are trained for 200 epochs with a batch size of 16 and a learning rate of 10−3 which is divided by 2 every 20 epochs.