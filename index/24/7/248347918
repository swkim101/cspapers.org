With the rapid growth of video resources on the Internet, text-video retrieval has become a common requirement. Scholars handled text-video retrieval tasks with two-broad-category: concept-based methods and neural semantics match networks. Besides deep neural semantics matching models, some scholars mined queries and videos relationships from click-graphs, which express the usersâ€™ implicit judgments on relevance relations. However, bad generalization of click-based or concept-based models hardly capture semantic information from short queries, which stunt existing methods to fully utilize the methods to enhance the IR performance. In this paper, we propose a framework ETHGS to combine the abilities of concept-based, click-based and semantic-based models in IR and publish a new video retrieval dataset QVT from a real-world video search engine. In ETHGS, we make use of tags (i.e. concept) to construct a heterogeneous graph to alleviate the sparsity of click-through data. And we also overcome the problem of long-tailed query representation without graph information by fusing tag embeddings to represent queries. ETHGS leverages semantic embeddings to review deviant semantic information from graph nodes information. Finally, we evaluate our model ETHGS on the QVT.