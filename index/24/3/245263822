To capture the geometry of an object by an autonomous system, next best view (NBV) planning can be used to determine the path a robot will take. However, current NBV planning algorithms do not distinguish between objects that need to be mapped and everything else in the environment; leading to inefficient search strategies. In this paper we present a novel approach for NBV planning that accounts for the importance of objects in the environment to inform navigation. Using weighted entropy to encode object utilities computed via semantic segmentation, we evaluate our approach over a set of virtual Gazebo environments comparable to construction scales. Our results show that using semantic information reduces the time required to capture a target object by at least 40 percent.