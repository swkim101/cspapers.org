Knowledge distillation (KD) has recently been identiﬁed as a method that can unintentionally leak private information regarding the details of a teacher model to an unauthorized student. Recent research in developing undistillable nasty teachers that can protect model conﬁdentiality has gained signiﬁcant attention. However, the level of protection these nasty models offer has been largely untested. In this paper, we show that transferring knowledge to a shallow sub-section of a student can largely reduce a teacher’s inﬂuence. By exploring the depth of the shallow subsection, we then present a distillation technique that enables a skeptical student model to learn even from a nasty teacher. To evaluate the efﬁcacy of our skeptical students, we conducted experiments with several models with KD under both training data-available and data-free scenarios for various datasets. While distilling from nasty teachers, compared to the normal student models, skeptical students consistently provide superior classiﬁcation performance of up to ∼ 59 . 5% . Moreover, similar to normal students, skeptical students maintain high classiﬁcation accuracy when distilled from a normal teacher, showing their efﬁcacy irrespective of the teacher being nasty or not. We believe the ability of skeptical students to largely diminish the KD-immunity of a potentially nasty teacher will motivate the research community to create more robust mechanisms for model conﬁdentiality. We have open-sourced the code at github.com/ksouvik52/Skeptical2021 .