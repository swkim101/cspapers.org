Several recent works in machine learning have focused on evaluating the test-time robustness of a classiﬁer: how well the classiﬁer performs not just on the target domain it was trained upon, but upon perturbed examples. In these settings, the focus has largely been on two extremes of robustness: the robustness to perturbations drawn at random from within some distribution (i.e., robustness to random perturbations), and the robustness to the worst case perturbation in some set (i.e., adversarial robustness). In this paper, we argue that a sliding scale between these two extremes provides a valuable additional metric by which to gauge robustness. Speciﬁcally, we illustrate that each of these two extremes is naturally characterized by a (func-tional) q -norm over perturbation space, with q = 1 corresponding to robustness to random perturbations and q = ∞ corresponding to adversarial perturbations. We then present the main technical contribution of our paper: a method for efﬁciently estimating the value of these norms by interpreting them as the partition function of a particular distribution, then using path sampling with MCMC methods to estimate this partition function (either traditional Metropolis-Hastings for non-differentiable perturbations, or Hamiltonian Monte Carlo for differentiable perturbations). We show that our approach provides substantially better estimates than simple random sampling of the actual “intermediate- q ” robustness of standard, data-augmented, and adversarially-trained classiﬁers, illustrating a clear tradeoff between classiﬁers that optimize different metrics