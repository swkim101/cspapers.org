Differentiable programs have recently attracted much interest due to their inter-pretability, compositionality, and their efﬁciency to leverage differentiable training. However, synthesizing differentiable programs requires optimizing over a combinatorial, rapidly exploded space of program architectures. Despite the development of effective pruning heuristics, previous works essentially enumerate the discrete search space of program architectures, which is inefﬁcient. We propose to encode program architecture search as learning the probability distribution over all possible program derivations induced by a context-free grammar. This allows the search algorithm to efﬁciently prune away unlikely program derivations to synthesize optimal program architectures. To this end, an efﬁcient gradient-descent based method is developed to conduct program architecture search in a continuous relaxation of the discrete space of grammar rules. Experiment results on four sequence classiﬁcation tasks demonstrate that our program synthesizer excels in discovering program architectures that lead to differentiable programs with higher F 1 scores, while being more efﬁcient than state-of-the-art program synthesis methods.