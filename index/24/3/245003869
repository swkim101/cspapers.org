The representations learned by large-scale NLP models such as BERT have been widely used in various tasks. However, the increasing model size of the pre-trained models also brings efﬁciency challenges, including inference speed and model size when deploying models on mobile devices. Speciﬁcally, most operations in BERT consist of matrix multiplications. These matrices are not low-rank and thus canonical matrix decompositions do not lead to efﬁcient approximations. In this paper, we observe that the learned representation of each layer lies in a low-dimensional space. Based on this observation, we propose DRONE ( d ata-awa r e l o w-ra n k compr e ssion), a provably optimal low-rank decomposition of weight matrices, which has a simple closed form solution that can be efﬁciently computed. DRONE can be applied to both fully-connected and self-attention layers appearing in the BERT model. In addition to compressing standard models, our method can also be used on distilled BERT models to further improve the compression rate. Experimental results show that DRONE is able to improve both model size and inference speed with limited loss in accuracy. Speciﬁcally, DRONE alone achieves 1.92x speedup on the MRPC task with only 1.5 % loss in accuracy, and when DRONE is combined with distillation, it further achieves over 12.3x speedup on various natural language inference tasks.