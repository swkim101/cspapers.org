What makes a classiﬁer have the ability to generalize? There have been a lot of important attempts to address this question, but a clear answer is still elusive. Proponents of complexity theory ﬁnd that the complexity of the classiﬁer’s function space is key to deciding generalization, whereas other recent work reveals that classiﬁers which extract invariant feature representations are likely to generalize better. Recent theoretical and empirical studies, however, have shown that even within a classiﬁer’s function space, there can be signiﬁcant differences in the ability to generalize. Speciﬁcally, empirical studies have shown that among functions which have a good training data ﬁt, functions with lower Kolmogorov complexity (KC) are likely to generalize better, while the opposite is true for functions of higher KC. Motivated by these ﬁndings, we propose, in this work, a novel measure of complexity called Kolmogorov Growth (KG), which we use to derive new generalization error bounds that only depend on the ﬁnal choice of the classiﬁcation function. Guided by the bounds, we propose a novel way of regularizing neural networks by constraining the network trajectory to remain in the low KG zone during training. Minimizing KG while learning is akin to applying the Occam’s razor to neural networks. The proposed approach, called network-to-network regularization, leads to clear improvements in the generalization ability of classiﬁers. We verify this for three popular image datasets (MNIST, CIFAR-10, CIFAR-100) across varying training data sizes. Empirical studies ﬁnd that conventional training of neural networks, unlike network-to-network regularization, leads to networks of high KG and lower test accuracies. Furthermore, we present the beneﬁts of N2N regularization in the scenario where the training data labels are noisy. Using N2N regularization, we achieve competitive performance on MNIST, CIFAR-10 and CIFAR-100 datasets with corrupted training labels, signiﬁcantly improving network performance compared to standard cross-entropy baselines in most cases. These ﬁndings illustrate the many beneﬁts obtained from imposing a function complexity prior like Kolmogorov Growth during the training process.