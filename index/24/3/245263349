In this paper we address an important problem in self-driving, forecasting multi-pedestrian motion and their shared scene occupancy map, which is critical for safe navigation. Our contributions are two-fold. First, we advocate for predicting both the individual motions as well as the scene occupancy map in order to effectively deal with missing detections caused by postprocessing, e.g. confidence thresholding and non-maximum suppression. Second, we propose a Scene-Actor Graph Neural Network (SA-GNN) which captures the interactions among pedestrians within the same scene, including those that have not been detected, by preserving the relative spatial information of pedestrians via 2D convolution and via message passing. We show that our scene-occupancy predictions are more accurate than those from state-of-the-art motion forecasting methods, while also matching their performance in pedestrian motion forecasting metrics on two large-scale real-world datasets, nuScenes and ATG4D.