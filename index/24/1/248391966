The content that a recommender system (RS) shows to users inﬂuences them. Therefore, when choosing a recommender to deploy, one is implicitly also choosing to induce speciﬁc internal states in users. Even more, systems trained via long-horizon optimization will have direct incentives to manipulate users: in this work, we focus on the incentive to shift user preferences so they are easier to satisfy. We argue that – before deployment – system designers should: estimate the shifts a recommender would induce; evaluate whether such shifts would be undesirable; and perhaps even actively optimize to avoid problematic shifts. These steps involve two challenging ingredients: estimation requires anticipating how hypothetical algorithms would inﬂuence user preferences if deployed – we do this by using historical user interaction data to train a predictive user model which implicitly contains their preference dynamics; evaluation and optimization additionally require metrics to assess whether such inﬂuences are manipulative or otherwise unwanted – we use the notion of “safe shifts”, that deﬁne a trust region within which behavior is safe: for instance, the natural way in which users would shift without interference from the system could be deemed “safe”. In simulated experiments, we show that our learned preference dynamics model is effective in estimating user preferences and how they would respond to new recommenders. Additionally, we show that recommenders that optimize for staying in the trust region can avoid manipulative behaviors while still generating engagement.