Sparse tensors appear frequently in federated deep learning, either as a direct artifact of the deep neural network’s gradients, or, as a result of an explicit sparsiﬁ-cation process. Existing communication primitives are agnostic to the challenges of deep learning; consequently, they impose unnecessary communication overhead. This paper introduces DeepReduce, a versatile framework for the compressed communication of sparse tensors, tailored to federated deep learning. DeepReduce decomposes sparse tensors into two sets, values and indices, and allows both independent and combined compression of these sets. We support a variety of standard compressors, such as Deﬂate for values, and Run-Length Encoding for indices. We also propose two novel compression schemes that achieve superior results: curve-ﬁtting based for values, and bloom-ﬁlter based for indices. DeepReduce is orthogonal to existing gradient sparsiﬁers and can be applied in conjunction with them, transparently to the end-user, to signiﬁcantly lower the communication overhead. As a proof of concept, we implement our approach on TensorFlow and PyTorch. Our experiments with real models demonstrate that DeepReduce transmits 320% less data than existing sparsiﬁers, without affecting accuracy. Code is available at https://github.com/hangxu0304/DeepReduce .