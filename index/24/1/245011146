Recently, Vision Transformer and its variants have shown great promise on various computer vision tasks. The ability of capturing local and global visual dependencies through self-attention is the key to its success. However, this also brings challenges due to quadratic computational overhead, especially for the high-resolution vision tasks ( e.g. , object detection). Many recent works have attempted to reduce the cost and improve model performance by applying either coarse-grained global attention or ﬁne-grained local attention. However, both approaches cripple the modeling power of the original self-attention mechanism of multi-layer Transformers, leading to sub-optimal solutions. In this paper, we present focal attention , a new attention mechanism that incorporates both ﬁne-grained local and coarse-grained global interactions. In this new mechanism, each token attends its closest surrounding tokens at ﬁne granularity and the tokens far away at coarse granularity, and thus can capture both short-and long-range visual dependencies efﬁciently and effectively. With focal attention, we build a new variant of Vision Transformer models, called Focal Transformers , which achieve superior performance over the state-of-the-art (SoTA) Vision Transformers on a range of public image classiﬁcation and object detection benchmarks. In particular, our Focal Transformer models with a moderate size of 51.1M and a large size of 89.8M achieve 83.6 % and 84.0 % Top-1 accuracy, respectively, on ImageNet classiﬁcation at 224 × 224 . When employed as the backbones, Focal Transformers achieve consistent and substantial improvements over the current SoTA Swin Transformers [43] across 6 different object detection methods. Our largest Focal Transformer yields 58.7 / 59.0 box mAPs and 50.9 / 51.3 mask mAPs on COCO mini-val/test-dev, and 55.4 mIoU on ADE20K for semantic segmentation, creating new SoTA on three of the most challenging computer vision tasks. Our code is available at: https://github. com/microsoft/Focal-Transformer .