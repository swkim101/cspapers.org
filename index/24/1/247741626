The scarcity of gold standard code-mixed to 001 pure language parallel data makes it difficult to 002 train translation models reliably. Prior work has 003 addressed the paucity of parallel data with data 004 augmentation techniques. Such methods rely 005 heavily on external resources making systems 006 difficult to train and scale effectively for multi-007 ple languages. We present a simple yet highly 008 effective two-stage back-translation training 009 scheme for adapting multilingual models to 010 the task of code-mixed translation which elim-011 inates the dependence on external resources. 012 We show substantial improvement in transla-013 tion quality (measured through BLEU), beat-014 ing existing prior work by up to +3.8 BLEU 015 on code-mixed Hi → En, Mr → En, and Bn → En 016 tasks. On the LinCE Machine Translation 017 leader board, we achieve the highest score for 018 code-mixed Es → En, beating existing best base-019 line by +6.5 BLEU, and our own stronger base-020 line by +1.1 BLEU. 021