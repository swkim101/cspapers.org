Rematerialization and ofﬂoading are two well known strategies to save memory during the training phase of deep neural networks, allowing data scientists to consider larger models, batch sizes or higher resolution data. Rematerialization trades memory for computation time, whereas Ofﬂoading trades memory for data movements. As these two resources are independent, it is appealing to consider the simultaneous combination of both strategies to save even more memory. We precisely model the costs and constraints corresponding to Deep Learning frameworks such as PyTorch or Tensorﬂow, we propose optimal algorithms to ﬁnd a valid sequence of memory-constrained operations and ﬁnally, we evaluate the performance of proposed algorithms on realistic networks and computation platforms. Our experiments show that the possibility to ofﬂoad can remove one third of the overhead of rematerialization, and that together they can reduce the memory used for activations by a factor 4 to 6, with an overhead below 20%.