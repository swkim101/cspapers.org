Using algorithms to compose creative and pleasing music has been an ambitious research goal since the 1950s. This trend continues to this day with the help of widely accessible, highly sophisticated music research tools created by big companies, such as Googleâ€™s Magenta. Due to the sequential nature of musical pieces, Recurrent Neural Networks (RNNs), as well as advanced variants such as Long-Short Term Memory networks (LSTMs), have been successfully employed for this purpose. Music scores data is made up of features like duration, pitch, rhythm, chords, etc. As more music features are integrated into the composition process, the space of encodings required to represent possible feature combinations grows significantly, making the process computationally infeasible. This consideration becomes of huge significance in situations with polyphonic pieces, where additional features such as harmonies and multiple voices are present. With an emphasis on efficiency without sacrificing quality, this research aims to further demonstrate the effectiveness of LSTMs for automated music generation by learning from existing music scores data. More specifically, we show that training separated models to learn individual music features and combining results to generate new music is, overall, superior to the common practice of learning resource-intensive complex models that simultaneously incorporate multiple desired features.