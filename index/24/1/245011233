This paper discusses model estimation in ofﬂine model-based reinforcement learning (MBRL), which is important for subsequent policy improvement using an estimated model. From the viewpoint of covariate shift, a natural idea is model estimation weighted by the ratio of the state-action distributions of ofﬂine data and real future data. However, estimating such a natural weight is one of the main challenges for off-policy evaluation, which is not easy to use. As an artiﬁcial alternative, this paper considers weighting with the state-action distribution ratio of ofﬂine data and simulated future data, which can be estimated relatively easily by standard density ratio estimation techniques for supervised learning. Based on the artiﬁcial weight, this paper deﬁnes a loss function for ofﬂine MBRL and presents an algorithm to optimize it. Weighting with the artiﬁcial weight is justiﬁed as evaluating an upper bound of the policy evaluation error. Numerical experiments demonstrate the effectiveness of weighting with the artiﬁcial weight.