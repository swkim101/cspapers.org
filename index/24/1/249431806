Ofﬂine reinforcement learning (RL) enables effective learning from previously collected data without exploration, which shows great promise in real-world applications when exploration is expensive or even infeasible. The discount factor, γ , plays a vital role in improving online RL sample efﬁciency and estimation accuracy, but the role of the discount factor in ofﬂine RL is not well explored. This paper examines two distinct effects of γ in ofﬂine RL with theoretical analysis, namely the regularization effect and the pessimism effect. On the one hand, γ is a regulator to trade-off optimality with sample efﬁciency upon existing ofﬂine techniques. On the other hand, lower guidance γ can also be seen as a way of pessimism where we optimize the policy’s performance in the worst possible models. We empirically verify the above theoretical observation with tabular MDPs and standard D4RL tasks. The results show that the discount factor plays an essential role in the performance of ofﬂine RL algorithms, both under small data regimes upon existing ofﬂine methods and in large data regimes without other conservative methods.