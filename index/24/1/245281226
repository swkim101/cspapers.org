We present a new neural network architecture, called NaturalNet, which uses a simplified biological neuron model and consists of a set of nonlinear ordinary differential equations. We model the membrane potential of each neuron by integrating the in-flowing currents, but we do not consider ion channels, nor individual spikes. To keep the membrane potential within a defined value range, we introduce a suitable clipping mechanism. With our approach, we aim to develop agents solving complex tasks by providing a higher biological plausibility than commonly used neural networks for deep learning applications, while also offering low computational complexity to enable fast training. To demonstrate the learning capabilities of NaturalNets, we use the virtual robotic environments of the OpenAI Gym framework, a widely-used toolkit for developing and comparing reinforcement learning algorithms. We compared a variety of different widespread neural network architectures, including long short-term memory (LSTM), gated recurrent units (GRUs), feedforward, and Elman networks. Our experiments show that NaturalNets were able to perform well for all considered virtual robotic control tasks, where we apply the covariance matrix adaptation evolutionary strategy (CMAES) for training.