In Vision-and-Language Navigation (VLN) task, an agent is asked to navigate inside 3D indoor environments following given instructions. Cross-modal alignment is one of the most critical challenges in VLN because the predicted trajectory needs to match the given instruction accurately. In this paper, we address the cross-modal alignment challenge from the perspective of ﬁne-grain. Firstly, to alleviate weak cross-modal alignment supervision from coarse-grained data, we introduce a human-annotated ﬁne-grained VLN dataset, namely Landmark-RxR. Secondly, to further enhance local cross-modal alignment under ﬁne-grained supervision, we investigate the focal-oriented rewards with soft and hard forms, by focusing on the critical points sampled from ﬁne-grained Landmark-RxR. More-over, to fully evaluate the navigation process, we also propose a re-initialization mechanism that makes metrics insensitive to difﬁcult points, which can cause the agent to deviate from the correct trajectories. Experimental results show that our agent has superior navigation performance on Landmark-RxR,