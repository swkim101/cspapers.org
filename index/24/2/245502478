Multilingual neural machine translation (MNMT) aims to translate multiple languages with a single model and has been proved successful thanks to effective knowledge transfer among different languages with shared parameters. However, it is still an open question which parameters should be shared and which ones need to be task-specific. Currently, the common practice is to heuristically design or search language-specific modules, which is difficult to find the optimal configuration. In this paper, we propose a novel parameter differentiation based method that allows the model to determine which parameters should be language-speciﬁc during training. Inspired by cellular differentiation, each shared parameter in our method can dynamically differentiate into more specialized types. We further deﬁne the differentiation criterion as inter-task gradient similarity. Therefore, parameters with conﬂicting inter-task gradients are more likely to be language-specific. Extensive experiments on multilingual datasets have demonstrated that our method signiﬁcantly outperforms various strong baselines with different parameter sharing conﬁgurations. Further analysis reveals that the parameter sharing configuration obtained by our method correlates well with the linguistic proximities.