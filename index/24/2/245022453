The advancement of deep neural networks over the last decade has enabled progress in scientiﬁc knowledge discovery in the form of learning Partial Differential Equations (PDEs) directly from experiment data. Nevertheless, forward simulation and backward learning of large-scale dynamic systems requires handling billions mutually interacting elements, the scale of which overwhelms current computing architectures. We propose Locality Sensitive Hashing Accelerated Simulation and Learning (LSH-S MI L E ), a uniﬁed framework to scale up both forward simulation and backward learning of physics systems. LSH-S MI L E takes advantages of (i) the locality of PDE updates, (ii) similar temporal dynamics shared by multiple elements. LSH-S MI L E hashes elements with similar dynamics into a single hash bucket and handles their updates at once. This allows LSH-S MI L E to scale with respect to the number of non-empty hash buckets, a drastic improvement over conventional approaches. Theoretically, we prove a novel bound on the errors introduced by LSH-S MI L E . Experimentally, we demonstrate that LSH-S MI L E simulates physics systems at comparable quality with exact approaches, but with way less time and space complexity. Such savings also translate to better learning performance due to LSH-S MI L E ’s ability to propagate gradients over a long duration.