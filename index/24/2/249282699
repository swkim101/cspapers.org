Neural architecture search (NAS) and network pruning are widely studied efficient AI techniques, but not yet perfect.

NAS performs exhaustive candidate architecture search, incurring tremendous search cost.

Though (structured) pruning can simply shrink model dimension, it remains unclear how to decide the per-layer sparsity automatically and optimally.

In this work, we revisit the problem of layer-width optimization and propose Pruning-as-Search (PaS), an end-to-end channel pruning method to search out desired sub-network automatically and efficiently.

Specifically, we add a depth-wise binary convolution to learn pruning policies directly through gradient descent.

By combining the structural reparameterization and PaS, we successfully searched out a new family of VGG-like and lightweight networks, which enable the flexibility of arbitrary width with respect to each layer instead of each stage.

Experimental results show that our proposed architecture outperforms prior arts by around 1.0% top-1 accuracy under similar inference speed on ImageNet-1000 classification task.

Furthermore, we demonstrate the effectiveness of our width search on complex tasks including instance segmentation and image translation.

Code and models are released.