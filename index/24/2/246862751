Fabricated media from deep learning models, or deepfakes , have been recently applied to facilitate social engineering efforts by constructing a trusted social persona. While existing works are primarily focused on deepfake detection, little is done to understand how users perceive and interact with deep-fake persona (e.g., proﬁles) in a social engineering context. In this paper, we conduct a user study ( n = 286) to quantitatively evaluate how deepfake artifacts affect the perceived trustworthiness of a social media proﬁle and the proﬁle’s likelihood to connect with users. Our study investigates artifacts isolated within a single media ﬁeld (images or text) as well as mismatched relations between multiple ﬁelds. We also evaluate whether user prompting (or training) beneﬁts users in this process. We ﬁnd that artifacts and prompting significantly decrease the trustworthiness and request acceptance of deepfake proﬁles. Even so, users still appear vulnerable with 43% of them connecting to a deepfake proﬁle under the best-case conditions. Through qualitative data, we ﬁnd numerous reasons why this task is challenging for users, such as the difﬁculty of distinguishing text artifacts from honest mistakes and the social pressures entailed in the connection decisions. We conclude by discussing the implications of our results for content moderators, social media platforms, and future defenses.