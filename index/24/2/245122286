The Conﬁgurable Markov Decision Process framework includes two entities: a Reinforcement Learning agent and a conﬁgurator that can modify some environmental parameters to improve the agent’s performance. This presupposes that the two actors have identical reward functions. What if the conﬁgurator does not have the same intentions as the agent? This paper introduces the Non-Cooperative Conﬁgurable Markov Decision Process, a framework that allows modeling two (possibly different) reward functions for the conﬁgurator and the agent. Then, we consider an online learning problem, where the conﬁgurator has to ﬁnd the best among a ﬁnite set of possible conﬁgurations. We propose two learning algorithms to minimize the conﬁgurator’s expected regret, which exploit the problem’s structure, depending on the agent’s feedback. While a naïve application of the UCB algorithm yields a regret that grows indeﬁnitely over time, we show that our approach suffers only bounded regret. Furthermore, we empirically validate the performance of our algorithm in simulated domains.