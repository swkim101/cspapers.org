Explaining the results of data-intensive computation via provenance has been extensively studied in the literature. We focus here on explaining the output of Machine Learning Classifiers, which are main components of many contemporary Data Science applications. We have developed a simple generic approach for explaining classification results, by looking for constrained perturbations to parts of the input that would have the most significant effect on the classification. Our solution requires white-box access to the model internals and a specification of constraints that define which perturbations are "reasonable" in the application domain; both are typically available to the data scientist. We propose to demonstrate CEC, a system prototype that is based on these foundations to provide generic explanations for Neural Networks and Random Forests. We will demonstrate the system usefulness in the context of two application domains: bank marketing campaigns, and visually clear explanations for image classifications. We will highlight the benefit that such explanations could yield to the data scientist and interactively engage the audience in computing and viewing explanations for different cases and different sets of constraints.