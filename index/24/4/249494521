Decision-Focused Learning (DFL) is a paradigm for tailoring a predictive model to a downstream optimization task that uses its predictions in order to perform better on that speciﬁc task . The main technical challenge associated with DFL is that it requires being able to differentiate through the optimization problem, which is difﬁcult due to discontinuous solutions and other challenges. Past work has largely gotten around this this issue by handcrafting task-speciﬁc surrogates to the original optimization problem that provide informative gradients when differentiated through. However, the need to handcraft surrogates for each new task limits the usability of DFL. In addition, there are often no guarantees about the convexity of the resulting surrogates and, as a result, training a predictive model using them can lead to inferior local optima. In this paper, we do away with surrogates altogether and instead learn loss functions that capture task-speciﬁc information. To the best of our knowledge, ours is the ﬁrst approach that entirely replaces the optimization component of decision-focused learning with a loss that is automatically learned. Our approach (a) only requires access to a black-box oracle that can solve the optimization problem and is thus generalizable , and (b) can be convex by construction and so can be easily optimized over. We evaluate our approach on three resource allocation problems from the literature and ﬁnd that our approach outperforms learning without taking into account task-structure in all three domains, and even hand-crafted surrogates from the literature.