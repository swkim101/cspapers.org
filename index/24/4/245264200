In this paper we present a novel approach to accomplishing soft robot configuration estimation and control using RGB-D cameras and SLAM-based methods. By placing cameras on the unactuated sections of our large-scale (approximately 2 meters long) pneumatic soft robot, we can map an environment and then estimate the orientation of the robot links using landmark-based localization. Using the orientations of each camera we can solve for the joint configurations between them. We first show that this method works for a traditional rigid robot (Baxter) where we can compare against the ground truth encoder values. For Baxter, the median joint angle error was on the order of 1-2◦. We then show that the SLAM-based method provides estimates for soft robot configuration that are within 1◦ when compared to our past methods of using a HTC Vive Tracker. While HTC Vive Trackers and commonly used motion capture systems require externally mounted sensors placed in the robot’s environment, the SLAM-based estimation method presented here works in any visually feature-rich environment. Finally we show that this method of estimation is effective for closed-loop control of soft robots by controlling our large-scale soft robot through a series of joint configurations.