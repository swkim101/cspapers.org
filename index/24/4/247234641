We prove analogues of the popular bounded difference inequality (also called McDiarmidâ€™s inequality) for functions of independent random variables under subGaussian and sub-exponential conditions. Applied to vector-valued concentration and the method of Rademacher complexities these inequalities allow an easy extension of uniform convergence results for PCA and linear regression to the case of potentially unbounded inputand output variables.