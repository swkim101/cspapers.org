Previous neural seq2seq models have shown the e ﬀ ectiveness for jointly extracting relation triplets. However, most of these models suf-fer from incompletion and disorder problems when they extract multi-token entities from input sentences. To tackle these problems, we propose a generative, multi-task learning framework, named GenerativeRE. We ﬁrstly propose a special entity labelling method on both input and output sequences. During the training stage, GenerativeRE ﬁne-tunes the pretrained generative model and learns the special entity labels simultaneously. During the inference stage, we propose a novel copy mechanism equipped with three mask strategies, to generate the most probable tokens by diminishing the scope of the model decoder. Experimental results show that our model achieves 4.6% and 0.9% F1 score im-provements over the current state-of-the-art methods in the NYT24 and NYT29 benchmark datasets respectively.