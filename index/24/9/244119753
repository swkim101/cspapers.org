Domain-speciﬁc pre-trained language models (PLMs) have achieved great success over various downstream tasks in different domains. However, existing domain-speciﬁc PLMs mostly rely on self-supervised learning over large amounts of domain text, without explicitly integrating domain-speciﬁc knowledge, which can be essential in many domains. Moreover, in knowledge-sensitive ar-eas such as the biomedical domain, knowledge is stored in multiple sources and formats, and existing biomedical PLMs either neglect them or utilize them in a limited manner. In this work, we introduce an architecture to integrate domain knowledge from diverse sources into PLMs in a parameter-efﬁcient way. More speciﬁcally, we propose to encode domain knowledge via adapters , which are small bottleneck feed-forward networks inserted between intermediate transformer layers in PLMs. These knowledge adapters are pre-trained for individual domain knowledge sources and integrated via an attention-based knowledge controller to enrich PLMs. Taking the biomedical domain as a case study, we explore three knowledge-speciﬁc adapters for PLMs based on the UMLS Metathesaurus graph, the Wikipedia articles for diseases, and the semantic grouping information for biomedical concepts. Extensive experiments on different biomedical NLP tasks and datasets demonstrate the beneﬁts of the proposed architecture and the knowledge-speciﬁc adapters across multiple PLMs.