Short text nowadays has become a more fashionable form of text data, e.g., Twitter posts, news titles, and product reviews. Extracting semantic topics from short texts plays a signiﬁcant role in a wide spectrum of NLP applications, and neural topic modeling is now a major tool to achieve it. Motivated by learning more coherent and semantic topics, in this paper we develop a novel neural topic model named Dual Word Graph Topic Model (DWG TM ), which extracts topics from simultaneous word co-occurrence and semantic correlation graphs. To be speciﬁc, we learn word features from the global word co-occurrence graph, so as to ingest rich word co-occurrence information; we then generate text features with word features, and feed them into an encoder network to get topic proportions per-text; ﬁnally, we reconstruct texts and word co-occurrence graph with topical distri-butions and word features, respectively. Besides, to capture semantics of words, we also apply word features to reconstruct a word semantic correlation graph computed by pre-trained word embeddings. Upon those ideas, we formulate DWG TM in an auto-encoding paradigm and efﬁciently train it with the spirit of neural variational inference. Empirical results validate that DWG TM can generate more semantically coherent topics than baseline topic models.