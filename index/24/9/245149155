Given an unsupervised outlier detection task on a new dataset, how can we automatically select a good outlier detection algorithm and its hyperparameter(s) (collectively called a model)? In this work, we tackle the unsupervised outlier model selection (UOMS) problem, and propose M ETA OD, a principled, data-driven approach to UOMS based on meta-learning. The UOMS problem is notoriously challenging, as compared to model selection for classiﬁcation and clustering, since ( i ) model evaluation is infeasible due to the lack of hold-out data with labels, and ( ii ) model comparison is infeasible due to the lack of a universal objective function. M ETA OD capitalizes on the performances of a large body of detection models on historical outlier detection benchmark datasets, and carries over this prior experience to automatically select an effective model to be employed on a new dataset without any labels, model evaluations or model comparisons . To capture task similarity within our meta-learning framework, we introduce specialized meta-features that quantify outlying characteristics of a dataset. Extensive experiments show that selecting a model by M ETA OD signiﬁcantly outperforms no model selection (e.g. always using the same popular model or the ensemble of many) as well as other meta-learning techniques that we tailored for UOMS. Moreover upon (meta-)training, M ETA OD is extremely efﬁcient at test time; selecting from a large pool of 300+ models takes less than 1 second for a new task. We open-source 1 M ETA OD and our meta-learning database for practical use and to foster further research on the UOMS problem.