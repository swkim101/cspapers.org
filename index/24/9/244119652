We present a new form of ensemble method– Devil’s Advocate, which uses a deliberately dissenting model to force other submodels within the ensemble to better collaborate. Our method consists of two di ﬀ erent training settings: one follows the conventional training process (Norm), and the other is trained by artiﬁcially generated labels (DevAdv). After training the models, Norm models are ﬁne-tuned through an additional loss function, which uses the DevAdv model as a constraint. In making a ﬁnal decision, the proposed ensemble model sums the scores of Norm models and then subtracts the score of the DevAdv model. The DevAdv model improves the overall performance of the other models within the ensemble. In addition to our ensemble framework being based on psychological background, it also shows comparable or improved performance on 5 text classiﬁcation tasks when compared to conventional ensemble methods.