We tackle the problem of self-training networks for NLU in low-resource environmentâ€” few labeled data and lots of unlabeled data. The effectiveness of self-training is a result of increasing the amount of training data while training. Yet it becomes less effective in low-resource settings due to unreliable labels pre-dicted by the teacher model on unlabeled data. Rules of grammar, which describe the gram-matical structure of data, have been used in NLU for better explainability. We propose to use rules of grammar in self-training as a more reliable pseudo-labeling mechanism, especially when there are few labeled data. We design an effective algorithm that constructs and expands rules of grammar without human involvement. Then we integrate the con-structed rules as a pseudo-labeling mechanism into self-training. There are two possible scenarios regarding data distribution: it is unknown or known in prior to training. We empir-ically demonstrate that our approach substan-tially outperforms the state-of-the-art methods in three benchmark datasets for both scenarios.