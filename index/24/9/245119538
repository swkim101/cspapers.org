In the paper, we propose a class of efﬁcient mirror descent ascent methods to solve the nonsmooth nonconvex-strongly-concave minimax problems by using dynamic mirror functions, and introduce a convergence analysis framework to conduct rigorous theoretical analysis for our mirror descent ascent methods. For our stochastic algorithms, we ﬁrst prove that the mini-batch stochastic mirror descent ascent (SMDA) method obtains a sample complexity of O ( κ 3 (cid:15) − 4 ) for ﬁnding an (cid:15) -stationary point, where κ denotes the condition number. Further, we propose an accelerated stochastic mirror descent ascent (VR-SMDA) method based on the variance reduced technique. We prove that our VR-SMDA method achieves a lower sample complexity G O ( κ 3 (cid:15) − 3 ) . For our deterministic algorithm, we prove that our deterministic mirror descent ascent (MDA) achieves a lower sample complexity of O ( κ(cid:15) − 2 ) under mild conditions, which improves the best known complexity by a factor of O ( √ κ ) . We conduct the experiments on fair classiﬁer and robust neural network training tasks to demonstrate the efﬁciency of our new algorithms.