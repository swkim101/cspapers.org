In this paper, we propose a simple few-shot domain adaptation paradigm for reading comprehension. We ﬁrst identify the lottery subnet-work structure within the Transformer-based source domain model via gradual magnitude pruning. Then, we only ﬁne-tune the lottery subnetwork, a small fraction of the whole parameters, on the annotated target domain data for adaptation. To obtain more adaptable sub-networks, we introduce self-attention attribution to weigh parameters, beyond simply pruning the smallest magnitude parameters, which can be seen as combining structured pruning and unstructured magnitude pruning softly. Experimental results show that our method outperforms the full model ﬁne-tuning adaptation on four out of ﬁve domains when only a small amount of annotated data available for adaptation. Moreover, introducing self-attention attribution reserves more parameters for important attention heads in the lottery subnetwork and improves the target domain model performance. Our further analyses reveal that, besides exploiting fewer parameters, the choice of subnetworks is critical to the effectiveness. 1