Recent multilingual pre-trained models, like XLM-RoBERTa (XLM-R), have been demon-strated effective in many cross-lingual tasks. However, there are still gaps between the contextualized representations of similar words in different languages. To solve this problem, we propose a novel framework named M ulti- V iew M ixed L anguage T raining (MVMLT), which leverages code-switched data with multi-view learning to ﬁne-tune XLM-R. MVMLT uses gradient-based saliency to extract keywords which are the most relevant to downstream tasks and replaces them with the corresponding words in the target language dynamically. Furthermore, MVMLT utilizes multi-view learning to encourage contextualized embeddings to align into a more reﬁned language-invariant space. Extensive experiments with four languages show that our model achieves state-of-the-art results on zero-shot cross-lingual sentiment classiﬁcation and dialogue state tracking tasks, demonstrating the effectiveness of our proposed model 1 .