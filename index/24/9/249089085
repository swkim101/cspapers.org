Stateful layer-4 load balancers (LB) are deployed at datacenter boundaries to distribute Internet trafﬁc to backend real servers. To steer terabits per second trafﬁc, traditional software LBs scale out with many expensive servers. Recent switch-accelerated LBs scale up efﬁciently, but fail to ofﬂoad a massive number of concurrent ﬂows into limited on-chip SRAMs. This paper presents Tiara, a hardware architecture for stateful layer-4 LBs that aims to support a high trafﬁc rate (> 1 Tbps), a large number of concurrent ﬂows (> 10M), and many new connections per second (> 1M), without any assumption on trafﬁc patterns. The three-tier architecture of Tiara makes the best use of heterogeneous hardware for stateful LBs, including a programmable switch and FPGAs for the fast path and x86 servers for the slow path. The core idea of Tiara is to divide the LB fast path into a memory-intensive task ( real server selection ) and a throughput-intensive task ( packet encap/decap ), and map them into the most suitable hardware, respectively (i.e., map real server selection into FPGA with large high-bandwidth memory (HBM) and packet encap/decap into a high-throughput programmable switch). We have implemented a fully functional Tiara prototype, and experiments show that Tiara can achieve extremely high performance (1.6 Tbps throughput, 80M concurrent ﬂows, 1.8M new connections per second, and less than 4 us latency in the fast path) in