Training implicit discourse relation classiﬁers suffers from data sparsity. Variational AutoEncoder (VAE) appears to be the proper solution. It is because that VAE is able to automatically generate inexhaustible varying samples by self supervision, and facilitates data augmentation. However, our experiments show that the uti-lization of VAE results in severe performance degradation. We ascribe this phenomenon to erroneous sampling. To address the issue, we use Conditional VAE (CVAE) to estimate the risk of erroneous sampling. Moreover, we develop a re-anchoring method which migrates the anchor of sampling area of VAE to re-duce the risk. The experiments on PDTB v2.0 demonstrate that, compared to the RoBERTa-based baseline, re-anchoring yields substantial improvements. In addition, we prove that re-anchoring can cooperate with other auxiliary strategies (transfer learning and interactive attention mechanism) to further improve the classiﬁcation performance. community by re-anchoring, transfer learning and interactive attention. F1-score (%) and Acc (%) are