The task of verifying the truthfulness of claims in textual documents, or fact-checking, has received signiﬁcant attention in recent years. Many existing evidence-based fact-checking datasets contain synthetic claims and the models trained on these data might not be able to verify real-world claims. Partic-ularly few studies addressed evidence-based fact-checking of health-related claims that re-quire medical expertise or evidence from the scientiﬁc literature. In this paper, we introduce H EALTH V ER , a new dataset for evidence-based fact-checking of health-related claims that allows to study the validity of real-world claims by evaluating their truthfulness against scientiﬁc articles. Using a three-step data creation method, we ﬁrst retrieved real-world claims from snippets returned by a search engine for questions about COVID-19. Then we automatically retrieved and re-ranked relevant scientiﬁc papers using a T5 relevance-based model. Finally, the relations between each evidence statement and the associated claim were manually annotated as S UPPORT , R EFUTE and N EUTRAL . To validate the created dataset of 14,330 evidence-claim pairs, we developed baseline models based on pretrained language models. Our experiments showed that training deep learning models on real-world medical claims greatly improves performance compared to models trained on synthetic and open-domain claims. Our results and manual analysis suggest that H EALTH V ER provides a realistic and challenging dataset for future efforts on evidence-based fact-checking of health-related claims. The dataset, source code, and