Image captioning systems are expected to have the ability to combine individual concepts when describing scenes with concept combinations that are not observed during training. In spite of signiÔ¨Åcant progress in image captioning with the help of the autoregressive generation framework, current approaches fail to generalize well to novel concept combinations. We propose a new framework that re-volves around probing several similar image caption training instances (retrieval), perform-ing analogical reasoning over relevant entities in retrieved prototypes (analogy), and en-hancing the generation process with reasoning outcomes (composition). Our method aug-ments the generation model by referring to the neighboring instances in the training set to produce novel concept combinations in generated captions. We perform experiments on the widely used image captioning benchmarks. The proposed models achieve substantial improvement over the compared baselines on both composition related evaluation metrics and conventional image captioning metrics.