Non-autoregressive machine translation (NAT) approaches enable fast generation by utilizing parallelizable generative processes. The re-maining bottleneck in these models is their decoder layers; unfortunately unlike in autoregressive models (Kasai et al., 2020), remov-ing decoder layers from NAT models signif-icantly degrades accuracy. This work proposes a sequence-to-lattice model that replaces the decoder with a search lattice. Our approach ﬁrst constructs a candidate lattice using efﬁcient lookup operations, generates lattice scores from a deep encoder, and ﬁnally ﬁnds the best path using dynamic program-ming. Experiments on three machine translation datasets show that our method is faster than past non-autoregressive generation approaches, and more accurate than naively reducing the number of decoder layers.