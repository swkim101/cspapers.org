Truncated linear regression is a classical challenge in statistics, wherein a label, 𝑦 = 𝑤 𝑇 𝑥 + 𝜀 , and its corresponding feature vector, 𝑥 ∈ R 𝑘 , are only observed if the label falls in some subset 𝑆 ⊆ R ; otherwise the existence of the pair ( 𝑥, 𝑦 ) is hidden from observation. Linear regression with truncated observations has remained a challenge, in its general form, since the early works of Tobin (1958); Amemiya (1973). When the distribution of the error is normal with known variance, recent work of Daskalakis et al. (2019) provides computationally and statistically efﬁcient estimators of the linear model, 𝑤 . In this paper, we provide the ﬁrst computationally and statistically efﬁcient estimators for truncated linear regression when the noise variance is unknown, estimating both the linear model and the variance of the noise. Our estimator is based on an efﬁcient implementation of Projected Stochastic Gradient Descent on the negative log-likelihood of the truncated sample. Importantly, we show that the error of our estimates is asymptotically normal, and we use this to provide explicit conﬁdence regions for our estimates.