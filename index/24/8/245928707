The Neural Tangent Kernel (NTK), deﬁned as Θ fθ ( x 1 , x 2 ) = (cid:2) ∂f ( θ, x 1 ) (cid:14) ∂θ (cid:3) (cid:2) ∂f ( θ, x 2 ) (cid:14) ∂θ (cid:3) T where (cid:2) ∂f ( θ, · ) (cid:14) ∂θ (cid:3) is a neural network (NN) Jacobian, has emerged as a central object of study in deep learning. In the inﬁnite width limit, the NTK can sometimes be computed analytically and is useful for understanding training and generalization of NN architectures. At ﬁnite widths, the NTK is also used to better initialize NNs, compare the conditioning across models, perform architecture search, and do meta-learning. Unfortunately, the ﬁnite width NTK is notoriously expensive to compute, which severely limits its practical utility. We perform the ﬁrst in-depth analysis of the compute and memory requirements for NTK computation in ﬁnite width networks. Leveraging the structure of neural networks, we further propose two novel algorithms that change the ex-ponent of the compute and memory requirements of the ﬁnite width NTK, dramatically improving efﬁciency. Our algorithms can be applied in a black box fashion to any differentiable function, including those implementing neural networks. We open-source our implementations within the Neural Tangents package (Novak et al., 2020) at github.com/google/neural-tangents.