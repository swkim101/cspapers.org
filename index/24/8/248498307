A number of recent studies of continuous variational autoencoder (VAE) models have noted, either directly or indirectly, the tendency of various parameter gradients to drift towards inﬁnity during training. Because such gradients could potentially contribute to numerical instabilities, and are often framed as a problematic phenomena to be avoided, it may be tempting to shift to alternative energy functions that guarantee bounded gradients. But it remains an open question: What might the unintended consequences of such a restriction be? To address this issue, we examine how unbounded gradients relate to the regularization of a broad class of autoencoder-based architectures, including VAE models, as applied to data lying on or near a low-dimensional manifold (e.g., natural images). Our main ﬁnding is that, if the ultimate goal is to simultaneously avoid over-regularization (high reconstruction errors, sometimes referred to as posterior collapse) and under-regularization (excessive latent dimensions are not pruned from the model), then an autoencoder-based energy function with inﬁnite gradients around optimal representations is provably required per a certain technical sense which we carefully detail. Given that both over-and under-regularization can directly lead to poor generated sample quality or suboptimal feature selection, this result suggests that heuristic modiﬁcations to or constraints on the VAE energy function may at times be ill-advised, and large gradients should be accommodated to the extent possible.