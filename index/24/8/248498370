Explaining the inﬂuence of training data on machine learning model predictions is a critical tool for debugging models through data curation. A recent appealing and efﬁcient approach for this task was provided via the concept of Representer Point Selection (RPS), i.e. a method the leverages the dual form of l 2 regularized optimization in the last layer of the neural network to identify the contribution of training points to the prediction. However, two key drawbacks of RPS-l 2 are that they (i) lead to disagreement between the originally trained network and the RPS-l 2 regularized network modiﬁcation and (ii) often yield a static ranking of training data for test points in the same class, independent of the test point being classiﬁed. Inspired by the RPS-l 2 approach, we propose an alternative method based on a local Jacobian Taylor expansion (LJE). We empirically compared RPS-LJE with the original RPS-l 2 on image classiﬁcation (with ResNet), text classiﬁcation recurrent neural networks (with Bi-LSTM), and tabular classiﬁcation (with XGBoost) tasks. Quantitatively, we show that RPS-LJE slightly outperforms RPS-l 2 and other state-of-the-art data explanation methods by up to 3% on a data debugging task. More critically, we qualitatively observe that RPS-LJE provides stable and individualized explanations that are more coherent to each test data point. Overall, RPS-LJE represents a novel approach to RPS-l 2 that provides a powerful tool for sample-based model explanation and debugging.