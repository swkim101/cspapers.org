Constructing Reinforcement Learning (RL) policies that adhere to safety requirements is an emerging ﬁeld of study. RL agents learn via trial and error with an objective to optimize a reward signal. Often policies that are designed to accumulate rewards do not satisfy safety speciﬁcations. We present a methodology for counterexample guided reﬁnement of a trained RL policy against a given safety speciﬁcation. Our approach has two main components. The ﬁrst component is an approach to discover failure trajectories using Bayesian Optimization over multiple parameters of uncertainty from a policy learnt in a model-free setting. The second component selectively modiﬁes the failure points of the policy using gradient-based updates. The approach has been tested on several RL environments, and we demonstrate that the policy can be made to respect the safety speciﬁcations through such targeted changes.