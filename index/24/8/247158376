We study online learning problems in which a decision maker wants to maximize their expected reward without violating a ﬁnite set of m resource constraints. By casting the learning process over a suitably deﬁned space of strategy mixtures , we recover strong duality on a Lagrangian relaxation of the underlying optimization problem, even for general settings with non-convex reward and resource-consumption functions. Then, we provide the ﬁrst best-of-both-worlds type framework for this setting, with no-regret guarantees both under stochastic and adversarial inputs. Our framework yields the same regret guarantees of prior work in the stochastic case. On the other hand, when budgets grow at least linearly in the time horizon, it allows us to provide a constant competitive ratio in the adversarial case, which improves over the O ( m log T ) competitive ratio of (Immor-lica et al., 2019). Moreover, our framework allows the decision maker to handle non-convex reward and cost functions. We provide two game-theoretic applications of our framework to give further evidence of its ﬂexibility.