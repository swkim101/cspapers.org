In this paper, we introduce spatiotemporal joint ﬁlter decomposition to decouple spatial and temporal learning, while preserving spatiotemporal dependency in a video. A 3D convolutional ﬁlter is now jointly decomposed over a set of spatial and temporal ﬁlter atoms respectively. In this way, a 3D convolutional layer becomes three: a temporal atom layer, a spatial atom layer, and a joint coefﬁcient layer, all three remaining convolutional. One obvious arithmetic manipulation allowed in our joint decomposition is to swap spatial or temporal atoms with a set of atoms that have the same number but different sizes, while keeping the remaining unchanged. For example, as shown later, we can now achieve tempo-invariance by simply dilating temporal atoms only. To illustrate this useful atom-swapping property, we further demonstrate how such a decomposition permits the direct learning of 3D CNNs with full-size videos through iterations of two consecutive sub-stages of learning: In the temporal stage, full-temporal downsampled-spatial data are used to learn temporal atoms and joint coefﬁcients while ﬁxing spatial atoms. In the spatial stage, full-spatial downsampled-temporal data are used for spatial atoms and joint coefﬁcients while ﬁxing temporal atoms. We show empirically on multiple action recognition datasets that, the decoupled spatiotemporal learning signiﬁcantly reduces the model memory footprints, and allows deep 3D CNNs to model high-spatial long-temporal dependency with limited computational resources while delivering comparable performance.