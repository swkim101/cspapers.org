Despite the tremendous success of reinforcement learning (RL) with function approximation, efﬁcient exploration remains a signiﬁcant challenge, both practically and theoretically. In particular, existing theoretically grounded RL algorithms based on upper conﬁdence bounds (UCBs), such as optimistic least-squares value iteration (LSVI), are often incompatible with practically powerful function ap-proximators, such as neural networks. In this paper, we develop a variant of bootstrapped LSVI, namely BooVI, which bridges such a gap between practice and theory. Practically, BooVI drives exploration through (re)sampling, making it compatible with general function approximators. Theoretically, BooVI inherits the worst-case (cid:101) O ( √ d 3 H 3 T ) -regret of optimistic LSVI in the episodic linear setting. Here d is the feature dimension, H is the episode horizon, and T is the total number of steps.