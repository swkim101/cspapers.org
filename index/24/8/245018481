In this paper, we comprehensively reveal the learning dynamics of normalized 1 neural network using Stochastic Gradient Descent (with momentum) and Weight 2 Decay (WD), named as Spherical Motion Dynamics (SMD). Most related works 3 focus on studying behavior of “effective learning rate" in “equilibrium" state, i.e. 4 assuming weight norm remains unchanged. However, their discussion on why this 5 equilibrium can be reached is either absent or less convincing. Our work directly 6 explores the cause of equilibrium, as a special state of SMD. Speciﬁcally, 1) we 7 introduce the assumptions that can lead to equilibrium state in SMD, and prove 8 equilibrium can be reached in a linear rate regime under given assumptions; 2) we 9 propose “angular update" as a substitute for effective learning rate to depict the state 10 of SMD, and derive the theoretical value of angular update in equilibrium state; 3) 11 we verify our assumptions and theoretical results on various large-scale computer 12 vision tasks including ImageNet and MSCOCO with standard settings. Experiment 13 results show our theoretical ﬁndings agree well with empirical observations. We 14 also show that the behavior of angular update in SMD can produce signiﬁcant 15 effect to the optimization of neural network in practice. 16