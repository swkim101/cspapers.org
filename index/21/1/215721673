With increasing traffic in urban areas, it is crucial to examine strategies to reduce traffic network congestion. Popular navigation policies currently tend to select the fastest path available for each vehicle. However, a top-down approach to navigation, which considers the traffic network as a whole, offers several speedup possibilities. Minimizing the average travel time of all vehicles in the network with respect to their separate travel deadlines improves traffic throughput. Because such a strategy does not guarantee an optimal navigation route for individual vehicles, we refer to it as a "selfless" policy and based on this observation we propose the Selfless Traffic Routing (STR) model. Hence, we propose a test bed based on Simulation of Urban MObility (SUMO) that can evaluate the performance of a traffic routing policy based on the average travel time of all vehicle agents in a given traffic grid. Continuously calculating optimal actions for multiple agents in real-time is computationally complex. We therefore introduce a value-based reinforcement learning strategy to achieve the benefits offered by a selfless traffic routing model. We explore how this approach can potentially achieve an optimal balance between action quality and the real-time performance of each decision.