Transfer learning has been widely studied in many real-world applications to help the model performance of data-deficient domains. However, the design choice, such as what, where and how to share, can greatly influence the model performance. To save the effort in the transfer learning model design, we propose a novel method to automatically fuse transferable network architecture. Extensive experiments on public datasets of semantic text matching tasks show that our proposed method has better performance than the state-of-the-art transfer learning architectures.