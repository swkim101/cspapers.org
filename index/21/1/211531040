Data ﬂow graphs are a popular program representation in machine learning, big data analytics, signal processing, and, increasingly, networking, where graph nodes correspond to processing primitives and graph edges describe control ﬂow. To improve CPU cache locality and exploit data-level parallelism, nodes usually process data in batches. Unfortunately, as batches are split across dozens or hundreds of parallel processing pathways through the graph they tend to fragment into many small chunks, leading to a loss of batch efﬁciency. We present Batchy, a scheduler for run-to-completion packet processing engines, which uses controlled queuing to efﬁciently reconstruct fragmented batches in accordance with strict service-level objectives (SLOs). Batchy comprises a runtime proﬁler to quantify batch-processing gain on different processing functions, an analytical model to ﬁne-tune queue backlogs, a new queuing abstraction to realize this model in run-to-completion execution, and a one-step receding horizon controller that adjusts backlogs across the pipeline. We present extensive experiments on ﬁve networking use cases taken from an ofﬁcial 5G benchmark suite to show that Batchy provides 2–3 × the performance of prior work while accurately satisfying delay SLOs.