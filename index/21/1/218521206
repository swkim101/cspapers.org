Time series forecasting is a key ingredient in the automation and optimization of business processes: in retail, deciding which products to order and where to store them depends on the forecasts of future demand in different regions; in cloud computing, the estimated future usage of services and infrastructure components guides capacity planning; and workforce scheduling in warehouses and factories requires forecasts of the future workload. Recent years have witnessed a paradigm shift in forecasting techniques and applications, from computer-assisted model- and assumption-based to data-driven and fully-automated. This shift can be attributed to the availability of large, rich, and diverse time series data sources and result in a set of challenges that need to be addressed such as the following. How can we build statistical models to efficiently and effectively learn to forecast from large and diverse data sources? How can we leverage the statistical power of “similar” time series to improve forecasts in the case of limited observations? What are the implications for building forecasting systems that can handle large data volumes? The objective of this tutorial is to provide a concise and intuitive overview of the most important methods and tools available for solving large-scale forecasting problems. We review the state of the art in both: (1) classical modeling of time series, (2) deep learning for forecasting. We also discuss the practical aspects of building a large scale forecasting system, including data integration, feature generation, backtesting framework, error tracking and analysis, etc. Accompanied with the practice side is a hands-on session, where we would engage the audience with Jupyter notebooks that demonstrates the key concepts in the theory part. Furthermore, we provides interactive demos, showing various avenues to solve business problems with AWS Forecasting offerings such as GluonTS, DeepAR (SageMaker), and Amazon Forecast.