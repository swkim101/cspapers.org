Machine Learning systems learn bias in addition to other patterns from input data on which they are trained. Bolukbasi et al. pioneered a method for quantifying gender bias learned from a corpus of text. Specifically, they compute a gender subspace into which words, represented as word vectors, can be placed and compared with one another. In this paper, we apply a similar methodology to a different type of bias, political bias. Unlike with gender bias, it is not obvious how to choose a set of definitional word pairs to compute a political bias subspace. We propose a methodology for doing so that could be used for modeling other types of bias as well. We collect and examine a 26 GB corpus of tweets from Republican and Democratic politicians in the United States (presidential candidates and members of Congress). With our definition of a political bias subspace, we observe several interesting and intuitive trends including that tweets from presidential candidates, both Republican and Democratic, show more political bias than tweets from other politicians of the same party. This work models political bias as a binary choice along one axis, as Bolukbasi et al. did for gender. However, most kinds of bias - political, racial and even gender bias itself - are much more complicated than two binary extremes along one axis. In this paper, we also discuss what might be required to model bias along multiple axes (e.g. liberal/conservative and authoritarian/libertarian for political bias) or as a range of points along a single axis (e.g. a gender spectrum).