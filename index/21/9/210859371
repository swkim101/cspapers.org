We study the problem of policy synthesis for uncertain partially observable Markov decision processes (uPOMDPs).

The transition probability function of uPOMDPs is only known to belong to a so-called uncertainty set, for instance in the form of probability intervals.

Such a model arises when, for example, an agent operates under information limitation due to imperfect knowledge about the accuracy of its sensors.

The goal is to compute a policy for the agent that is robust against all possible probability distributions within the uncertainty set.

In particular, we are interested in a policy that robustly ensures the satisfaction of temporal logic and expected reward specifications.

We state the underlying optimization problem as a semi-infinite quadratically-constrained quadratic program (QCQP), which has finitely many variables and infinitely many constraints.

Since QCQPs are non-convex in general and practically infeasible to solve, we resort to the so-called convex-concave procedure to convexify the QCQP.

Even though convex, the resulting optimization problem still has infinitely many constraints and is NP-hard.

For uncertainty sets that form convex polytopes, we provide a transformation of the problem to a convex QCQP with finitely many constraints. 

We demonstrate the feasibility of our approach by means of several case studies that highlight typical bottlenecks for our problem. 

In particular, we show that we are able to solve benchmarks with hundreds of thousands of states, hundreds of different observations, and we investigate the effect of different levels of uncertainty in the models.