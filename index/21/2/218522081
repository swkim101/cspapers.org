In this paper, we present a pre-trained transformer network Q-BERT, in which siamese network architecture is employed to produce semantically meaningful embeddings of SPARQL queries to be compared via cosine-similarity. A core idea of Q-BERT is to put SPARQL query into the category of natural language to ensure that each entity mention or relation phrase in different knowledge bases has the same vector representation. Moreover, based on Q-BERT, we present a practical approach for the SPARQL query-empty-answer problem by accessing the RDF repository. The experiments on real datasets show the effectiveness and efficiency of Q-BERT.