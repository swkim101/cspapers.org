The emergence of transformers has revolution-001 ized natural language processing (NLP), as ev-002 idenced in various NLP tasks. While graph 003 neural networks (GNNs) show recent promise 004 in NLP, they are not standalone replacements 005 for transformers. Rather, recent research ex-006 plores combining transformers and GNNs. Ex-007 isting GNN-based approaches rely on static 008 graph construction methods requiring excessive 009 text processing, and most of them are not scal-010 able with the increasing document and word 011 counts. We address these limitations by propos-012 ing a novel dynamic graph construction method 013 for text documents based on vector visibility 014 graphs (VVGs) generated from transformer 015 output. Then, we introduce visibility pooler 016 (VISPool), a scalable model architecture that 017 seamlessly integrates VVG convolutional net-018 works into transformer pipelines. We evaluate 019 the proposed model on the General Language 020 Understanding Evaluation (GLUE) benchmark 021 datasets. VISPool outperforms the baselines 022 with less trainable parameters, demonstrating 023 the viability of the visibility-based graph con-024 struction method for enhancing transformers 025 with GNNs. 1 026