Temporal Knowledge Graph (TKG) reasoning 001 seeks to predict future incomplete facts leverag-002 ing historical data. While existing approaches 003 have shown effectiveness in addressing the task 004 through various perspectives, such as graph 005 learning and logic rules, they are limited in 006 capturing the indeterminacy in future events, 007 particularly in the case of rare/unseen facts. To 008 tackle the highlighted issues, we introduce a 009 novel approach by conceptualizing TKG rea-010 soning as a sequence denoising process for fu-011 ture facts, namely DiffuTKG. Concretely, we 012 first encodes the historical events as the condi-013 tional sequence. Then we gradually introduce 014 Gaussian noise to corrupt target facts during the 015 forward process and then employ a transformer-016 based conditional denoiser to restore them in 017 the reverse phase. Moreover, we introduce an 018 uncertainty regularization loss to mitigate the 019 risk of prediction biases by favoring frequent 020 scenarios over rare/unseen facts. Empirical re-021 sults on four real-world datasets show that Dif-022 fuTKG outperforms state-of-the-art methods 023 across multiple evaluation metrics 1 . 024