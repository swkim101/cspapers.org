A Large Language Model (LLM) tends to gen-001 erate inconsistent and sometimes contradictory 002 outputs when presented with a prompt that has 003 equivalent semantics but is expressed differ-004 ently from the original prompt. To achieve 005 semantic consistency of an LLM, one of the 006 key approaches is to finetune the model with 007 prompt-output pairs with semantically equiv-008 alent meanings. Despite its effectiveness, a 009 data-driven finetuning method incurs substan-010 tial computation costs in data preparation and 011 model optimization. In this regime, an LLM 012 is treated as a “black box”, restricting our 013 ability to gain deeper insights into its inter-014 nal mechanism. In this paper, we are moti-015 vated to enhance the semantic consistency of 016 LLMs through a more interpretable method 017 (i.e., model editing) to this end. We first 018 identify the model components (i.e., attention 019 heads) that have a key impact on the semantic 020 consistency of an LLM. We subsequently inject 021 biases into the output of these model compo-022 nents along the semantic-consistency activation 023 direction. It is noteworthy that these modifi-024 cations are cost-effective, without reliance on 025 mass manipulations of the original model pa-026 rameters. Through comprehensive experiments 027 on the constructed NLU and open-source NLG 028 datasets, our method demonstrates significant 029 improvements in the semantic consistency and 030 task performance of LLMs. Additionally, our 031 method exhibits promising generalization capa-032 bilities by performing well on tasks beyond the 033 primary tasks.