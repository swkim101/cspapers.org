Among the recently emerged knowledge edit-001 ing methods, in-context knowledge editing 002 (IKE) (Zheng et al., 2023) has shown re-003 spectable abilities on knowledge editing in 004 terms of generalization and specificity. Not-005 ing the promising advantages but unexplored 006 issues of IKE, we propose DistillMIKE as a 007 novel extension of IKE, i.e., editing distill ation 008 of “ M assive” I n-context K nowledge E diting in 009 large language models (LLMs), mainly consist-010 ing of two expansions; 1) Massive in-context 011 knowledge editing (MIKE) , which extends IKE 012 to a massive editing task, aiming to inject not a 013 single edit but a set of massive edits to LLMs; 014 To preserve specificity, our key novel extension 015 is a “selective” retrieval augmentation, where 016 the retrieval-augmented IKE is only applied 017 to “in-scope” examples, whereas the unedited 018 model without IKE is employed for “out-of-019 scope” ones. 2) Editing distillation of MIKE 020 using low-rank adaptation (LoRA), which dis-021 tills editing abilities of MIKE to parameters of 022 LLMs in a manner of eliminating the need of 023 lengthy in-context demonstrations, thus remov-024 ing the computational overhead encountered 025 at the inference time. Experimental results on 026 the zsRE and CounterFact datasets demonstrate 027 that MIKE shows the state-of-the-art perfom-028 rances and DistilMIKE show comparable per-029 formances with MIKE. Our code is available at 030 https://github.com/xxxx/xxxx . 031