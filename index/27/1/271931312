Large Language Models (LLMs) have revolu-001 tionized various domains with extensive knowl-002 edge and creative capabilities. However, a crit-003 ical issue with LLMs is their tendency to pro-004 duce outputs that diverge from factual reality. 005 This phenomenon is particularly concerning 006 in sensitive applications such as medical con-007 sultation and legal advice, where accuracy is 008 paramount. Inspired by human lie detectors 009 using physiological responses, we introduce 010 the LLM Factoscope, a flexible and extendable 011 pipeline that leverages the inner states of LLMs 012 for factual detection. Our investigation reveals 013 distinguishable patterns in LLMs’ inner states 014 when generating factual versus non-factual con-015 tent. We demonstrate its effectiveness across 016 various architectures, achieving over 96% ac-017 curacy on our custom-collected factual detec-018 tion dataset. Our work opens a new avenue 019 for utilizing LLMs’ inner states for factual de-020 tection and encourages further exploration into 021 LLMs’ inner workings for enhanced reliability 022 and transparency. 023