Conversational question answering (ConvQA) 001 over knowledge graphs (KGs) involves answer-002 ing multi-turn natural language questions about 003 information contained in a KG. State-of-the-art 004 methods of ConvQA often struggle with inex-005 plicit question-answer pairs. These inputs are 006 easy for human beings to understand given a 007 conversation history, but hard for a machine to 008 interpret, which can degrade ConvQA perfor-009 mance. To address this issue, we propose a rein-010 forcement learning (RL) based model, C ORN - 011 N ET , which utilizes question reformulations 012 generated by large language models (LLMs) 013 to improve ConvQA performance. C ORN N ET 014 adopts a teacher-student architecture where a 015 teacher model learns question representations 016 using human writing reformulations, and a stu-017 dent model to mimic the teacher modelâ€™s output 018 via reformulations generated by LLMs. The 019 learned question representation is then used by 020 a RL model to locate the correct answer in a 021 KG. Extensive experimental results show that 022 C ORN N ET outperforms state-of-the-art Con-023 vQA models. 024