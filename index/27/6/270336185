We propose coactive learning as a model and feed-back mechanism for training large language models (LLMs). The key insight is that users provide implicit feedback whenever they edit the text y proposed by an LLM. While the edited text ¯ y is typically not a gold-standard example for supervised training, coactive learning merely requires that the edited text ¯ y is an improvement over the proposed text y . Note that such weak implicit preference feedback ¯ y ≻ y is available in many application settings on a per-user basis, thus enabling the personalization of LLMs. In this paper, we develop the theoretical basis for coactive training of non-linear models, and we derive CoRLL as the first coactive learning algorithm for LLMs. Empirical results indicate that CoRLL is effective even for weak and noisy coactive preference feed-back, making it a promising algorithm for training and personalization of LLMs from feedback that is naturally collected in many use cases.