In the past few years, the emergence of artificial intelligence combined with the surface electromyography (sEMG) paved the way to the development of smart and wearable electronic systems for human-machine interaction. However, the monitoring and the classification of surface electromyography signals remain a difficult task due to their inherently stochastic nature and substantial variability. Several models for analyzing and classifying sEMG signals have been assessed such as Convolutional Neural Networks [1], Support Vector Machines [2], and other classical machine learning algorithms [3], [4]. Although these approaches provide acceptable performance, a hardware implementation of such machine learning models in energy-efficient in-memory-computing architectures remains very challenging. Then, we developed an alternative brain-inspired learning algorithm based on High-Dimensional Computing. In this work, we propose a comprehensive comparative analysis of classifiers for gesture recognition using surface electromyographic signals of the largest public database existing at the time [5]. After a preprocessing of those signals and an optimization of hyperparameters, we show that our High-Dimensional Computing model outperforms the performance of well-established classification algorithms. For the first time, we implemented a new temporal encoder based solely on addition operation (no permutation) for the formation of Ngram sequences. The ability of our model to recognize hand, wrist, grasping and functional movements on the most comprehensive benchmark scientific databases (Ninapro) is demonstrated with high accuracy (up to 85.6%), exceeding the performance of usual machine learning algorithms.