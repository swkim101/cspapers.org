Physics-informed neural networks (PINNs) have shown promising potential for solving partial differential equations (PDEs) using deep learning. 

However, PINNs face training difficulties for evolutionary PDEs, particularly for dynamical systems whose solutions exhibit multi-scale or turbulent behavior over time.

The reason is that PINNs may violate the temporal causality property since all the temporal features in the PINNs loss are trained simultaneously. 

This paper proposes to use implicit time differencing schemes to enforce temporal causality, and use transfer learning to sequentially update the PINNs in space as surrogates for PDE solutions in different time frames.

The evolving PINNs are better able to capture the varying complexities of the evolutionary equations, while only requiring minor updates between adjacent time frames.

Our method is theoretically proven to be convergent if the time step is small and each PINN in different time frames is well-trained.

In addition, we provide state-of-the-art (SOTA) numerical results for a variety of benchmarks for which existing PINNs formulations may fail or be inefficient.

We demonstrate that the proposed method improves the accuracy of PINNs approximation for evolutionary PDEs and improves efficiency by a factor of 4â€“40x.

The code is available at https://github.com/SiqiChen9/TL-DPINNs.