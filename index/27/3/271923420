Recent years have seen the rapid development 001 of large generative models for text; however, 002 much less research has explored the connection 003 between text and another “language” of com-004 munication – music . Music, much like text, can 005 convey emotions, stories, and ideas, and has its 006 own unique structure and syntax. In our work, 007 we bridge text and music via a text-to-music 008 generation model that is highly efficient, ex-009 pressive, and can handle long-term structure. 010 Specifically, we develop a cascading latent dif-011 fusion approach that can generate multiple min-012 utes of high-quality stereo music at 48kHz from 013 textual descriptions. Moreover, our model fea-014 tures high efficiency, which enables real-time 015 inference on a single consumer GPU with a 016 reasonable speed. Through experiments and 017 property analyses, we show our model’s com-018 petence over a variety of criteria compared with 019 existing music generation models. Lastly, to 020 promote the open-source culture, we provide 021 a collection of open-source libraries with the 022 hope of facilitating future work in the field. 1 023