Adjusting the outdated behaviors of large langu-001 gae models (LLMs) after deployment remains 002 a significant challenge. It motivates the model 003 editing research, which is however mainly ex-004 plored in a restricted task form with triplet-005 based edit requests. Some recent works have 006 initiated a transition to a more practical and 007 unified editing task that takes free-form text 008 as edit requests. However, there is gaps in nu-009 anced benchmark designs and re-evaluation of 010 existing methods. To bridge the gaps, we in-011 troduce a multi-level benchmark for free text 012 model editing ( M ULFE ). The benchmark cate-013 gorizes probe queries into three levels of gen-014 eralization, ranging from basic literal memory 015 to deeper understanding and reasoning. Based 016 on the benchmark, we conduct extensive exper-017 iments across various base models, edit sizes, 018 and editing methods, including adaptations of 019 mainstream locate-and-edit and hypernetwork 020 methods. The results highlight the inconsistent 021 behaviors of edited models on different gener-022 alization levels. Higher level of generalization 023 is still difficult for current methods. Based on 024 the findings, we propose S IDE , a simple yet 025 effective method based on in-context distilla-026 tion to enhance the generalization performance. 027 The benchmark and baseline methods will be 028 publicly available for facilitating further study. 029