Human gaze data provide cognitive information 001 that reflect human language comprehension and 002 has been effectively integrated into a variety 003 of natural language processing (NLP) tasks, 004 demonstrating improved performance over cor-005 responding plain text-based models. In this 006 work, we propose to integrate a gaze module 007 into pre-trained language models (PLMs) at the 008 fine-tuning stage to improve their capabilities 009 to learn representations that are grounded in 010 human language processing. This is done by 011 extending the conventional purely text-based 012 fine-tuning objective with an auxiliary loss to 013 exploit cognitive signals. The gaze module is 014 only included during training, retaining com-015 patibility with existing PLM-based pipelines. 016 We evaluate the proposed approach using two 017 distinct PLMs on the GLUE benchmark and ob-018 serve that the proposed model improves perfor-019 mance compared to both standard fine-tuning 020 and traditional text augmentation baselines. All 021 code is available on anonymous_git . 022