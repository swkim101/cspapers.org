While large language models have significantly 001 enhanced the effectiveness of discourse relation 002 classifications, it remains unclear whether their 003 comprehension is faithful and reliable. We pro-004 vide D I SQ, a new method for evaluating the 005 faithfulness of understanding discourse based 006 on question answering. We first employ in-007 context learning to annotate the reasoning for 008 discourse comprehension, based on the connec-009 tions among key events within the discourse. 010 Following this, D I SQ interrogates the model 011 with a sequence of questions to assess its grasp 012 of core event relations, its resilience to counter-013 factual queries, as well as its consistency to its 014 previous responses. 015 We then evaluate language models with differ-016 ent architectural designs using D I SQ, finding: 017 (1) D I SQ presents a significant challenge for 018 all models, with the top-performing GPT model 019 attaining only 41% of the ideal performance in 020 PDTB; (2) D I SQ is robust to domain shifts and 021 paraphrase variations; (3) Open-source mod-022 els generally lag behind their closed-source 023 GPT counterparts, with notable exceptions be-024 ing those enhanced with chat and code/math 025 features; (4) Our analysis validates the effec-026 tiveness of explicitly signalled discourse con-027 nectives, the role of contextual information, and 028 the benefits of using historical QA data. 029