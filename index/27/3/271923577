As the scale of Large Language Models 001 (LLMs) increases, it is necessary to compress 002 the models to reduce the substantial demand on 003 computational resources. Network pruning sig-004 nificantly reduces the model size by converting 005 the weight matrix from dense to sparse data for-006 mat. Current methodologies advocate for one-007 shot pruning to avoid the expense of retrain-008 ing, ensuring the maintenance of model perfor-009 mance under conditions of 50%-60% unstruc-010 tured pruning. Nevertheless, matrices charac-011 terized by this level of sparsity could not be 012 treated as sparse matrices, because the indices 013 would incur significant costs. To mitigate this 014 problem, NVIDIA introduced the 2:4 struc-015 tured sparsity. However, we observe a notable 016 decline in model performance when adopting 017 2:4 structured sparsity due to group constraints. 018 In this paper, we introduce the Weight Recover 019 Prune (WRP) approach. By recovering a min-020 imal set of critical weights, WRP aims to en-021 hance model performance while maintaining 022 the efficiency of the compression. Our evalua-023 tion of the WRP method on the LLAMA2 and 024 OPT models shows that it outperforms other 025 2:4 pattern one-shot pruning methods. Mean-026 while, WRP can guarantee a compression rate 027 of about 60% compared to the dense model. 028 Our code is available at: https://anonymous. 029 4open.science/r/WRP-0A5F . 030