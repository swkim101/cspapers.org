Document-level relation extraction (DocRE) 001 aims to extract relations between entities in a 002 whole document. One of the pivotal challenges 003 of DocRE is to capture the intricate interde-004 pendencies between relations of entity pairs. 005 Previous methods have shown that logical rules 006 are able to explicitly help capture such interde-007 pendencies. These methods either learn logical 008 rules to refine the output of a trained DocRE 009 model, or first learn logical rules from anno-010 tated data and then inject the learnt rules to a 011 DocRE model using auxiliary training objec-012 tive. In this paper, we argue that these learning 013 pipelines may suffer from the issue of error 014 propagation. To mitigate this issue, we propose 015 Joint Modeling Relation extraction and Logi-016 cal rules or JMRL for short, a novel rule-based 017 framework that jointly learns both a DocRE 018 model and logical rules in an end-to-end fash-019 ion. Specifically, we parameterize a rule reason-020 ing module in JMRL to simulate the inference 021 of logical rules, thereby explicitly modeling the 022 reasoning process. We also introduce an auxil-023 iary loss and a residual conection mechanism 024 in JMRL to better reconcile the DocRE model 025 and the rule reasoning module. Experimental 026 results on two benchmark datasets demonstrate 027 that the proposed JMRL framework is consis-028 tently superior to existing rule-based frame-029 works on both datasets, improving five baseline 030 models for DocRE by a significant margin. 031