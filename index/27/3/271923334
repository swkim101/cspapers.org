Simultaneous Machine Translation (SiMT) 001 aims to yield a real-time partial translation 002 with a monotonically growing source-side 003 context. However, there is a counterintuitive 004 phenomenon about the context usage between 005 training and inference: e.g. , in wait-k inference, 006 model consistently trained with wait-k is much 007 worse than that model inconsistently trained 008 with wait-k ′ ( k ′ ̸ = k ) in terms of translation 009 quality. To this end, we first investigate the 010 underlying reasons behind this phenomenon 011 and uncover the following two factors: 1) the 012 limited correlation between translation quality 013 and training loss; 2) exposure bias between 014 training and inference. Based on both reasons, 015 we then propose an effective training approach 016 called context consistency training accordingly, 017 which encourages consistent context usage 018 between training and inference by optimizing 019 translation quality and latency as bi-objectives 020 and exposing the predictions to the model 021 during the training. The experiments on 022 three language pairs demonstrate that our 023 SiMT system encouraging context consistency 024 outperforms existing SiMT systems with 025 context inconsistency for the first time. 026