LLMs confront inherent limitations in terms 001 of its knowledge, memory, and action. The 002 retrieval augmentation stands as a vital mecha-003 nism to address these limitations, which brings 004 in useful information from external sources to 005 augment the LLM. However, existing retrieval 006 methods encounter two pressing issues. On 007 one hand, the general retrievers are not prop-008 erly optimized for retrieval augmentation hence 009 exhibit limited effectiveness; on the other hand, 010 the task-specific retrievers excel in the targeted 011 retrieval augmentation scenario, while lack the 012 versatility to handle diverse scenarios. In this 013 work, we propose LLM-Embedder for the 014 unified support of diverse retrieval augmen-015 tation scenarios. Our method presents three 016 technical contributions. Firstly, we introduce 017 a new reward formulation , namely rank-aware 018 reward. It exploits the ranking position of the 019 desired output among N sampled outputs from 020 the LLM, which leads to fine-grained and ro-021 bust computation of reward from the LLM’s 022 feedback. Secondly, we design a novel distil-023 lation objective , called graded distillation. It 024 incorporates both the absolute value and the 025 relative order of the reward for more sufficient 026 utilization of the LLM’s feedback. Thirdly, we 027 systematically optimize the multi-task learning , 028 which effectively unifies the multiple retrieval 029 functionalities into one model. In our exper-030 iment, LLM-Embedder notably improves the 031 LLM’s performances in various downstream 032 tasks, and outperforms both general and task-033 specific retrievers with a substantial advantage. 034