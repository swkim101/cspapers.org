The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated re-inforcement learning (RL) with compiler feed-back for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder , a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a C urriculum of C ode C ompletion S ubtasks, while FGO only optimizes the model by masking the unexe-cuted code segments to provide F ine-G rained O ptimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks 1 .