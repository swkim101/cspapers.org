Both transduction and rejection have emerged as important techniques for defending against adversarial perturbations. A recent work by (Gold-wasser et al., 2020) showed that rejection combined with transduction can give provable guarantees (for certain problems) that cannot be achieved otherwise. Nevertheless, under recent strong adversarial attacks (GMSA (Chen et al., 2022)), Goldwasser et al.’s work was shown to have low performance in a practical deep-learning setting. In this paper, we take a step towards realizing the promise of transduction + rejection in more realistic scenarios. Our key observation is that a novel application of a reduction technique in (Tramèr, 2022), which was until now only used to demonstrate the vulnerability of certain defenses, can be used to actually construct e ff ective defenses. Theoretically, we show that a careful application of this technique in the transductive setting can give significantly improved sample-complexity for robust generalization. Our theory guides us to design a new transductive algorithm for learning a selective model; extensive experiments using state of the art attacks (AutoAttack, GMSA) show that our approach provides significantly better robust accuracy (81.6% on CIFAR-10 and 57.9% on CIFAR-100 under l ∞ with budget 8 / 255) than existing techniques (Croce et al., 2020). The implementation is available at https://github.com/ nilspalumbo/transduction-rejection .