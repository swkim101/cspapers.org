Video Frame Interpolation (VFI), which aims at gener-ating high-frame-rate videos from low-frame-rate inputs, is a highly challenging task. The emergence of bio-inspired sensors known as event cameras, which boast microsecond-level temporal resolution, has ushered in a transformative era for VFI. Nonetheless, the application of event-based VFI techniques in domains with distinct environments from the training data can be problematic. This is mainly because event camera data distribution can undergo substan-tial variations based on camera settings and scene conditions, presenting challenges for effective adaptation. In this paper, we propose a test-time adaptation method for event-based VFI to address the gap between the source and target domains. Our approach enables sequential learning in an online manner on the target domain, which only provides low-frame-rate videos. We present an approach that lever-ages confident pixels as pseudo ground-truths, enabling stable and accurate online learning from low-frame-rate videos. Furthermore, to prevent overfitting during the con-tinuous online process where the same scene is encountered repeatedly, we propose a method of blending historical sam-ples with current scenes. Extensive experiments validate the effectiveness of our method, both in cross-domain and con-tinuous domain shifting setups. The code is available at https://github.com/Chohoonhee/TTA-EVF.