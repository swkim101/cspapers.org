Few-shot learning represents a paradigm shift in deep learning, which is defined by massive amounts of data and computing capacity. It solves these issues by utilizing novel training approaches and optimization goals. This shift also applies to meta-learning and provides an alternative to standard deep learning, which requires a lot of resources. The conventional technique indirectly infers the underlying structure between classes in the context of classification problems. In contrast, this work attempts to clearly portray the links between classes, which is a substantial change from previous techniques. This study looks into embedding adaptability for novel data in a high computing power environment. This is accomplished by investigating a dynamic interaction between samples utilizing their centroids inside a support. To better capture the subtle interactions between classes, this study uses Graph Neural Networks (GNNs) in conjunction with attention processes. The ability to explicitly characterize class relationships boosts the flexibility and efficacy of few-shot learning, demonstrating how it can turn limited input and processing resources into maximum results. The research extends the potential of deep learning in situations where large-scale datasets and powerful computers are not readily available, offering up new avenues for real-world applications.