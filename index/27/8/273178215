Multimodal Named Entity Recognition and 001 Grounding (MNERG) aims to extract paired 002 textual and visual entities from texts and im-003 ages. It has been well explored through a two-004 step paradigm: initially identifying potential 005 visual entities using object detection methods 006 and then aligning the extracted textual entities 007 with their corresponding visual entities. How-008 ever, when it comes to fine-grained MNERG, 009 the long-tailed distribution of textual entity cat-010 egories and the performance of object detectors 011 limit the effectiveness of traditional methods. 012 Specifically, more detailed classification leads 013 to many low-frequency categories, and existing 014 object detection methods often fail to pinpoint 015 subtle regions within images. To address these 016 challenges, we propose the G ranular E ntity 017 M apper (GEM) framework. Firstly, we design 018 a multi-granularity entity recognition module, 019 followed by a reranking module based on the 020 Multimodal Large Language Model (MLLM) 021 to incorporate hierarchical information of en-022 tity categories, visual cues, and external tex-023 tual resources collectively for accurate fine-024 grained textual entity recognition. Then, we 025 utilize a pre-trained Large Visual Language 026 Model (LVLM) as an implicit visual entity 027 grounder that directly deduces relevant visual 028 entity regions from the entire image without the 029 need for bounding box training. Experimental 030 results on the GMNER and FMNERG datasets 031 demonstrate that our GEM framework achieves 032 state-of-the-art results on the fine-grained con-033 tent extraction task. 034