In this work, we propose Stochastic Gradient Descent on Random Mixtures (SGDRM) as a simple way of protecting data under data breach threats. We prove that SGDRM converges to a critical point for the least-squares problem and for deep neural networks with linear activations. We also conduct extensive experiments, and observe that SGDRM can be applied to general deep learning tasks as well.