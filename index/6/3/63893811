Many-core shared memory architectures are ubiquitous in the design of both High-Performance Computing (HPC) and commodity systems because they provide an excellent trade-oﬀ between performance, power eﬃciency and programmability. The MPI programming model is very popular in scientiﬁc applications and provides a strong development platform for future power-constrained systems. MPI’s abstraction of explicit communication across distributed memory helps developers minimize data motion. Unfortunately, MPI’s distributed memory abstraction makes it diﬃcult for MPI implementations to fully leverage the capabilities of shared memory hardware. OS-level separations between processes force MPI to perform unnecessary copying to transfer data within shared memory nodes. This paper presents a new approach to the design of MPI libraries that shares the addresses spaces of MPI ranks executing on the same node, allowing them to communicate directly like native threaded applications. While prior approaches to the design of such libraries are based on threading, we demonstrate that this approach is both brittle and performs poorly in practice. We instead propose a novel approach based on OS processes that enables both high-performance shared memory communication and full, high-performance integration with existing MPI libraries for inter-node communication.