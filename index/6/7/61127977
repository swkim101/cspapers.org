As machine learning technologies become part of the business process in the guise of knowledge discovery and data mining we are forced to re-evaluate the importance of some of the constraints of the technology. Recently the scaling problem of machine learning has attracted considerable interest. The constraints of the commercial world, in terms of both time and money, force us to consider the consequences of handling very large data sets or undertaking computationally very expensive knowledge discovery tasks. The two approaches traditionally advocated to deal with this problem are to either take a subset of the data set and attempt to learn from that, or to accept the cost of learning and simply be patient. We argue that neither of these approaches is adequate, and that given efficient polynomially complex algorithms some knowledge discovery tasks still remain impractical. This paper is an attempt to review the various systems of parallel knowledge discovery that appear in the literature, and to develop a classification that allows us to group them together meaningfully. We conclude by outlining a new approach for parallel machine learning based on the results of an empirical investigation of the properties of classifiers learned on subsets of datasets. (5 pages)