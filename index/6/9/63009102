In this paper we describe our work on algorithms for an Automatic Moral Agent that follows only one rule: “always try to increase user’s wellbeing but never violate common sense”, hoping that machines with context recognition and growing knowledge processing capability not only will keep us safe but also will make us better human beings indicating our cognitive biases and errors. To make a prototype of such system we utilize natural language processing and web-mining techniques to collect and analyze human experiences (concentrating on reasons and consequences) from the WWW to recognize which act is moral and wich is not. By choosing crowd-based approach we can: a) avoid relying on a particular ethical approach or a set of moral rules created by one person or a small group of people; b) avoid never-ending job of creating ethical rules about whatever might happen in the world. We introduce our ideas on acquiring common sense knowledge and utilizing lexicons for recognizing similar situations. Then we introduce our ideas on average moral evaluation of these situations (direct evaluation, made by users, and indirect evaluation from consequences like “being sentenced”). We also discuss strengths and weaknesses of the approach and propose further progress of the approach which by default should doubt the human common sense and also constantly confirm acquired knowledge with actual law regulations or the latest scientific papers.