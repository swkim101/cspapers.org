The idea of using “optimistic” evaluations to guide exploration has been around for a long time, but has received renewed interest recently thanks to the development of bound-based guarantees [Kolter and Ng, 2009]. Asmuth et al. [2009] have recently presented a way to construct optimistic evaluations by merging sampled MDPs. Their BOSS algorithm is polynomial in the size of the state-action-space of the merged MDP, and thus a multiple of the size of the actual state space. The success of BOSS raises the question whether it might be possible to avoid the sampling step and instead use an approximate parametrization of the belief over values. To test this idea, we construct an approximate Gaussian (i.e. log quadratic) joint posterior over the values of states under a given policy. Our derivation ignores the effect of future expected experience (in contrast to a fully Bayesian treatment as in the BEETLE algorithm [Poupart et al., 2006]). On a discrete state-space environment described by a vector of expected rewards r (i.e. the expected reward from the state-action pair (s, a)i is ri), transition dynamics described by a matrix T (i.e. p(sj ∣(s, a)i) = Tij), and a policy Π (i.e. p((s, a)j ∣si) = Πij), the values q of state-action pairs are determined by Bellman’s equation Π(I − TΠ)q = ΠT r. Using, for example, conjugate prior inference with a Dirichlet prior on T and a Gaussian prior on r, leads to a belief over q which, while obviously not analytically Gaussian, can often be characterized reasonably well by a mean vector and covariance matrix. We draw inspiration from the Expectation Propagation algorithm [Minka, 2001], using moment matching to construct an approximate Gaussian marginal. Unfortunately, the structure of Bellman’s equation makes it difficult to perform moment matching directly on the marginal of q. But it is straight-forward to do so on r, from which an approximate Gaussian message to q can be constructed. The resulting approximate belief over q provides optimistic point estimates as a function of the policy Π, e.g. as ∥ q + eq∥, where eq is the eigenvector associated with the largest eigenvalue of the joint belief over q. We compared this approach to BEETLE and BOSS, on the widely used Chain environment. Our simplistic, parametric, approximate approach significantly outperforms the two more expensive contemporary algorithms on the most challenging, “full” setup, and shows an instructive disadvantage to them in the “semi-tied” setup. (The “tied” setup is too simple, all algorithms perform well on it). This suggests that there is scope for the application of light-weight probabilistic algorithms based on approximate message-passing, as solvers for general Reinforcement Learning Problems. Their modular structure, allows for the explicit modeling of environment structure and thus for increased performance on real-world problems.