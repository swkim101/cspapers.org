Since the emergence of large annotated datasets [1], state-of-the-art hand pose estimation methods have been mostly based on discriminative learning [2]. Recently, a hybrid approach has embedded a kinematic layer into the deep learning structure in such a way that the pose estimates obey the physical constraints of human hand kinematics [3]. However, the existing approach relies on a single personâ€™s hand shape parameters, which are fixed constants. Therefore, the existing hybrid method has problems to generalize to new, unseen hands. In this work, we extend the kinematic layer to make the hand shape parameters adaptable. In this way, the learnt network can generalize towards arbitrary hand shapes. Furthermore, we show that by applying Spatial Transformer Network [4], the performance of a regression task can be also improved. The effectiveness and limitations of our proposed approach are evaluated on the Hands 2017 challenge dataset [1].