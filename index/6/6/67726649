In order to simplify the adaptation of learning algorithms to growing amounts of data as well as to the growing need for accurate and confident predictions in critical applications, in this paper we propose a novel and provably effective parallelisation scheme. In contrast to other parallelisation techniques, our scheme can be applied to a broad class of learning algorithms without further mathematical derivations and without writing a single line of additional code. We achieve this by treating the learning algorithm as a black-box that is applied in parallel to random data subsets. The resulting hypotheses are then assigned to the leaves of an aggregation tree which bottom-up replaces each set of hypotheses corresponding to an inner node of the tree by its Radon point. Considering the confidence parameters epsilon, delta as the input, a learning algorithm is efficient if its sample complexity is polynomial in 1/epsilon, ln(1/delta) and its time complexity is polynomial in its sample complexity. With our parallelisation scheme, the algorithm can achieve the same guarantees when applied on a polynomial number of cores in polylogarithmic time. This result allows the effective parallelisation of a broad class of learning algorithms and is intrinsically related to Nick's class (NC) of decision problems as well as NC-learnability for exact learning. The cost of this parallelisation is in the form of a slightly larger sample complexity. Our empirical study confirms the potential of our parallisation scheme on a range of data sets and for several learning algorithms.