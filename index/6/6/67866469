There has been a rapid proliferation of auto-graders for introductory programming courses to cope with rapidly expanding Computer Science enrollments. In this poster we report on our research in developing and deploying a problem solving learning environment, Spinoza 3.0, which goes beyond auto-grading and provides active learning exercises to teach debugging, testing, and good coding style in addition to providing immediate feedback on program correctness. The key idea is to collect all of the attempts by all students to a particular problem generated by a particular class and to form them into equivalence classes where two attempts are equivalent if they produce exactly the same values on all of the unit tests. This allows the system to find the most common errors made by that class. The system then generates a variety of different kinds of problems by selecting either one of the most common errors or one of the correct solutions and asking the students to critique it. After a student makes a critique they are shown (a subset of) the critiques of their fellow students and are asked to pick the best one before proceeding to the next problem. Since the system generates new problems precisely tailored to this particular class, by turning selected attempts into new problems, we call this approach Recursive Pedagogy. This poster reports on preliminary results we have obtained using Recursive Pedagogy in an Introduction to Python Programming course with 19 college students.