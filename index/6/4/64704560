Biological vision is envied for its ability to learn to recognize objects in the 3D world undergoing physical transformations. A recent hypothesis is that the ventral visual pathway exploits the manifold nature of these transformations to form neural codes that are efÔ¨Åcient for discrimination, but there is no compelling model for how this representation is learned from data. We propose a computational model that performs unsupervised learning on received retinal imagery to infer identity-preserving transformations. We show that such a model can successfully learn useful representations on a subset of objects that can be transferred to new objects. We also demonstrate that this model can be used to infer 3D transformations from 2D imagery despite the ill-conditioned nature of the problem. This model for 3D inference can account for psychophysical experiments such as 3D shape perception from random-dot kinematograms.