1 PROBLEM STATEMENT Long latency stores can have a significant impact in performance by delaying the committing of instructions and putting pressure on CPU resources. This problem is exacerbated on architectures with TSO (Total Store Order) memory ordering, since a long latency store at the head of the store-queue will delay committing of all subsequent stores [2]. A store-buffer mitigates this problem, by gathering long latency stores and allows them (and subsequent ones) to retire immediately [1]. The write-backs to L1 are later performed in program order as early as possible. Since this buffer needs to be accessed on every load (for correctness), and is faster and more energy efficient than an L1, it functions similarly to a filter-cache[3]. However, low store-buffer hit-ratios (figure 1) hinders performance and energy benefits expected of a filter-cache. Moreover, to minimize memory access latency, the store-buffer and L1 are probed in parallel (since the load is likely to miss in the store-buffer), eliminating any load dynamic energy improvement, independently of store-buffer hit-ratio. In this work we investigate the possibility of using the storebuffer as a filter-cache without compromising its original purpose of hiding store-miss latency. This objective poses us with 3 challenges: (1) determine if and by how much can the store-buffer hit-ratio be improved (section 2); (2) how to minimize L1 cache accesses without increasing average load latency (section 3); and (3) how to delay stores in the store-buffer in order to maximize store-buffer hits without increasing CPU stalls (section 3).