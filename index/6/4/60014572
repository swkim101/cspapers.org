The use of constrained formal/computational systems just adequate for modeling various aspects of language-syntax, semantics, pragmatics and discourse, among others-has proved to be not only an effective research strategy but has led to deeper understanding of these aspects, with implications to both machine processing as well as human processing. This approach enables one to distinguish between universal and stipulative constraints. The other approach is to start with the most general and most powerful formal/computational system and use it to model these phenomena, thus making all constraints stipulative, in a sense. The tension between these approaches, together with the increasing use of empirical methods combining structural and statistical information, has made the relationship between natural language processing and AI quite stormy. I will review some of the past and current research following the first approach and also suggest how the stormy relationship could be improved.