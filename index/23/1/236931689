Dataﬂow computation dominates the landscape of big data processing, in which a program is structured as a directed acyclic graph (DAG) of operations. As dataﬂow computation consumes extensive resources in clusters, making sense of its performance becomes critically important. This, how-ever, can be difﬁcult in practice due to the complexity of DAG execution. In this paper, we propose a new approach that learns to characterize the performance of dataﬂow computation based on code analysis . Unlike existing performance reasoning techniques, our approach requires no code instrumentation and applies to a wide variety of dataﬂow frameworks . Our key insight is that the source code of an operation contains learnable syntactic and semantic patterns that reveal how it uses resources. Our approach establishes a performance-resource model that, given a dataﬂow program, infers automatically how much time each operation has spent on each resource (e.g., CPU, network, disk) from past execution traces and the program source code, using machine learning techniques. We then use the model to predict the program runtime under varying resource conﬁgurations. We have implemented our solution as a CLI tool called CrystalPerf . Extensive evaluations in Spark, Flink, and TensorFlow show that CrystalPerf can predict job performance under conﬁguration changes in multiple resources with high accuracy. Real-world case studies further demonstrate that CrystalPerf can accurately detect runtime bottle-necks of DAG jobs, simplifying performance debugging.