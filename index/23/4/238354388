In this article, we investigate the spectral behavior of random features kernel matrices of the type K = E w [ σ ( w T x i ) σ ( w T x j )] ni,j =1 , with nonlinear function σ ( · ) , data x 1 , . . . , x n ∈ R p , and random projection vector w ∈ R p having i.i.d. entries. In a high-dimensional setting where the number of data n and their dimension p are both large and comparable, we show, under a Gaussian mixture model for the data, that the eigenspectrum of K is independent of the distribution of the i.i.d. (zero-mean and unit-variance) entries of w and only depends on σ ( · ) via its (generalized) Gaussian moments E z ∼N (0 , 1) [ σ ′ ( z )] and E z ∼N (0 , 1) [ σ ′′ ( z )] . As a result, for any kernel matrix K of the form above, we propose a novel random features technique, called Ternary Random Feature (TRF), that (i) asymptotically yields the same limiting kernel as the original K in a spectral sense and (ii) can be computed and stored much more efﬁciently, by wisely tuning (in a data-dependent counterpart expensive kernels. Our article comes along with (Couillet et al., 2021) as ﬁrst steps in re-designing machine learning algorithms using Random Matrix Theory, in order to be able to perform computations on massive data using desktop computers instead of relying on energy consuming giant servers.