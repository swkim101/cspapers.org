From robots to autonomous vehicles, the design of artificial moral agents (AMAs) is an area of growing interest to HCI, and rising concern for the implications of deploying moral machines raises questions about which ethical frameworks are being used in AMAs. We performed a literature review to identify and synthesize key themes in how ethical theories have been applied to AMAs. We reviewed 53 papers and performed a thematic analysis to describe and synthesize the current conversation across HCI. We found many describe the value of ethical theories and implement them as technical contributions, but very few conduct empirical studies in real settings. Furthermore, we found AMA development is dominated by two ethical theories: deontology and consequentialism. We argue that the focus on deontology and consequentialism risks creating AMAs based on a narrow set of Western ethical values and concepts at the expense of other forms of moral reasoning.