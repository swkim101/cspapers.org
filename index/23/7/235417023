The adversarial vulnerability of deep neural networks has attracted signiﬁcant attention in machine learning. From a causal viewpoint, adversarial attacks can be considered as a speciﬁc type of distribution change on natural data. As causal reasoning has an instinct for modeling distribution change, we propose to incorporate causality into mitigating adversarial vulnerability. However, causal formulations of the intuition of adversarial attack and the development of robust DNNs are still lacking in the literature. To bridge this gap, we construct a causal graph to model the generation process of adversarial examples and deﬁne the adversarial distribution to formalize the intuition of adversarial attacks. From a causal perspective, we ﬁnd that the label is spuriously correlated with the style (content-independent) information when an instance is given. The spurious correlation implies that the adversarial distribution is constructed via making the statistical conditional association between style information and labels drastically diﬀerent from that in natural distribution. Thus, DNNs that ﬁt the spurious correlation are vulnerable to the adversarial distribution. Inspired by the observation, we propose the adversarial distribution alignment method to eliminate the diﬀerence between the natural distribution and the adversarial distribution. Extensive experiments demonstrate the eﬃcacy of the proposed method. Our method can be seen as the ﬁrst attempt to leverage causality for mitigating adversarial vulnerability.