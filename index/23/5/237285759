Event cameras are neuromorphic vision sensors that are able to capture high dynamic range with low latency in microseconds, without motion blur. Their strength lies in the unique representation of data as asynchronous events, enabling detection of scene structures less invariantly from dynamic luminance changes. However, a single event does not represent spatial information, and events must be integrated to translate into meaningful information. Therefore, state-of-the-art deep learning algorithms have focused on reconstructing the original scene from events. However, as environmental variances are also captured throughout events and restored in reconstructed images, simple reconstruction does not help achieving robust visual place recognition. In this paper, we suggest to use reconstructed event edges denoised for place recognition. While brightness wavers with dynamic environmental variances, edge contours only change with gradient magnitude scale. We utilize the high dynamic range of event cameras to detect these scaled edges from different environments and show that using reconstructed edges shows robust performance in overcoming day-to-night illumination variance without a large training set.