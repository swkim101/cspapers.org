We study reinforcement learning in mean-ﬁeld games. To achieve the Nash equilibrium, which consists of a policy and a mean-ﬁeld state, existing algorithms require obtaining the optimal policy while ﬁxing any mean-ﬁeld state. In practice, however, the policy and the mean-ﬁeld state evolve simultaneously, as each agent is learning while playing. To bridge such a gap, we propose a ﬁctitious play algorithm, which alternatively updates the policy (learning) and the mean-ﬁeld state (playing) by one step of policy optimization and gradient descent, respectively. Despite the nonsta-tionarity induced by such an alternating scheme, we prove that the proposed algorithm converges to the Nash equilibrium with an explicit convergence rate. To the best of our knowledge, it is the ﬁrst provably efﬁcient algorithm that achieves learning while playing.