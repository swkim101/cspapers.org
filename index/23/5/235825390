As its width tends to inﬁnity, a deep neural network’s behavior under gradient descent can become simpliﬁed and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit inﬁnite-width limits that can learn features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modiﬁcations to the standard parametrization to allow for feature learning in the limit. Using the Tensor Programs technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We ﬁnd that they outperform both NTK baselines and ﬁnite-width networks, with the latter approaching the inﬁnite-width feature learning performance as width increases. See arXiv:2011.14522 for the full version of this paper.