Machine learning systems often do not share the same inductive biases as humans and, as a result, extrapolate or generalize in ways that are inconsistent with our expectations. The tradeoff between exemplar- and rule-based generalization has been studied extensively in cognitive psychology; in this work, we present a protocol inspired by these experimental approaches to probe the inductive biases that control this tradeoff in category-learning systems. We isolate two such inductive biases: feature-level bias (differ-ences in which features are more readily learned) and exemplar or rule bias (differences in how these learned features are used for generalization). We ﬁnd that standard neural network models are feature-biased and exemplar-based, and discuss the implications of these ﬁndings for machine learning research on systematic generalization, fairness, and data augmentation.