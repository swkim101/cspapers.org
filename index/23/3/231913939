A high-quality free-motion rendering is one of the most vital traits to achieve an immersive human-robot interaction. Rendering free-motion is notably challenging for rehabilitation exoskeletons due to their relatively high weight and powerful actuators required for strength training and support. In the presence of dynamic human movements, accurate feedback linearization of the robot’s dynamics is necessary to allow for a linear synthesis of interaction wrench controllers. Hence, we introduce a virtual model controller that uses two 6-DoF force sensors to control the interaction wrenches of a multi-DoF torque-controlled exoskeleton over the joint accelerations and inverse dynamics. Furthermore, we propose a disturbance observer for controlling the joint acceleration to diminish the influence of modeling errors on the inverse dynamics. To provide a high-bandwidth, low-bias estimation of the system’s acceleration, we introduce a bias-observer which fuses the information from joint encoders and seven low priced IMUs. We have validated the performance of our proposed control structure on the shoulder and arm exoskeleton ANYexo. The experimental comparison of the controllers shows a reduction of the felt inertia and maximum reflected joint torque by a factor of more than three compared to state of the art. The controllers’ robustness w.r.t. a model mismatch is validated. The experiments show that the closed-loop acceleration control improves the tracking, particularly at joints with low inertia. The proposed controllers’ performance sets a new benchmark in haptic transparency for comparable devices and should be transferable to other applications.