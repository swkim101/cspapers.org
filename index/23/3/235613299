Solving continuous minimax optimization is of extensive practical interest, yet notoriously unstable and difﬁcult. This paper introduces the learning to optimize ( L2O ) methodology to the minimax problems for the ﬁrst time and addresses its accompanying unique challenges. We ﬁrst present Twin-L2O , the ﬁrst dedicated minimax L2O framework consisting of two LSTMs for updating min and max variables separately. The decoupled design is found to facilitate learning, particularly when the min and max variables are highly asymmetric. Empirical experiments on a variety of minimax problems corroborate the effectiveness of Twin-L2O. We then discuss a crucial concern of Twin-L2O, i.e., its inevitably limited generalizability to unseen optimizees. To address this issue, we present two complementary strategies. Our ﬁrst solution, Enhanced Twin-L2O , is empirically applicable for general mini-max problems, by improving L2O training via leveraging curriculum learning. Our second alternative, called Safeguarded Twin-L2O , is a preliminary theoretical exploration stating that under some strong assumptions, it is possible to theoretically establish the convergence of Twin-L2O. We benchmark our algorithms on several testbed problems and compare against state-of-the-art minimax solvers. The code is available at: https://github.