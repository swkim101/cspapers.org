CogSketch is an open-domain sketch understanding system. CogSketch takes a unique approach to sketch understanding that focuses less on low-level recognition and more on highlevel reasoning with sketches. In addition to demonstrating the basic system, we will showcase applications to cognitive simulation and education. Overview of CogSketch CogSketch is an open-domain sketch understanding system built on the nuSketch architecture (Forbus & Usher, 2001). The idea behind nuSketch is that sketching is often a multimodal interaction where users use a combination of drawing and language to convey ideas. Many sketching systems rely on bottom-up recognition of the objects being sketched (e.g. Avarado & Davis, 2001). Such systems have been shown to be quite useful. However they often place restrictions on drawing conventions and are limited to domains for which recognition libraries have been constructed. We take a different approach, based on the observation that recognition is not essential for sketchbased reasoning. Humans are often not great artists in real time and rely on language to provide clues to the conceptual content of their sketches. We provide a complimentary approach to recognition based sketching systems by allowing users to conceptually label objects they have sketched, telling the system what each object represents. There are two benefits to this approach: 1) by removing recognition, we are free to focus on deeper reasoning about objects in a sketch and 2) we do not place any domain or depiction constraints on users. The basic unit of a CogSketch sketch is a glyph. Glyphs contain ink and content. The ink is the set of points drawn by a user and the content is the knowledge about what the glyph represents. The content consists of one or more conceptual labels chosen by the user. Conceptual labels are links to concepts in an underlying knowledge base. In CogSketch the knowledge base is currently a subset of the OpenCyc KB containing over 1.8 million facts, over 1 CogSketch is available online at http://spatiallearning.org/projects/cogsketch_index.html 2 OpenCyc is available online at http://www.opencyc.org/ 58,000 concepts, and over 17,000 predicates. For a more detailed description of the nuSketch architecture underlying CogSketch see (Forbus & Usher, 2001; Forbus et al, 2004) CogSketch automatically computes a number of qualitative spatial relationships between glyphs in a sketch. These include topological relations, relative position of glyphs, and relative size. CogSketch uses the topological relations to identify groups of glyphs that are connected or contained within a single outer glyph. For more on the spatial relationships computed in CogSketch see (Forbus & Usher, 2003). While we rely on users to draw their ink as glyphs, CogSketch can also decompose glyphs into their component edges and build up structural representations of a glyph’s shape, to allow deeper spatial reasoning. For example, the shapes of glyphs can be compared, and CogSketch can identify transformations between shapes, such as rotations and reflections. Analogy and Similarity A central feature of CogSketch is our use of analogical processing based on Gentner’s structure mapping theory (Gentner, 1983). We use the Structure Mapping Engine (SME) (Falkenhainer, Forbus and Gentner, 1989) to allow users to compare objects in a sketch and detect similarities and differences. Analogies in CogSketch are based on both the visual and the conceptual material in a sketch. There is psychological evidence that structural alignment occurs in visual processing (Markman & Gentner, 1996; Gentner & Markman, 1997), and SME captures many aspects of this processing accurately. This provides a powerful mechanism for CogSketch to determine when things in a sketch will look alike to users.