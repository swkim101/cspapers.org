A fundamental obstacle in learning information from data is the presence of nonlinear redun-dancies and dependencies in it. To address this, we propose a Fourier-based approach to extract relevant information in the supervised setting. We ﬁrst develop a novel Fourier expansion for functions of correlated binary random variables. This expansion is a generalization of the standard Fourier analysis on the Boolean cube beyond product probability spaces. We further extend our Fourier analysis to stochastic mappings. As an important application of this analysis, we investigate learning with feature subset selection. We reformulate this problem in the Fourier domain and introduce a computationally efﬁcient measure for selecting features. Bridging the Bayesian error rate with the Fourier coefﬁcients, we demonstrate that the Fourier expansion provides a powerful tool to characterize nonlinear dependencies in the features-label relation. Via theoretical analysis, we show that our proposed measure ﬁnds provably asymptotically optimal feature subsets. Lastly, we present an algorithm based on our measure and verify our ﬁndings via numerical experiments on various datasets.