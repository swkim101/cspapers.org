Motivated by emerging applications such as live-streaming e-commerce, promotions and recommendations, we introduce a general class of multi-armed bandit problems that have the following two features: (i) the decision maker can pull and collect rewards from at most K out of N different arms in each time period; (ii) the expected reward of an arm immediately drops after it is pulled, and then non-parametrically recovers as the idle time increases. With the objective of maximizing expected cumulative rewards over T time periods, we propose, construct and prove performance guarantees for a class of “Purely Periodic Policies”. For the ofﬂine problem when all model parameters are known, our proposed policy obtains an approximation ratio that is at the order of 1 − O (1 / √ K ) , which is asymptotically optimal when K grows to inﬁnity. For the online problem when the model parameters are unknown and need to be learned, we design an Upper Con-ﬁdence Bound (UCB) based policy that approximately has (cid:101) O ( N √ T ) regret against the ofﬂine benchmark. Our framework and policy design may have the potential to be adapted into other ofﬂine planning and online learning applications with non-stationary and recovering rewards.