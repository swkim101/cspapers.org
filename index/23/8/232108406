Network analysis and veriﬁcation tools are often a godsend for network operators as they free them from the fear of introducing outages or security breaches. As with any complex software though, these tools can (and often do) have bugs. For the operators, these bugs are not necessarily problematic except if they affect the precision of the model as it applies to their speciﬁc network. In that case, the tool output might be wrong: it might fail to detect actual conﬁguration errors and/or report non-existing ones. In this paper, we present Metha , a framework that systematically tests network analysis and veriﬁcation tools for bugs in their network models. Metha automatically generates syntactically-and semantically-valid conﬁgurations; compares the tool’s output to that of the actual router software; and detects any discrepancy as a bug in the tool’s model. The challenge in testing network analyzers this way is that a bug may occur very rarely and only when a speciﬁc set of conﬁg-uration statements is present. We address this challenge by leveraging grammar-based fuzzing together with combinatorial testing to ensure thorough coverage of the search space and by identifying the minimal set of statements triggering the bug through delta debugging. We implemented Metha and used it to test three well-known tools. In all of them, we found multiple (new) bugs in their models, most of which were conﬁrmed by the developers.