In general, college students enrolled in computer science courses are provided some software design classes and exercises. In a cross-reviewing learning/exercise environment, however, beginners often cannot provide useful feedback and viewpoints to others. One of the reasons for this is that beginners are not confident in their understanding of the software design models. These learners should be encouraged by strengthening their understanding of the models. In this research, we propose a self-review supporting environment that provides learners with feedback generated by translating their Unified Modeling Language (UML) class diagrams into natural language descriptions. This enables beginners to recognize discrepancies between what they would like to describe in their class diagrams and what they actually describe in their class diagrams by comparing them in natural language. We designed three types of templates to make learners aware of the discrepancies. Moreover, we performed an experiment to evaluate whether the feedback produced by translating a learner's model into natural language can support the learner to detect errors in the model themselves, and whether this feedback can support the learner to correct errors in the model themselves. The results of the experiment indicated that the subjects detected errors in their class diagrams with 89.2% accuracy and detected 47.3% of all errors in their diagrams. Furthermore, the subjects corrected the errors they correctly detected with 78.6% accuracy.