Neural text decoding algorithms strongly inﬂuence the quality of texts generated using language models, but popular algorithms like top-k , top-p (nucleus), and temperature-based sampling may yield texts that have objectionable repetition or incoherence. Although these methods generate high-quality text after ad hoc parameter tuning that depends on the language model and the length of generated text, not much is known about the control they provide over the statistics of the output. This is important, however, since recent reports show that humans prefer when perplexity is neither too much nor too little and since we experimentally show that cross-entropy (log of perplexity) has a near-linear relation with repetition. First we provide a theoretical analysis of perplexity in top-k , top-p , and temperature sampling, under Zipﬁan statistics. Then, we use this analysis to design a feedback-based adaptive top-k text decoding algorithm called mirostat that generates text (of any length) with a predetermined target value of perplexity without any tuning. Experiments show that for low values of k and p , perplexity drops signiﬁcantly with generated text length and leads to excessive repetitions (the boredom trap). Contrarily, for large values of k and p , perplexity increases with generated text length and leads to incoherence (confusion trap). Mirostat avoids both traps. Speciﬁcally, we show that setting target perplexity value beyond a threshold yields negligible sentence-level repetitions. Experiments with human raters for ﬂuency, coherence, and quality further verify our ﬁndings.