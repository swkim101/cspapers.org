Task-speciﬁc ﬁne-tuning on pre-trained transformers has achieved performance breakthroughs in multiple NLP tasks. Yet, as both computation and parameter size grows linearly with the number of sub-tasks, it is increasingly difﬁcult to adopt such methods to the real world due to unrealistic memory and computation overhead on computing devices. Previous works on ﬁne-tuning focus on reducing the growing parameter size to save storage cost by parameter sharing. However, compared to storage, the constraint of computation is a more critical issue with the ﬁne-tuning models in modern computing environments. In this work, we propose LeTS , a framework that leverages both computation and parameter sharing across multiple tasks. Compared to traditional ﬁne-tuning, LeTS proposes a novel neural architecture that contains a ﬁxed pre-trained transformer model, plus learnable additive components for sub-tasks. The learnable components reuse the intermediate activations in the ﬁxed pre-trained model, decoupling computation dependency. Differentiable neural architecture search is used to determine a task-speciﬁc computation sharing scheme, and a novel early stage pruning is applied to additive components for sparsity to achieve parameter sharing. Extensive experiments show that with 1.4% of extra parameters per task, LeTS reduces the computation by 49.5% on GLUE benchmarks with only 0.2% accuracy loss compared to full ﬁne-tuning.