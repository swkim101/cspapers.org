Our work focuses on the interaction of two key challenges in deep learning: achieving model compactness and sparsity simultaneously with adversarial robustness . Robustness aware network pruning methods showed recent success in this domain. Nevertheless, no effective method existed for robust end-to-end sparse training. Motivating question: How can we enable learning with state-of-the-art robust training objectives by end-to-end sparse training under strict connectivity constraints? Optimizing the network with a negative log-posterior loss which combines a sparsity prior with the robust training objective . We update both the connectivity conÔ¨Åguration and the weights such that we are sampling network parameters from the posterior via stochastic gradient Langevin dynamics. Incorporating the sparsity prior by a weight re-parametrization trick: