We study function approximation for episodic re-inforcement learning with entropic risk measure. We ﬁrst propose an algorithm with linear function approximation. Compared to existing algorithms, which suffer from improper regularization and regression biases, this algorithm features debiasing transformations in backward induction and regression procedures. We further propose an algorithm with general function approximation, which is shown to perform implicit debiasing transformations. We prove that both algorithms achieve a sublinear regret and demonstrate a tradeoff be-tween generality and efﬁciency. Our analysis provides a uniﬁed framework for function approximation in risk-sensitive reinforcement learning, which leads to the ﬁrst sub-linear regret bounds in the setting.