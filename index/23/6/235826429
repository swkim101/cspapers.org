Selective classiﬁcation is a powerful tool for decision-making in scenarios where mistakes are costly but abstentions are allowed. In general, by allowing a classiﬁer to abstain, one can improve the performance of a model at the cost of reducing coverage and classifying fewer samples. However, recent work has shown, in some cases, that selective classiﬁcation can magnify disparities between groups, and has illustrated this phenomenon on multiple real-world datasets. We prove that the sufﬁciency criterion can be used to mitigate these disparities by ensuring that selective classiﬁcation increases performance on all groups, and introduce a method for mitigating the disparity in precision across the entire coverage scale based on this criterion. We then provide an upper bound on the conditional mutual information between the class label and sensitive attribute, conditioned on the learned features, which can be used as a regular-izer to achieve fairer selective classiﬁcation. The effectiveness of the method is demonstrated on the Adult, CelebA, Civil Comments, and CheXpert datasets.