Deep learning models often raise privacy concerns as they leak information about their training data. This leakage enables membership inference attacks (MIA) that can identify whether a data point was in a model’s training set. Research shows that some data augmentation mechanisms may reduce the risk by combatting a key factor increasing the leakage, overﬁtting. While many mechanisms exist, their effectiveness against MIAs and privacy properties have not been studied systematically. Employing two recent MIAs, we explore the lower bound on the risk in the absence of formal upper bounds. First, we evaluate 7 mechanisms and differential privacy, on three image classiﬁcation tasks. We ﬁnd that applying augmentation to increase the model’s utility does not mitigate the risk and protection comes with a utility penalty. Further, we also investigate why popular label smoothing mechanism consistently ampliﬁes the risk. Finally, we propose loss-rank-correlation (LRC) metric to assess how similar the effects of different mechanisms are. This, for example, reveals the similarity of applying high-intensity augmentation against MIAs to simply reducing the training time. Our ﬁndings emphasize the utility-privacy trade-off and provide practical guidelines on using augmentation to manage the trade-off.