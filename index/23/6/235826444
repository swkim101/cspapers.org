It is well-known that, for separable data, the reg-ularised two-class logistic regression or support vector machine re-normalised estimate converges to the maximal margin classiﬁer as the regular-isation hyper-parameter λ goes to 0. The fact that different loss functions may lead to the same solution is of theoretical and practical relevance as margin maximisation allows more straightforward considerations in terms of generalisation and geometric interpretation. We investigate the case where this convergence property is not guaranteed to hold and show that it can be fully characterised by the distribution of error terms in the latent variable interpretation of linear classiﬁers. In particular, if errors follow a regularly varying distribution, then the regularised and re-normalised estimate does not converge to the maximal margin classiﬁer. This shows that classiﬁcation with fat tails has a qualitatively different behaviour, which should be taken into account when considering real-life data.