Understanding human navigational intent is essential for robots to be able to interact with and navigate around humans safely and naturally. Current methods typically perform inference through only one mode of perception such as human motion trajectory, and a single theoretical framework such as a learning-based or classical approach. In contrast, this paper studies prediction of human navigational intent using multimodal perception within a hybrid framework. Our framework consists of two modules: a) a learning-based prediction module to predict a human’s future goal position, and b) a classical control theory-inspired reconstruction module to reconstruct a possible future trajectory or a set of possible future positions using the predicted future goal position. For the prediction module, we propose an end-to-end LSTM-CNN hybrid neural network for predicting a human’s future position in the real world, given human motion, human body pose and head orientation. This visual information from an egocentric perspective is used to make predictions of a human’s future position in world space, essential for robotic navigation algorithms and planning. In the reconstruction module, we present two control theoretic methods to reconstruct possible future trajectories of human: trajectory generation for differentially flat system and reachability analysis. We evaluate the performance of our framework on a newly collected dataset called SFU-Store-Nav. Experimental results reveal that our method outperforms various baselines especially when a relatively small amount of data is available.