Neural networks trained with SGD were recently shown to rely preferentially on linearly-predictive features and can ignore complex, equally-predictive ones. This simplicity bias can explain their lack of robustness out of distribution (OOD). The more complex the task to learn, the more likely it is that statistical artifacts (i.e. selection biases, spurious correlations) are simpler than the mechanisms to learn. We demonstrate that the simplicity bias can be mitigated and OOD generalization improved. We train a set of similar models to fit the data in different ways using a penalty on the alignment of their input gradients. We show theoretically and empirically that this induces the learning of more com-plex predictive patterns. OOD generalization fundamentally requires information beyond i. i.d. examples, such as multiple training environ-ments, counterfactual examples, or other side information. Our approach shows that we can defer this requirement to an independent model selection stage. We obtain SOTA re-sults in visual recognition on biased data and generalization across visual domains. The method - the first to evade the simplicity bias - highlights the need for a better under-standing and control of inductive biases in deep learning.