Dynamic Bayesian networks (DBNs) are powerful tools for unsupervised discovery of structure in time series, using generative learning algorithms such as expectation maximization (EM). While in many problems, the DBN latent variables are the primary quantities of interest for estimation purposes, unsupervised learning is often a way to perform automatic feature learning for a downstream supervised task, such as time series classification and sequential labeling. We show that the DBNs model structure allow for combination of the two aforementioned objectives using backpropagation. Our proposed architecture is motivated by the problem of nonstationary multivariate time series classification and sequential labeling. We derive supervised learning algorithms for parameter estimation and inference of latent variables in two commonly used DBN models for such time series, namely the switching vector autoregressive (SVAR) model and the switching Kalman filter (SKF). The result of our procedure can be interpreted as a compactly-parameterized recurrent neural network, which can be pre-trained using EM. Using examples from three nonstationary neurophysiological time series cohorts, we show how discriminatively-discovered dynamics in time series lead to a significant improvement in classification performance over methods based on feature learning via EM, as well as other commonly used time series classifiers.