The goal of this paper is to analyze an intriguing phenomenon recently discovered in deep networks, that is their instability to adversarial perturbations (Szegedy et al., 2014). We provide a theoretical framework for analyzing the robustness of classifiers to adversarial perturbations, and establish fundamental limits on the robustness of some classifiers in terms of a distinguishability measure between the classes. Our result implies that in tasks involving small distinguishability, no classifier in the considered set will be robust to adversarial perturbations, even if a good accuracy is achieved. Moreover, we show the existence of a clear distinction between the robustness of a classifier to random noise and its robustness to adversarial perturbations. Specifically, in high dimensions, the former is shown to be much larger than the latter for linear classifiers. This result gives a theoretical explanation for the discrepancy between the two robustness properties, which was empirically observed in (Szegedy et al., 2014) in the context of neural networks. Our theoretical framework shows that the adversarial instability is a phenomenon that goes beyond deep networks, and affects all classifiers. Unlike the initial belief that adversarial examples are caused by the high non-linearity of neural networks, our results suggest instead that this phenomenon is due to the low flexibility of classifiers, compared to the difficulty of the classification task, which is captured by the distinguishability measure. We believe these results ICML 2015 Workshop on Deep Learning, Lille, France. Copyright 2015 by the author(s). contribute to a better understanding of the phenomenon of adversarial instability to reach the goal of designing robust classifiers.