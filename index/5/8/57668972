Linear regression models, some of the most widely used and well-studied models for regression, often assume that all features can be evaluated without difficulty, while in many real-world regression scenarios this assumption simply does not hold. For example, in a medical domain features used in linear regression may correspond to the results of various diagnostic tests which have a cost and incur discomfort to the patient, or in a financial setting a regression model often requires evaluating information-gathering features that incur time and monetary costs. These and related problems can be formalized more precisely with the notion of feature costs. Assuming that prediction error and feature costs can be measured in commensurable units, we can reformulate our linear regression problem w.r.t. a parsimonious objective criterion: clearly there is no benefit in reducing error by using more features if the financial advantage gained by obtaining more accurate predictions does not outweigh the financial cost.