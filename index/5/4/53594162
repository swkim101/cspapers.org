Sequence-to-sequence attention-based models are a promising approach for end-to-1 end speech recognition. The increased model power makes the training procedure 2 more difﬁcult, and analysing failure modes of these models becomes harder because 3 of the end-to-end nature. In this work, we present various analyses to better 4 understand training and model properties. We analyse different patterns of errors 5 and the effect on the amount of training data. We investigate on pretraining variants 6 such as growing in depth and width, and their impact on the ﬁnal performance, 7 which leads to over 8% improvement in word error rate. We also highlight the high 8 variance in different training runs. For a better understanding of how the attention 9 process works, we study the encoder output and the attention energies and weights. 10 Our experiments were performed on Switchboard and LibriSpeech. 11