Many works have shown that neural network language models consistently achieve significant improvements in applications like speech recognition and statistical machine translation. However, little research is devoted to explore optimal training strategies. This paper presents an extensive study on the best-practice to train large neural network language models on a corpus of more than 5.5 billion words. We provide solutions to questions like: how to train a neural network on so many examples using the back-propagation algorithm and data selection ? Is careful initialization important ? How to speed-up training ? Can we benefit from deep architectures ? Our best neural network language model can be trained in less than 40 hours on a GPU card and achieves a 25% perplexity reduction. An important finding is that deep architectures systematically achieve better translation quality than shallow ones. We also investigate training of feed-forward architectures on context sizes of more than 30 words. By these means, we aim at achieving an “unlimited history” similar to recurrent architectures.