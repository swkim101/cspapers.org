In the last decade, researchers and medical device companies have made major advances towards transforming passive capsule endoscopes into active medical robots. One of the major challenges is to endow capsule robots with accurate perception of the environment inside the human body, which will provide necessary information and enable improved medical procedures. We extend the success of deep learning approaches from various research fields to the problem of sensor fusion for endoscopic capsule robots in the case of asynchronous and asymmetric sensor data without any need of calibration between sensors. The results performed on real pig stomach datasets show that our method achieves high precision for both translational and rotational movements and contains various advantages over traditional sensor fusion techniques.