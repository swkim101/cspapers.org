Most robot navigation systems perform place recognition using a single sensor modality and using one, or at most two heterogeneous map scales. In contrast, mammals likely per- form navigation by combining sensing from a wide variety of modalities including vision, auditory, olfaction and tactile senses. Re- cent robotics research has shown that using multiple homogeneous mapping scales improves localization performance; but this re- search has used only a single visual sensing modality, missing out on the inherent variation in spatial localization specificity pro- vided by different sensors like cameras and WiFi. In this paper we develop a multi- scale, multi-sensor system for mapping and localization, that combines spatial localization hypotheses from different sensors and at different scales to calculate an overall localization estimate. In two real-world experiments across a library and university cam- pus, we evaluate the place recognition performance of the proposed multi-scale sensing using camera and WiFi sensory input. The results demonstrate that a universal improvement in place recognition performance is achieved using the multi-scale system.