Recent deep learning (DL) models are moving more and more to dynamic neural network (NN) architectures, where the NN structure changes for every data sample. However, existing DL programming models are inefﬁcient in handling dynamic network architectures because of: (1) substantial overhead caused by repeating dataﬂow graph construction and processing every example; (2) difﬁculties in batched execution of multiple samples; (3) inability to incorporate graph optimization techniques such as those used in static graphs. In this paper, we present “Cavs”, a runtime system that overcomes these bottlenecks and achieves efﬁcient training and inference of dynamic NNs. Cavs represents a dynamic NN as a static vertex function F and a dynamic instance-speciﬁc graph G . It avoids the overhead of repeated graph construction by only declaring and constructing F once, and allows for the use of static graph optimization techniques on pre-deﬁned operations in F . Cavs performs training and inference by scheduling the execution of F following the dependencies in G , hence naturally exposing batched execution opportunities over different samples. Experiments comparing Cavs to state-of-the-art frameworks for dynamic NNs (TensorFlow Fold, PyTorch and DyNet) demonstrate the efﬁcacy of our approach: Cavs achieves a near one order of magnitude speedup on training of dynamic NN architectures, and ablations verify the effectiveness of our proposed design and optimizations.