In the framework of robotics, Reinforcement Learning (RL) deals with the learning of a task by the robot itself. This paper presents a hierarchical-planning approach in which the robot learns the optimal behavior for different levels in a decoupled way. For high-level discrete actions, Q-learning was chosen, whereas for the low level we utilize Policy Improvement with Path Integrals (PI2) algorithm to learn the parameters of policies, represented by rhythmic Dynamic Movement Primitives (DMPs). The paper studies the case of a 4-finger-gripper manipulator, which performs the task of continuously spinning a ball around a desired axis. The results demonstrate the efficacy of the hierarchical planning and the increased performance of the task when PI2 is used in conjunction with rhythmic DMPs in a real environment.