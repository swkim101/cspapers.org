The ever increasing hardware speeds has led systems and application designers to push software to its limits and create applications with microsecond response times. Moreover, such interactive applications, e.g. websearch, run at massive scales, with large fan-in and fan-out patterns, where the importance of tail-latency becomes crucial to guarantee Service Level Agreements (SLAs). So being able to measure latency accurately is vital to debug those system, identify their behaviour and specify proper SLAs. Accurate latency measurement proves to be very challenging. Unlike throughput, latency is a very sensitive metric that is a ected by multiple factors, both within the application and the operating system [2], but also the measuring client itself. Consequently, we are faced with two major challenges related to latency measurement. We need tools that are able to measure latency in such a low microsecond scale, while independently of the tools, we need an accurate an sound methodology that is able to produce unbiased and realistic results that are collected under settings that highly resemble a production environment. In this work we first collect and analyze some common methodology pitfalls that have been overlooked by previous approaches [9] and then compare existing tools and identify missing features, related to the experiment methodology, that can either lead to more statistically sound results or reduce necessary resources (time and compute) for latency experiments.