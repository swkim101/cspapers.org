We address scene layout modeling for recognizing agent-in-place actions, which are actions associated with \textit{agents} who perform them and the \textit{places} where they occur, in the context of outdoor home surveillance. We introduce a novel representation to model the geometry and topology of scene layouts so that a network can generalize from the layouts observed in the training scenes to unseen scenes in the test set. This Layout-Induced Video Representation (LIVR) abstracts away low-level appearance variance and encodes geometric and topological relationships of places to explicitly model scene layout. LIVR partitions the semantic features of a scene into different places to force the network to learn generic place-based feature descriptions which are independent of specific scene layouts; then, LIVR dynamically aggregates features based on connectivities of places in each specific scene to model its layout. We introduce a new Agent-in-Place Action (APA) dataset\footnote{The dataset is pending legal review and will be released upon the acceptance of this paper.} to show that our method allows neural network models to generalize significantly better to unseen scenes.