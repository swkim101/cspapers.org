We propose a novel neural network architecture specifically tailored to treestructured decoding, which: • maintains separate depth and width recurrent states and combines them to obtain hidden states for every node in the tree. • has a mechanism to predict tree topology explicitly (as opposed to implicitly by adding nodes with special tokens). Our experiments show that this architecture • is capable of recovering trees from encoded representations • achieves state-of-the-art performance in a task consisting of mapping sentences to simple functional programs • exhibits desirable invariance properties over sequential architectures