The adoption of indoor navigation for smartphones has been relatively slow in the past years, although it would be direly needed in complex indoor areas. The primary barriers for its adoption include the lack of fine-grained and up-to-date indoor maps and the potential deployment and maintenance cost. In this paper we investigate the feasibility of utilizing crowdsourced data for building a smartphone-based indoor navigation system, focusing on the technical challenges caused by the varying quality of crowdsourced data. We developed iMoon, an indoor navigation system based on sensor-enriched 3D models of indoor environment, and evaluated its performance via a field study in a public building covering around 1,100 square meters. iMoon builds 3D models of indoor environment from crowdsourced 2D photos, and compiles a navigation mesh from the generated 3D models. Depending on the input for 3D modelling, indoor pedestrian paths may be partly missing from the output. iMoon solves this issue by integrating into navigation mesh the pedestrian paths recognized from crowdsourced user motion trajectories. With photo-based 3D models, iMoon supports image-based localization that identifies user's position and facing direction with photos, and provides visual navigation instructions that show when and where to turn. To reduce response delay while maintaining the accuracy of localization, iMoon partitions 3D models based on the density of 3D points, and selects partitions for image-based localization using Wi-Fi fingerprints. According to our experimental results, iMoon works properly, and our solution of indoor localization achieves competitive performance compared with traditional approaches: in most cases, a user can be located within an average of 3.85 seconds with a location error of less than 2 meters and a facing direction error of less than 6 degrees.