Convolutional neural networks (CNNs) recently emerged as a promising and successful technique to tackle artificial intelligent (AI) problems including image recognition and image generation. For example, CNNs can recognize a thousand categories of objects in the ImageNet dataset not only faster but also more accurate than people. CNN is compute-intensive. Let us take AlexNet for example. It consists of five convolutional layers, and each layer demands 100 million to 450 million multiplications. Recognizing a small image (224× 224) no larger than the App icons of smartphones (512 × 512) demands more than one billion multiplications in total, let alone the computing complexity of processing large images or videos. Low-bitwidth CNNs exhibit significantly reduced computing complexity, and thus they recently attracted great attention and extensive studies. Low-bitwidth CNNs typically restrict themselves to utilizing oneto four-bit, fixed-point weight and activation values instead of floating-point values. For example, DoReFa-Net [4] proposes to set the bitwidth of weights and activations between one and four bits. Binarized neural networks (BNN) [1] and XNOR-Net [3] push the limit even further to one-bit and replace floating-point multiply operations with bitwise XNOR operations. However, we observe a key limitation of the baseline lowbitwidth CNNs as well as their training methods that the bitwidth of these low-bitwidth CNNs has to be decided at training time, and then the bitwidth has no adjustment flexibility at inference time. Figure 1 illustrates this limitation we observe. Let us take DoReFa-Net on ImageNet for example. One can separately train 1-bit and 3-bit CNNs, as denoted by the first and second bars in the figure. Intuitively, the 3-bit CNN achieves higher accuracy than the 1-bit counterpart, and this benefit comes at the cost of higher computing complexity (i.e., 3-bit multiplications vs. 1-bit multiplications). A less-intuitive phenomenon is that if one deliberately truncates the weights and activations of the 3-bit CNN to 1-bit at inference time, the accuracy does not hold at the level of a 1-bit CNN (the first bar) as one may expect; instead, the truncated CNN would be totally ruined, and the accuracy would be far lower than what the 1-bit CNN can achieve (denoted by the third bar) (In Figure 1 and later