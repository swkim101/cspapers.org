With the increasingly more powerful mobile devices, it becomes possible to perform more deep learning tasks on the devices, and there are also important advantages of learning on devices, such as personalization and efficiency. However, a good understanding of the capabilities of modern mobile devices for deep learning is generally lacking. To address this gap in knowledge, this paper presents a comprehensive study on performing training and inference of deep neural networks (DNNs) on mobile devices. This study is based on TensorFlow+, an extension of the widely used TensorFlow framework that enables it to train DNNs on devices and use the available GPUs to accelerate the learning. The most significant results of our study are: 1) The size of the network is crucial not only to meet the device's memory constraint but also for training performance; 2) Hardware acceleration is important to the learning speed on devices. By accelerating both the forward and backward path with the device's GPU, our extended TensorFlow can cut down the training time by 44.8%; 3) Comparing CPU, memory, and battery usages, memory size is the most serious constraint to training networks on devices.