Our eyes see well only what is directly in front of them; they must continually scan the faces, words, and objects around us. Perceptual integration is the process of combining the resulting jumpy, nonuniform images (Figure 1) into our stable, comprehensive perception of the world. Visual robots that scan their environment with moving cameras must also integrate visual information. We describe IRV, a visual robot that sees across camera movements [Prokopowicz, 1995]. Furthermore, IRV learns to integrate from experience, which consists of a series of random movements of a camera mounted on a motorized pan-tilt platform, observing the day-to-day activity in a laboratory. The learning procedure makes minimal assumptions, is robust, and scales. That is, learning proceeds without a prior analytic model, external calibration references, or a contrived environment, and can compensate for arbitrary imaging distortions, including lens aberrations, rotation of the camera about its viewing axis, and spatially-varying or even random sampling patterns.