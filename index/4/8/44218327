We propose a novel optimization framework for learning a low-rank matrix which is also constrained to lie in a linear subspace. Exploiting the duality theory, we present a factorization that decouples the low-rank and structural constraints onto separate factors. The optimization problem is formulated on the Riemannian spectrahedron manifold, where the Riemannian framework allows to develop computationally efficient conjugate gradient and trust-region algorithms. Our approach easily accommodates popular non-smooth loss functions, e.g., L1-loss, and our algorithms are scalable to large-scale problem instances. The numerical comparisons show that our algorithms outperform state-of-the-art in standard, robust, and non-negative matrix completion, Hankel matrix learning, and multi-task feature learning problems on various benchmarks.