We propose a new approach to running machine learning (ML) web app on resource-constrained embedded devices by offloading ML computations to servers. We can dynamically offload computations depending on the problem size and network status. The execution state is saved in the form of another web app called snapshot which simplifies the state migration. Some issues related to ML such as how to handle the Canvas object, the ML model, and the privacy of user data are addressed. The proposed offloading works for real web apps with a performance comparable to running the app entirely on the server.