A common assumption in machine learning is the independence and identical distribution (i.i.d.) of the data samples, which states that training and test samples are independent and drawn from the same probability distribution. Unfortunately, real-world data rarely satisfies this assumption. The covariate shift assumption [10] states that the training data X and test data X are sampled from two multivariate random variables X and X with different distributions p (X) and p (X), while the conditional distribution of labels given the data p (Y |X) is assumed to be the same over training and test data, i.e., p (Y |X) = p (Y |X). A common approach to perform covariate shift adaptation (CSA) is through representation learning: learn a mapping f that projects the training and test data onto new representations Z and Z having similar distributions p (Z) and p (Z), then learn p (Y |Z) using standard machine learning algorithms [2].