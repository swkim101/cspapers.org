This abstract presents an approach to fusing hundreds of 3D point clouds to make complete models of 3D objects[1]. This topic first arose in the mid-1990s, but has now become particularly relevant because of the availability of consumer video-rate 3D sensors, such as the Kinect. These sensors can easily acquire hundreds or thousands of 3D images of objects or scenes in a few seconds, which has led researchers to ask again how to fuse the individual 3D datasets. This project focused on fusing hundreds of views of the exterior of an object, in contrast to hundreds of views of a scene, as in the KinectFusion [2] or SLAM approaches. The core motivating problem was how to manage registration error. For example, incremental image to image registration allows the registration errors to accumulate, and so the shape gradually distorts. The SLAM approach reduces the error by ‘loop closure’. The intuition was that one could create an alignment error minimization approach, whereby each individual scan minimized its registration error with all of the other scans. As each scan was attempting to do this independently, with the right registration error formulation, the scans would settle into a minimum error position. Previous approaches have incorporated similar alignment update schemes wherein transforms and point correspondences are alternatively optimized while keeping the other fixed ([3], [4], [5], [6]). By alternating the update of view transforms and error estimation, the two solutions tend to mutually improve one another during the process and converge to appropriate solutions.