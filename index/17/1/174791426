Camera-based technologies have become a growing component of robotic and autonomous systems such as self-driving cars. Much recent research has investigated how key autonomous system capabilities such as visual place recognition can be made robust to real-world challenges such as extreme appearance change. The original sequence-based methods were incapable of handling platform velocity variability; to address this some follow-up techniques incorporated sophisticated sequence-matching search algorithms that also increased the likelihood of false positive matches. Other new methods solve this problem by explicitly incorporating motion information but either require a suitable motion encoding sensor, or use state-of-the-art visual odometry techniques that degrade in low light, motion blur, and rapid camera movement. In this work, we develop a solution for the middle ground by developing a robust translational motion estimator which is then used to normalize camera trajectories and improve place recognition performance. We also present a new odometry fusion method that seamlessly switches between state-of-the-art visual odometry and a more robust but cruder motion estimation pipeline, enabling continuity of motion estimation and hence high place recognition performance. We demonstrate the effectiveness of these contributions on several challenging datasets that explicitly test the appearance-and speed-invariance of place recognition techniques.