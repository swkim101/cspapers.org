As it was mentioned in Section 5 to show the pure effect of redundancy in distributed training we start with a warm-up experiment. In this experiment RI-SGD has only 1 round of communication at the end of training, hence each device does the training locally with a bit of redundancy infused. We choose different redundancy values from μ ∈ {0.0, 0.05, 0.1, 0.25} and do the training and compare the results with syncSGD. Please note that when μ = 0, our algorithm is equivalant to PR-SGD (Yu et al., 2018). Figure 5 shows the result of this experiment. When we do not have any redundancy, there is a gap between training error of this setting and SyncSGD, which is evident. However, as we add more redundancy this gap is diminished and we can reach smaller error rate way sooner than SyncSGD. That is the most interesting part that we are gaining the speed-up with respect to SyncSGD (almost twice as fast as SyncSGD) with only adding redundancy to the training process. Increasing the redundancy rate would slightly increase the time of training, however, reduces the final error. Hence, as we move from fully synchronous SGD to distributed local SGD we are trading accuracy with speed. On the other hand, redundancy can increase the accuracy, with roughly the same speed-up, thus we can benefit from the advantages of the two settings in our RI-SGD.