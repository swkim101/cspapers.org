We give a computational framework for estimating the bias in coverage resulting from making approximations in Bayesian inference. Coverage is the probability credible sets cover prior parameter values. We show how to estimate the coverage an approximation scheme achieves when the ideal but intractable observation model and the prior can be simulated, but have been replaced, in the Monte Carlo, with approximations. Coverage estimation procedures given in Lee et al. (2018) work on simple problems, but do not scale well, as those authors note. For example, Lee et al. (2018) calibrate a completely collapsed MCMC algorithm for partition structure in a Dirichlet process model for random effects in a hierarchical model and a small data set, but they note it fails when the model is applied to clustering on a larger dataset. By exploiting the symmetry of the coverage error under permutation of low level group labels and smoothing with Bayesian Additive Regression Trees, we show that the original approximate inference had poor coverage for these data and should not be trusted.