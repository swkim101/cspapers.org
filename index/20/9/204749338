We perform reward-agnostic attacks on an Importance Weighted Actor-Learner Architecture (IMPALA) based deep reinforcement learning agent by perturbing the input pixels in different ways. In one environment, where the agent needs to locate and pick up objects, we show that blacking out a two pixel wide border heavily impacts the agent’s performance, while blacking out the center of the agent’s visual field has minimal impact. This suggests that the agent is relying mainly on peripheral vision to solve this task. Interestingly, the same border-attack is less effective in a pure navigation task. We also find that an agent trained in a multi-task setup seems to be more resilient to attacks. Further, we report preliminary results of per-frame pixel attacks using a differential evolution algorithm and discuss directions for further research. Surprisingly, the most successful pixel perturbations found by differential evolution on the “collect good objects” task are concentrated at the edge of the frame, which agrees with the effectiveness of the attack that blacks out the border.