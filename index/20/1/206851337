This paper presents a framework to embody a user (e.g. disabled persons) into a humanoid robot controlled by means of brain-computer interfaces (BCI). With our framework, the robot can interact with the environment, or assist its user. The low frequency and accuracy of the BCI commands is compensated by vision tools, such as objects recognition and mapping techniques, as well as shared-control approaches. As a result, the proposed framework offers intuitive, safe, and accurate robot navigation towards an object or a person. The generic aspect of the framework is demonstrated by two complex experiments, where the user controls the robot to serve him a drink, and to raise his own arm.