At present, wireless sensing based gesture recognition is becoming a rising star due to its convenience and non-invasiveness without privacy issues, while the strict requirement of the deployment and surrounding environment is still an unavoidable issue which limits its development and generalization. Although there are some works involving the environmental variance, the changes of relative positions between devices and users are ignored. As one of the most popular wireless sensing methods, RFID is widely used in activity recognition with its stable low-level physical characters such as phase and RSS. Besides, the signals reflected from RFID tags intuitively delineate its movements. On the other hand, many interactive gesture-driven applications, such as gesture input for video games, have a paramount and unavoidable issue about the latency between completion of a gesture and its recognition. Inspired by deep learning, this paper presents a real-time ongoing gesture recognition system EUIGR, which efficiently integrates phase and RSS data streams, and extracts both environment and user invariant features. The proposed system seamlessly integrates CNNs (Convolutional Neural Networks) and LSTM (Long Short-Term Memory) to fuse RFID low-level physical characters and extract space-temporal information. Furthermore, with adversarial learning, EUIGR suppresses environment-related factors and the user-specific features, and obtains strong robustness to individual diversity and decreases the environmental dependence. We also implement the system with COTS RFID devices, and extensive experimental results show the effectiveness and accuracy of EUIGR.