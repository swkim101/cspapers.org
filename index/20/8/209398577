Text categorization is one of the key functions for utilizing vast amount of documents. It can be seen as a classificauon problem, which has been studmd in pattern recogmtion and machine learning fields for a long time and several classffication methods have been developed such as staustical classfficauon, decision tree, support vector machines and so on. Many researchers applied those classification methods to text categorization and reported their performance (e.g., decision tree[3/, Bayes classifier[2/, support vector macbane[1]). Yang conducted comprehensive study of comparison of text categorization and reported that k nearest neighbor and support vector machines works well for text categorization[4/. In the previous studies, classification methods were usually compared using single pair of training and test data However, classification method with more complex family of classffiers requires more training data and small training data may result in deriving unreliable classifier, that is, the performance of the derived classifier vanes much depending on training data. Therefore, we need to take the size of traamng data into account when comparing and selecting a classification method. In this paper, we discuss how to select a classifier from those derived by various classification methods and how the size of training data affects the performance of the denved classifier. In order to evaluate the reliability of classfficatlon method, we consider the variance of accuracy of derived classffier. We first construct a statistical model. In the text categorization, each document is usually represented with a feature vector that consists of weighted frequencies of terms. In the vector space model, document is a point in high dimensional feature space and a classifier separates the feature space into subspaces each of which is labeled with a category. Let us consider the problem of classifying documents into c categories, and suppose we obtain a classffier which separate the feature space into m subspaces s~, s 2 , . . . , s,~. In the case of Rocchio's classification method, the number m of the future subspaces is the number c of categories, while it is the number of leaves for decision trees. Let p, denote occurrence probability, that is, the probability that a document vector is in subspace s,. Notice that ~ = 1 P, = 1 holds. Suppose the category of a subspace si is c~, then the accuracy of si, denoted by cq, is