Autonomous robots have become a very popular topic within the artificial intelligence field. These systems are able to perform difficult or risky tasks that could be dangerous when done by humans or trained animals. Vision is commonly considered the most relevant input sensor for autonomous robots and tracking systems. However, auditory information is also important in some specific situations where vision cannot provide any useful information when navigating. In this work, a spike-based model of the medial superior olive of the inner ear has been implemented in reconfigurable hardware for performing sound source localization in real time. Future works will focus on integrating this information with vision in order to achieve a fully bio-inspired autonomous tracking system. CCS CONCEPTS • Applied computing → Event-driven architectures.