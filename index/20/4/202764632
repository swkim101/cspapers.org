Can a gradual transition from the source to the target dataset improve knowledge transfer when ﬁne-tuning a convolutional neural network to a new domain? Can we use training examples from general image datasets to improve classiﬁcation on ﬁne-grained datasets? We present two image similarity metrics and two methods for progressively transitioning from the source dataset to the target dataset when ﬁne-tuning to a new domain. Preliminary results, us-ing the Flowers 102 dataset, show that the ﬁrst proposed method, stochastic domain subset training, gives an improvement in classiﬁcation accuracy compared to standard ﬁne-tuning, for one of the two similarity metrics. However, the second method, continuous domain subset training, re-sults in a reduction in classiﬁcation performance.