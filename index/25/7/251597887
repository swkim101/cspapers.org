Many of the challenges entailed in detecting online misinformation are related to our own cognitive limitations as human beings: We can only see a small part of the world at once, so we need to rely on others to pre-process part of that information for us. This makes us vulnerable to misinformation and points at AI as a necessary means to amplify our ability to deal with it at scale. Recent advances [1] demonstrate it is possible to build semi-automatic tools to detect online misinformation. However, the limitations are still many: our algorithms are hard to explain to human stakeholders, the reduced availability of ground truth data is a bottleneck to train better models, and our processing pipelines are long and complex, with multiple points of potential failure. To address such limitations, strategies that wisely combine algorithms that learn from data with explicit knowledge representations are fundamental to reason with misinformation while engaging [2] with human stakeholders. In this talk, I advocate for a partnership between humans and AI to deal with online misinformation detection, go through the challenges such partnership faces, and share some of the ongoing work that pursues this vision.