Recently, pre-trained language models (PLMs) have achieved great success on various NLP tasks and have shown a trend of exponential growth in model size. To alleviate the unaffordable computational costs brought by the size growth, model compression has been widely explored. Existing efforts have achieved promising results in compressing medium-sized models for speciﬁc tasks, while task-agnostic compression for big models with over billions of parameters is rarely studied. Task-agnostic compression can provide an efﬁ-cient and versatile big model for both prompting and delta tuning, leading to a more general impact than task-speciﬁc compression. Hence, we introduce a task-agnostic compression toolkit BMCook for big models. In BMCook, we implement four representative compression methods, including quantization, pruning, distillation, and MoEﬁcation. Devel-opers can easily combine these methods towards better efﬁciency. To evaluate BMCook, we apply it to compress T5-3B (a PLM with 3 billion parameters). We achieve nearly 12x ef-ﬁciency improvement while maintaining over 97% of the original T5-3B performance on three typical NLP benchmarks. Moreover, the ﬁnal