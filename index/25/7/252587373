Efforts have been made to apply topic seed 001 words to improve the topic interpretability of 002 topic models. However, due to the semantic di-003 versity of natural language, supervisions from 004 seed words could be ambiguous, making it 005 hard to be incorporated into the current neu-006 ral topic models. In this paper, we propose 007 SeededNTM , a neural topic model enhanced 008 with supervisions from seed words on both 009 word and document levels. We introduce a 010 context-dependency assumption to alleviate the 011 ambiguities with context document informa-012 tion, and an auto-adaptation mechanism to au-013 tomatically balance between multi-level infor-014 mation. Moreover, an intra-sample consistency 015 regularizer is proposed to deal with noisy su-016 pervisions via encouraging perturbation and 017 semantic consistency. Extensive experiments 018 on multiple datasets show that SeededNTM can 019 derive semantically meaningful topics and out-020 performs the state-of-the-art seeded topic mod-021 els in terms of topic quality and classification 022 accuracy. 023