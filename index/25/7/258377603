Graph neural networks have achieved state-of-the-art performance on graph-related tasks through layer-wise neighborhood aggregation. Previous works aim to achieve powerful capability via designing injective neighborhood aggregation functions in each layer, which is difficult to determine and numerous additional parameters make it difficult to train these models. It is the input space and the aggregation function that achieve powerful capability at the same time. Instead of designing complexity aggregation functions, we propose a simple and effective framework, namely MV-GNN, to improve the model expressive power via constructing the new input space. Precisely, MV-GNN samples multi-view subgraphs for each node, and any GNN model can be applied to these views. The representation of target node is finally obtained via aggregating all views injectively. Two typical GNNs (i.e., GCN and GAT) are adopted as base models in the proposed framework, and we demonstrate the effectiveness of MV-GNN through extensive experiments.