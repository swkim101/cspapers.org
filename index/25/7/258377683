Software systems that learn from data are being deployed in increasing numbers in real-world application scenarios. It is a difcult and tedious task to ensure at development time that the end-to-end ML pipelines for such applications adhere to sound experimentation practices, such as the strict isolation of train and test data. Furthermore, there is a need to enforce legal and ethical compliance in automated decision-making with ML. For example, to determine whether a model works equally well for diferent groups. To enforce privacy rights (such as the ‘right to be forgotten’), we must identify which models consumed the user’s data for model training to retrain them without this data. Moreover, model predictions can be corrupted due to undetected data distribution shift, e.g., when the train/test data was incorrectly sampled or changed over time (covariate shift) or when the distribution of the target label changed (label shift). Data scientists also require support for pipeline debugging and for uncovering erroneous data, e.g., to identify samples that are not helpful for the classifer and potentially dirty or mislabeled or to identify subsets of data for which a model does not work well. Towards automated low-efort tooling for ML pipelines. Most of the listed issues are typically addressed manually in an ad-hoc way and require signifcant expertise and extra code. In many cases, there is no system support for detecting particular issues. Typically, data has to be integrated frst, as common libraries assume the input to be in a single table. Furthermore, specialised solutions are often incompatible with popular libraries. Even simple tasks like computing fairness metrics can be challenging. This situation is in stark contrast to the software engineering world, with established best practices and infrastructure for testing and CI. Provenance is all you need. At the core of these issues is missing provenance tracking in current tooling. For example, we fnd that can automate the detection of many common correctness issues in ML pipelines with access to (�) the materialised artifacts of a pipeline (its input relations, and its outputs, e.g., the feature matrices, labels, and predictions of a classifer) as well as (��) their whyprovenance [4] (e.g., which input records were used to compute a particular output). This allows us to design screening techniques with low invasiveness for declaratively written ML pipelines. Modeling ML pipelines as datafow computations. We base our approach on a recently introduced model of treating ML pipelines