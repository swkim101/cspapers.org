As the first step of modern natural language processing, text representation encodes discrete texts as continuous embeddings. Pre-trained language models (PLMs) have demonstrated strong ability in text representation and significantly promoted the development of natural language understanding (NLU). However, existing PLMs represent a text solely by its context, which is not enough to support knowledge-intensive NLU tasks. Knowledge is power , and fusing external knowledge explicitly into PLMs can provide knowledgeable text representations. Since previous knowledge-enhanced methods differ in many aspects, making it difficult for us to reproduce previous meth-ods, implement new methods, and transfer be-tween different methods. It is highly desirable to have a unified paradigm to encompass all kinds of methods in one framework. In this paper, we propose , a knowledge-enhanced text representation toolkit for nat-ural language understanding. According to our proposed Uni fied K nowledge-E nhanced P aradigm ( UniKEP ), CogKTR consists of four key stages, including knowledge acquisition, knowledge representation, knowledge injection, and knowledge application. CogKTR currently supports easy-to-use knowledge acquisition interfaces, multi-source knowledge embeddings, diverse knowledge-enhanced models, and various knowledge-intensive NLU tasks. Our unified, knowledgeable and modular toolkit is publicly available at GitHub 1 , with an online sys-tem 2 and a short instruction video 3 .