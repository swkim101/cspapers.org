This paper considers few-shot learning under the cross-domain scenario. The cross-domain setting imposes a critical challenge, i.e. , using very few (support) samples to generalize the already-learned model to a novel domain. We hold a hypothesis, i.e. , if a deep model is capable to fast generalize itself to different domains (using very few samples) during training, it will maintain such domain generalization capacity for testing. It motivates us to propose a novel Domain-Switch Learning (DSL) framework. DSL embeds the cross-domain scenario into the training stage in a “fast switching” manner. Speciﬁcally, DSL uses a single domain for a training iteration and switches into another domain for the following iteration. During the switching, DSL enforces two constraints: 1) the deep model should not over-ﬁt the domain in the current iteration and 2) the deep model should not forget the already-learned knowledge of other domains. These two constraints jointly promote fast generalization across different domains. Experimental results conﬁrm that the cross-domain generalization capacity can be inherited from the training stage to the testing stage, validating our key hypothesis. Consequentially, DSL signiﬁcantly improves cross-domain few-shot classiﬁcation and sets up new state of the art.