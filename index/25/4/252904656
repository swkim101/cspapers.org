In the scenarios of low-resource natural language applications, Semi-supervised Relation Extraction (SRE) plays a key role in mitigating the scarcity of labelled sentences by harnessing a large amount of unlabeled corpus. Current SRE methods are mainly designed based on the paradigm of Self-Training with Validation (STV), which employs two learners and each of them plays the single role of annotator or validator. However, such a single role setting under-utilizes the potential of learners in promoting new labelled instances from unlabeled corpus. In this paper, we propose a generic SRE paradigm, called Co-Training with Validation (CTV), for making full use of learners to benefit more from unlabeled corpus. In CTV, each learner alternately plays the roles of annotator and validator to generate and validate pseudo-labelled instances. Thus, more high-quality instances are exploited and two learners can be reinforced by each other during the learning process. Experimental results on two public datasets show that our CTV considerably outperforms the state-of-the-art SRE techniques, and works well with different kinds of learners for relation extraction.