Learned classifiers should often possess certain invariance properties meant to en-1 courage fairness, robustness, or out-of-distribution generalization. Multiple recent 2 works empirically demonstrate that common invariance-inducing regularizers are 3 ineffective in the over-parameterized regime, in which classifiers perfectly fit (i.e. 4 interpolate) the training data. In this work we provide a theoretical justification for 5 these observations. We prove that - even in the simplest of settings - any interpo-6 lating classifier (with nonzero margin) will not satisfy these invariance properties. 7 We then propose and analyze an algorithm that - in the same setting - successfully 8 learns a non-interpolating classifier that is provably invariant. Validation of our 9 theoretical observations is performed on simulated data and the Waterbirds dataset