Barlow Twins and VICReg are self-supervised representation learning models that use regularizers to decorrelate features. Although these models are as effective as conventional representation learning models, their training can be computationally demanding if the dimension $d$ of the projected embeddings is high. As the regularizers are defined in terms of individual elements of a cross-correlation or covariance matrix, computing the loss for $n$ samples takes $O(nd^{2})$ time. In this paper, we propose a relaxed decorre-lating regularizer that can be computed in $O(nd\log d)$ time by Fast Fourier Transform. We also propose an inexpensive technique to mitigate undesirable local minima that develop with the relaxation. The proposed regularizer exhibits accuracy comparable to that of existing regularizers in down-stream tasks, whereas their training requires less memory and is faster for large $d$. The source code is available. 11https://github.com/yutaro-s/scalable-decorrelation-ssl.git