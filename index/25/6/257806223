We describe an application of Knowledge Dis-tillation used to distill and deploy multilingual Transformer models for voice assistants, enabling text classiﬁcation for customers globally. Transformers have set new state-of-the-art results for tasks like intent classiﬁcation, and multilingual models exploit cross-lingual transfer to allow serving requests across 100+ languages. However, their prohibitive inference time makes them impractical to deploy in real-world scenarios with low latency requirements, such as is the case of voice assistants. We address the problem of cross-architecture distillation of multilingual Transformers to simpler models, while maintaining multi-linguality without performance degradation. Training multilingual student models has received little attention, and is our main focus. We show that a teacher-student framework, where the teacher’s unscaled activations (log-its) on unlabelled data are used to supervise student model training, enables distillation of Transformers into efﬁcient multilingual CNN models. Our student model achieves equivalent performance as the teacher, and outperforms a similar model trained on the labelled data used to train the teacher model. This approach has enabled us to accurately serve global customer requests at speed (18x improvement), scale, and low cost.