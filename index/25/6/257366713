As computer science is integrated into a wider variety of fields, block-based programming languages like Snap!, which assemble code with visual blocks rather than text syntax, are increasingly used to teach computational thinking (CT) to students from diverse backgrounds. Although automated evaluators (autograders) for programming assignments usually focus on runtime efficiency and output accuracy, effective evaluation of a student's CT skills requires assessing coding best practices, such as decomposition, abstraction, and algorithm design. While autograders are commonplace for text languages like Python, we present a machine learning approach to assess how effectively block-based code demonstrates understanding of CT fundamentals. Our dataset consists of Snap! programs written by students new to coding and evaluated by instructors using a CT rubric. We explore how to best transform these programs into low-dimensional features to allow encapsulation and repetition patterns to emerge. Experimentation involves comparing the effectiveness of a suite of clustering models and similarity metrics by analyzing how directly automated feedback correlates to the course staff's manual evaluation. Lastly, we demonstrate the practical application of the autograder in a classroom setting and discuss scalability and feasibility in other domains of CS education.