. Grasp pose estimation is an important issue for robots to interact with the real world. However, most of existing methods require exact 3D object models available beforehand or a large amount of grasp annotations for training. To avoid these problems, we propose TransGrasp, a category-level grasp pose estimation method that predicts grasp poses of a category of objects by labeling only one object instance. Specif-ically, we perform grasp pose transfer across a category of objects based on their shape correspondences and propose a grasp pose reﬁnement module to further ﬁne-tune grasp pose of grippers so as to ensure successful grasps. Experiments demonstrate the eﬀectiveness of our method on achieving high-quality grasps with the transferred grasp poses. Our code is available at https://github.com/yanjh97/TransGrasp.