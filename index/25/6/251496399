As machine learning has become increasingly ubiquitous, there has been a growing need to assess the trustworthiness of learned models. One important aspect to model trust is conceptual soundness, i.e., the extent to which a model uses features that are appropriate for its intended task. Deep networks have quickly become the face of modern machine learning, with unparalleled success at complex human tasks such as vision and natural language processing. However, deep networks are notoriously opaque, further emphasizing the need for transparency into their internal logic. A large body of work has arisen to address this problem, by providing explanations that distill aspects of a model’s behavior to be better understood by human practitioners. In this demonstration,1 we present TruLens, a new cross-platform library for explaining deep network behavior that implements a general class of gradient-based explanations captured by the “influence-directed” explanation framework of Leino et al. (2018). Throughout our presentation, we will take the unique perspective that to accurately assess the conceptual soundness of a model, an explanation must be faithful—i.e., the explanation must be causally related to the model’s behavior. By contrast, the literature has often attempted to justify explanations based on their appeal to human intuition. However, this begs the question, as it assumes the model captured human intuition in the first place. Instead, we argue that the utility of an explanation framework comes from its flexibility to faithfully answer a wide range of queries, but not from its tendency to produce reasonable, visually-appealing, or intuitive explanations. Our demonstration will show that faithful explanations can surface erroneous model behavior that may not be manifested in the validation set, and would therefore otherwise go unnoticed prior to model deployment. Thus, conversely, faithful explanations that align with our expectations of conceptually sound predictions, serve as evidence that the model is trustworthy. Finally, we observe that erroneous behavior caused by adversarial examples (Szegedy et al., 2014) is indicative of a lack of conceptual soundness. As adversarial examples are ubiquitous in standard deep networks, this observation suggests that robustness to adversarial examples is necessary for establishing conceptual soundness.