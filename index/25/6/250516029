LiDAR sensors are becoming crucial for achieving higher levels of autonomy. With the current sensor technology, LiDAR sensors are still susceptible to erroneous measurements in adverse weather conditions due to weather artifacts observed in the point cloud data. In this work, we analyze the performance of deep learning LiDAR object detectors in adverse weather conditions. We study the under-researched albeit crucial aspect of deep learning - the size and the content of the training data, as we believe that efforts in data curation bring more benefit than often preferred complex algorithmic enhancements. We argue that with sufficient data, learning-based object detectors are inherently capable of distinguishing between points from a true object and weather-induced artifacts in an end-to-end manner, thus, making explicit, handcrafted and computationally expensive point cloud prefiltering steps obsolete. We corroborate our hypothesis by conducting experiments on a variety of LiDAR object detection architectures over two subsets of training data - one comprising of all weather conditions and the other one comprising of only clear conditions. Contrary to popular belief that object detectors need to be trained on data from adverse weather conditions to be performant in adverse weather, we show that training on datasets with a higher number of annotated objects, predominantly acquired in clear conditions, is sufficient to achieve almost similar or sometimes better KPIs across all weather conditions. This makes the data collection more accessible and convenient compared to adverse weather conditions that are often hardly accessible (e.g., heavy snow and fog). Finally, our proposed methodology streamlines the LiDAR perception pipeline in the direction of keeping well-known end-to-end trainable architectures, removing additional pre-processing blocks that are often handcrafted and bring an additional computational cost.