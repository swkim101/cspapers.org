We revisit the question of why neural models tend to produce high-conﬁdence predictions on inputs that appear nonsensical to humans. Previous work has suggested that the models fail to assign low probabilities to such inputs due to model overconﬁdence. We evaluate various regularization methods on fact ver-iﬁcation benchmarks and ﬁnd that this problem persists even with well-calibrated or un-derconﬁdent models, suggesting that overcon-ﬁdence is not the only underlying cause. We also ﬁnd that regularizing the models with reduced examples helps improve interpretability but comes with the cost of miscalibration. We show that although these reduced examples are incomprehensible to humans, they can contain valid statistical patterns in the dataset utilized by the model. 1