Concept bottleneck models (CBMs) enhance the interpretability of their predictions by ﬁrst predicting high-level concepts given features, and subsequently predicting outcomes on the basis of these concepts. Recently, it was demonstrated that training the label predictor directly on the probabilities produced by the concept predictor as opposed to the ground-truth concepts, improves label predictions. However, this results in corruptions in the concept predictions that impact the concept accuracy as well as our ability to intervene on the concepts – a key proposed beneﬁt of CBMs. In this work, we investigate and address two issues with CBMs that cause this disparity in performance: having an insufﬁcient concept set and using inexpressive concept predictor. With our modiﬁcations, CBMs become competitive in terms of predictive performance, with models that otherwise leak unintended information in the concept probabilities, while having dramatically increased concept accuracy and intervention accuracy.