Deep learning models have been found with a tendency of relying on shortcuts, i.e., decision rules that perform well on standard benchmarks but fail when transferred to more challenging testing conditions. Such reliance may hinder deep learning models from learning other task-related features and seriously affect their performance and robustness. Although recent studies have shown some characteristics of shortcuts, there are few investigations on how to help the deep learning models to solve shortcut problems. This paper proposes a framework to address this issue by setting up roadblocks on shortcuts. Speciﬁcally, roadblocks are placed when the model is urged to learn to complete a gently modiﬁed task to ensure that the learned knowledge, including shortcuts, is insufﬁcient the complete the task. Therefore, the model trained on the modiﬁed task will no longer over-rely on shortcuts. Extensive experiments demonstrate that the proposed framework signiﬁcantly improves the training of networks on both synthetic and real-world datasets in terms of both classiﬁcation accuracy and feature diversity. Moreover, the visualization results show that the mechanism behind the proposed our method is consistent with our expectations. In summary, our approach can effectively disable the shortcuts and thus learn more robust features.