Machine learning models are often deployed in settings where they must be constantly updated in response to the changes in class deﬁnitions while retaining high accuracy on previously learned deﬁnitions. A classical use case is fraud detection, where new fraud schemes come one after another. While such an update can be accomplished by re-training on the complete data, the process is inefﬁcient and prevents real-time and on-device learning. On the other hand, efﬁcient methods that incrementally learn from new data often result in the forgetting of previously-learned knowledge. We deﬁne this problem as Learning with Dynamic Deﬁnition (LDD) and demonstrate that popular models, such as the Vision Transformer and Roberta, exhibit substantial forgetting of past deﬁnitions. We present a ﬁrst practical and provable solution to LDD. Our proposal is a hash-based sparsity model RIDDLE that solves evolving deﬁnitions by associating samples only to relevant parameters. We prove that our model is a universal function approximator and theoretically bounds the knowledge lost during the update process. On practical tasks with evolving class deﬁnition in vision and natural language processing, RID-DLE outperforms baselines by up to 30% on the original dataset while providing competitive accuracy on the update dataset.