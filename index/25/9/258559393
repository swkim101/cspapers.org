Model serving systems observe massive volumes of inference requests for many emerging interactive web services. These systems need to be scalable, guarantee high system goodput and maximize resource utilization across compute units. However, achieving all three goals simultaneously is challenging since inference requests have very tight latency constraints (10–500ms), and production workloads can be extremely unpredictable at such small time granularities. We present S HEPHERD , a model serving system that achieves all three goals in the face of workload unpredictability. S HEPHERD uses a two-level design that decouples model serving into planning and serving modules. For planning, S HEPHERD exploits the insight that while individual request streams can be highly unpredictable, aggregating request streams into moderately-sized groups greatly improves pre-dictability, permitting high resource utilization as well as scalability. For serving, S HEPHERD employs a novel online algo-rithm that provides guaranteed goodput under workload un-predictability by carefully leveraging preemptions and model-specific batching properties. Evaluation results over production workloads show that S HEPHERD achieves up to 18 . 1 × higher goodput and 1 . 8 × better utilization compared to prior state-of-the-art, while scaling to hundreds of workers.