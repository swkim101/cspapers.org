Previous works show global covariance pooling (GCP) has great potential to improve deep architectures especially on visual recognition tasks, where post-normalization of GCP plays a very important role in ﬁnal performance. Although several post-normalization strategies have been studied, these methods pay more close attention to effect of normalization on covariance representations rather than the whole GCP networks, and their effectiveness requires further understanding. Meanwhile, existing effective post-normalization strategies (e.g., matrix power normalization) usually suffer from high computational complexity (e.g., O ( d 3 ) for d -dimensional inputs). To handle above issues, this work ﬁrst analyzes the effect of post-normalization from the perspective of training GCP networks. Particularly, we for the ﬁrst time show that effective post-normalization can make a good trade-off between representation decorrelation and information preservation for GCP, which are crucial to alleviate over-ﬁtting and increase representation ability of deep GCP networks, respectively . Based on this ﬁnding, we can improve existing post-normalization methods with some small modiﬁcations, providing further support to our observation. Furthermore, this ﬁnding encourages us to propose a novel pre-normalization method for GCP (namely DropCov), which develops an adaptive channel dropout on features right before GCP, aiming to reach trade-off between representation decorrelation and information preservation in a more efﬁcient way. Our DropCov only has a linear complexity of O ( d ) , while being free for inference.