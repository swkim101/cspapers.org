In this study, we analyze the model intrinsic features of a summarization model by varying the ﬁne-tuning objectives and datasets. We ﬁne-tune BART models combining three ﬁne-tuning objectives (negative log-likelihood, unlikeli-hood, and contrastive loss) and two datasets (CNN/DailyMail and XSum) and provide shuf-ﬂed or aligned documents to observe changes in the model predictions and intrinsic features. We ﬁnd that ( i ) the inductive bias for factual consistency during the ﬁne-tuning procedure depends on both the objectives and datasets, and ( ii ) summarization models with relatively low factual consistency are more likely to model summaries that are not conditional to the documents. We demonstrate that splitting data based on the unconditional and conditional summary modeling difﬁculty affects the factual consistency and intrinsic features of the summarization models. Our experimental results high-light the importance of studying the inductive bias during ﬁne-tuning for factual consistency.