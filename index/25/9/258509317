Many practical settings allow a classiﬁer to defer predictions to one or more costly experts . For example, the learning to defer paradigm allows a classiﬁer to defer to a human expert, at some monetary cost. Similarly, the adaptive inference paradigm allows a base model to defer to one or more large models, at some computational cost. The goal in these settings is to learn classiﬁcation and deferral mechanisms to optimise a suitable accuracy-cost tradeo � . To achieve this, a central issue studied in prior work is the design of a coherent loss function for both mechanisms. In this work, we demonstrate that existing losses can underﬁt the training set when there is a non-trivial deferral cost, owing to an implicit application of a high level of label smoothing. To resolve this, we propose two post-hoc estimators that ﬁt a deferral function on top of a base model, either by threshold correction, or by learning when the base model’s error rate exceeds the cost of deferring to the expert. Both approaches are equipped with theoretical guarantees, and empirically yield e � ective accuracy-cost tradeo � s on learning to defer and adaptive inference benchmarks.