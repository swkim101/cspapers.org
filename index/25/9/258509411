We introduce the ﬁrst direct policy search algorithm which provably converges to the globally optimal dynamic ﬁlter for the classical problem of predicting the outputs of a linear dynamical system, given noisy, partial observations. Despite the ubiquity of partial observability in practice, theoretical guarantees for direct policy search algorithms, one of the backbones of modern reinforcement learning, have proven difﬁcult to achieve. This is primarily due to the degeneracies which arise when optimizing over ﬁlters that maintain an internal state. In this paper, we provide a new perspective on this challenging problem based on the notion of informativity , which intuitively requires that all components of a ﬁlter’s internal state are representative of the true state of the underlying dynamical system. We show that informativity overcomes the aforementioned degeneracy. Speciﬁcally, we propose a regularizer which explicitly enforces informativity, and establish that gradient descent on this regularized objective – combined with a “reconditioning step” – converges to the globally optimal cost at a O (1 /T ) rate.