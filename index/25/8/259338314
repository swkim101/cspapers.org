In this paper, we present a Radar-Inertial Odometry (RIO) approach that utilizes performance improving modules, enhanced for the sparse and noisy radar signals, from the vision community in order to estimate the full 6DoF pose and 3D velocity of a robot in an unprepared environment. Our method leverages a multi-state approach in which we make use of several past robot poses and trails of measurements from a lightweight and inexpensive Frequency Modulated Continuous Wave (FMCW) radar sensor. Furthermore, in our estimation framework we include a method for promoting measurement trails to persistent landmarks which correspond to salient features in the environment. In an Extended Kalman Filter (EKF) framework, we fuse the range measurements to the persistent landmarks, trails, and the velocity measurements of the detected 3D points together with the Inertial Measurement Unit (IMU) readings. Our method is particularly relevant for (but not limited to) Unmanned Aerial Vehicles (UAV), enabling them to localize while performing missions in Global Navigation Satellite System (GNSS)-denied environments and, thanks to the properties of the radar sensor, in environments generally challenging for robot perception due to external factors such as smoke or extreme illumination. We show in real flight experiments the effectiveness of our estimator and compare it to the state-of-the-art.