Semi-supervised learning is a popular machine learning paradigm that utilizes 1 a large amount of unlabeled data as well as a small amount of labeled data to 2 facilitate learning tasks. While semi-supervised learning has achieved great success 3 in training neural networks, its theoretical understanding remains largely open. In 4 this paper, we aim to theoretically understand a semi-supervised learning approach 5 based on pre-training and linear probing. We prove that, under a certain data 6 generation model and two-layer convolutional neural network, the semi-supervised 7 learning approach can achieve nearly zero test loss, while a neural network directly 8 trained by supervised learning on the same amount of labeled data can only achieve 9 constant test loss. Through this case study, we demonstrate a separation between 10 semi-supervised learning and supervised learning in terms of test loss provided the 11 same amount of labeled data