Pretrained multilingual Transformers have achieved great success in cross-lingual transfer learning. Current methods typically activate the cross-lingual transferability of multi-lingual Transformers by fine-tuning them on end-task data. However, the methods cannot perform cross-lingual transfer when end-task data are unavailable. In this work, we explore whether the cross-lingual transferability can be activated without end-task data. We pro-pose a cross-lingual transfer method, named P LUG I N -X. P LUG I N -X disassembles mono-lingual and multilingual Transformers into sub-modules, and reassembles them to be the multilingual end-task model. After representation adaptation, P LUG I N -X finally performs cross-lingual transfer in a plug-and-play style. Experimental results show that P LUG I N -X successfully activates the cross-lingual transferability of multilingual Transformers without accessing end-task data. Moreover, we analyze how the cross-model representation alignment affects the cross-lingual transferability.