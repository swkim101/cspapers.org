Most text-to-SQL models, even though based on the same grammar decoder 1 , generate the SQL structure first and then fill in the SQL slots with the correct schema items. This second step depends on schema linking : aligning the entity references in the question with the schema columns or tables. This is generally approached via E xact M atch based S chema L inking ( EMSL ) within a neural network-based schema linking module. EMSL has become standard in text-to-SQL: many state-of-the-art models employ EMSL, with performance dropping significantly when the EMSL component is removed. In this work, however, we show that EMSL reduces robustness, rendering models vulnerable to synonym substi-tution and typos. Instead of relying on EMSL to make up for deficiencies in question-schema encoding, we show that using a pre-trained language model as an encoder can improve performance without using EMSL, giving a more robust model. We also study the design choice of the schema linking module, finding that a suitable design benefits performance and inter-pretability. Finally, based on the above study of schema linking, we introduce the grammar linking to help model align grammar references in the question with the SQL keywords. 2