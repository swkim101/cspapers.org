Deep neural networks (DNNs) have been proven to be sensitive towards perturbations on input samples, and previous works high-light that adversarial samples are even more vulnerable than normal ones. In this work, this phenomenon is illustrated from the perspective of sharpness via visualizing the input loss landscape of models. We ﬁrst show that adversarial samples locate in steep and narrow local minima of the loss landscape ( high sharpness ) while normal samples, which differs distinctly from adversarial ones, reside in the loss surface that is more ﬂatter ( low sharpness ). Based on this, we propose a simple and effective sharpness-based detector to distinct adversarial samples by maximizing the loss increment within the region where the inference sample is located. Considering that the notion of sharpness of a loss landscape is relative, we further propose an adaptive optimization strategy in an attempt to fairly compare the relative sharpness among different samples. Experimental results show that our approach can outperform previous detection methods by large margins (average +6.6 F1 score) for four advanced attack strategies considered in this paper across three text classiﬁcation tasks. Our codes are publicly available at https://github