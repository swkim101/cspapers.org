Catastrophic forgetting and the stability-plasticity dilemma are two major obstacles to continual learning. In this paper, we ﬁrst propose a theoretical analysis of a SPCA-based continual learning algorithm using high-dimensional statistics. Second, we design OSCL (Optimized Spca-based Continual Learning) which builds on a ﬂexible task optimization based on the theory. By optimizing a single task, catastrophic forgetting can be prevented theoretically. While optimizing multi-tasks, the trade-off between integrating knowledge from the new task and retaining previous knowledge of the old tasks can be achieved by assigning appropriate weights to corresponding tasks in compliance with the objectives. Experimental results conﬁrm that the various theoretical conclusions are robust to a wide range of data distributions. Besides, several applications on synthetic and real data show that the proposed method while being computationally efﬁcient, achieves comparable results with some state of the art.