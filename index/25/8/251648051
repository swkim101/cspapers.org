We present a novel framework to train a large deep neural network (DNN) for only once , which can then be pruned to any sparsity ratio to preserve competitive accuracy without any re-training . Conventional methods often require (iterative) pruning followed by re-training, which not only incurs large overhead beyond the original DNN training but also can be sensitive to retraining hyperparameters. Our core idea is to re-cast the DNN training as an explicit pruning-aware process: that is formulated with an auxiliary K -sparse polytope constraint, to encourage network weights to lie in a convex hull spanned by K -sparse vectors, potentially resulting in more sparse weight matrices. We then leverage a stochastic Frank-Wolfe (SFW) algorithm to solve this new constrained optimization, which naturally leads to sparse weight updates each time. We further note an overlooked fact that existing DNN initializations were derived to enhance SGD training (e.g., avoid gradient explosion or collapse), but was unaligned with the challenges of training with SFW. We hence also present the ﬁrst learning-based initialization scheme speciﬁcally for boosting SFW-based DNN training. Experiments on CIFAR-10 and Tiny-ImageNet datasets demonstrate that our new framework named SFW-pruning consistently achieves the state-of-the-art performance on various benchmark DNNs over a wide range of pruning