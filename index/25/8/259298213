We develop a new approximation and statistical estimation analysis of deep feed-forward neural networks (FNNs) with the Rectified Linear Unit (ReLU) activation. The functions of interests for the approximation and estimation are assumed to be from Sobolev spaces defined over the d -dimensional unit sphere with smoothness index r > 0 . In the regime where r is of the constant order (i.e., r = O (1) ), it is shown that at most d d active parameters are required for getting d − C approximation rate for some constant C > 0 . In the regime where the index r grows in the order of d (i.e., r = O ( d ) ) asymptotically, we prove the approximation error decays in the rate d − d β with 0 < β < 1 up to some constant factor independent of d . The required number of active parameters in the networks for the approximation increases polynomially in d as d → ∞ . It is also shown that bound on the excess risk has a d d factor, when r = O (1) , whereas it has d O (1) factor, when r = O ( d ) . We emphasize our findings by making comparisons to the results on the approximation and estimation errors of deep ReLU FNN when functions are from Sobolev spaces defined over d -dimensional cube. In this case, we show that with the current state-of-the-art result, d d factor remain both in the approximation and estimation errors, regardless of the order of r .