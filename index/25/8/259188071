We investigate the impact of different tokenizers on downstream performance in Japanese NLP, with the case of BERT architecture.