Motivated by applications to process massive datasets, we study streaming algorithms for pure exploration in Stochastic Multi Armed Bandits (MABs). This problem was ﬁrst formulated by Assadi and Wang [STOC 2020] as follows: A collection of n arms with unknown rewards are arriving one by one in a stream, and the algorithm is only allowed to store a limited number of arms at any point. The goal is to ﬁnd the arm with the largest reward while minimizing the number of arm pulls (sample complexity) and the maximum number of stored arms (space complexity). Assuming ∆ [2] is known, Assadi and Wang designed an algorithm that uses a memory of just one arm and still achieves the sample complexity of O ( n/ ∆ 2[2] ) which is worst-case optimal even for non-streaming algorithms; here ∆ [ i ] is the gap between the rewards of the best and the i -th best arms. In this paper, we extended this line of work to stochastic MABs in the streaming model with the instance-sensitive sample complexity, i.e. the sample complexity of O ( (cid:80) ni =2 1∆ 2[ i ] log log ( 1∆ [ i ] )) , similar in spirit to Karnin et.al. [ICML 2013] and Jamieson et.al. [COLT 2014] in the classical setting. We devise strong negative results under this setting: our results show that any streaming algorithm under a single pass has to use either asymptotically higher sample complexity than the instance-sensitive bound, or a memory of Ω( n ) arms, even if the parameter ∆ [2] is known. In fact, the lower bound holds under much stronger assumptions, including the random order streams or the knowledge of all gap parameters { ∆ [ i ] } ni =2 . We complement our lower bounds by proposing a new algorithm that uses a memory of a single arm and