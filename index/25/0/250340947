Many bandit problems are characterized by the learner making decisions under constraints. The learner in Linear Contextual Bandits with Knap-sacks (LinCBwK) receives a resource consumption vector in addition to a scalar reward in each time step which are both linear functions of the context corresponding to the chosen arm. For a ﬁxed time horizon T , the goal of the learner is to maximize rewards while ensuring resource consumptions do not exceed a pre-speciﬁed budget. We present algorithms and characterize regret for LinCBwK in the smoothed setting where base context vectors are assumed to be perturbed by Gaussian noise. We consider both the stochastic and adversarial settings for the base contexts, and our analysis of stochastic LinCBwK can be viewed as a warm-up to the more challenging adversarial LinCBwK. For the stochastic setting, we obtain O p? T q additive regret bounds compared to the best context dependent ﬁxed policy. The analysis combines ideas for greedy parameter estimation in (Kannan et al., 2018; Sivakumar et al., 2020) and the primal-dual paradigm ﬁrst explored in (Agrawal & Devanur, 2016; 2014a). Our main contribution is an algorithm with O p log T q competitive ratio relative to the best context dependent ﬁxed policy for the adversarial setting. The algo-rithm for the adversarial setting employs ideas from the primal-dual framework (Agrawal & De-vanur, 2016; 2014a) and a novel adaptation of the doubling trick (Immorlica et al., 2019).