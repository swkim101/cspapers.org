Normalizing ﬂows map an independent set of latent variables to their samples using a bijective transformation. Despite the exact correspondence between samples and latent variables, their high level relationship is not well understood. In this paper we characterize the geometric structure of ﬂows using principal manifolds and understand the relationship between latent variables and samples using contours. We introduce a novel class of normalizing ﬂows, called principal component ﬂows (PCF), whose contours are its principal manifolds, and a variant for injective ﬂows (iPCF) that is more efﬁcient to train than regular injective ﬂows. PCFs can be constructed using any ﬂow architecture, are trained with a regularized maximum likelihood objective and can perform density estimation on all of their principal manifolds. In our experiments we show that PCFs and iPCFs are able to learn the principal manifolds over a variety of datasets. Additionally, we show that PCFs can perform density estimation on data that lie on a manifold with variable dimensionality, which is not possible with existing normalizing ﬂows.