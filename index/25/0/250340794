Recently, some accelerated stochastic variance reduction algorithms such as Katyusha and ASVRG-ADMM achieve faster convergence than non-accelerated methods such as SVRG and SVRG-ADMM. However, there are still some gaps between the oracle complexities and their lower bounds. To ﬁll in these gaps, this paper propos-es a novel Directly Accelerated stochastic Variance reductIon algorithm with two Snapshots (DAVIS) for non-strongly convex (non-SC) unconstrained problems. Our theoretical results show that DAVIS achieves the optimal convergence rate O (1 / ( nS 2 )) and optimal gradient complexity O ( n + (cid:112) nL/(cid:15) ) , which is identical to its lower bound. To the best of our knowledge, this is the ﬁrst directly accelerated algorithm that attains the lower bound and improves the convergence rate from O (1 /S 2 ) to O (1 / ( nS 2 )) . More-over, we extend DAVIS and theoretical results to non-SC problems with an equality constraint, and prove that the proposed DAVIS-ADMM algorithm with double snapshots for each variable also attains the optimal convergence rate O (1 / ( nS )) and optimal oracle complexity O (cid:0) n + L/(cid:15) (cid:1) for such problems, and it is at least by a factor n/S faster than existing accelerated stochastic algorithms,