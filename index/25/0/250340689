Recent works have examined theoretical and empirical properties of wide neural networks trained in the Neural Tangent Kernel (NTK) regime. Given that biological neural networks are much wider than their artiﬁcial counterparts, we consider NTK regime wide neural networks as a possible model of biological neural networks. Lever-aging NTK theory, we show theoretically that gradient descent drives layerwise weight updates that are aligned with their input activity corre-lations weighted by error, and demonstrate empirically that the result also holds in ﬁnite-width wide networks. The alignment result allows us to formulate a family of biologically-motivated, backpropagation-free learning rules that are theoretically equivalent to backpropagation in inﬁnite-width networks. We test these learning rules on benchmark problems in feedforward and recurrent neural networks and demonstrate, in wide networks, comparable performance to backpropagation. The proposed rules are particularly effective in low data regimes, which are common in biological learning settings.