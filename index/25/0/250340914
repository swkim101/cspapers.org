Deep neural networks (DNNs) have revealed severe vulnerability to adversarial perturbations, beside empirical adversarial training for robustness, the design of provably robust classiﬁers attracts more and more attention. Randomized smoothing methods provide the certiﬁed robustness with agnostic architecture, which is further extended to a provable robustness framework using f-divergence. While these methods cannot be applied to smoothing measures with bounded support set such as uniform probability measure due to the use of likelihood ratio in their certiﬁcation methods. In this paper, we generalize the f -divergence-based framework to a Wasserstein-distance-based and total-variation-distance-based framework that is ﬁrst able to analyze robustness properties of bounded support set smoothing measures both theoretically and experimentally. By applying our methodology to uniform probability measures with support set l p ( p = 1 , 2 , ∞ and general ) ball, we prove negative certiﬁed robustness properties with respect to l q ( q = 1 , 2 , ∞ ) perturbations and present experimental results on CIFAR-10 dataset with ResNet to validate our theory. And it is also worth mentioning that our certiﬁcation procedure only costs constant computation time.