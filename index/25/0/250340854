We study decision making problems in which an agent sequentially interacts with a stochastic environment deﬁned by means of a tree structure . The agent repeatedly faces the environment over time, and, after each round, it perceives a utility and a cost , which are both stochastic. The goal of the agent is to learn an optimal strategy in an online fashion, while keeping costs below a given safety threshold at the same time. Our model naturally ﬁts many real-world scenarios, such as, e.g. , opponent exploitation in games and web link selection. We study the hard-threshold problem of achieving sublinear regret while guaranteeing that the threshold constraint is satisﬁed at every iteration with high probability. First, we show that, in general, any algorithm with such a guarantee incurs in a linear regret. This motivates the introduction of a relaxed problem, called the soft-threshold problem, in which we only require that the cumulative violation of the threshold constraint grows sublin-early, and, thus, we can provide an algorithm with sublinear regret. Next, in the hard-threshold problem, we show how a sublinear regret algorithm can be designed under the additional assumption that there exists a known strategy strictly satisfying the threshold constraint. We also show that our regret bounds are tight. Finally, we cast the opponent exploitation problem to our model, and we experimentally evaluate our algorithms on a standard testbed of sequential games.