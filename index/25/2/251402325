. Egocentric videos offer fine-grained information for high-fidelity modeling of human behaviors. Hands and interacting objects are one crucial aspect of understanding a viewer’s behaviors and in-tentions. We provide a labeled dataset consisting of 11,243 egocentric images with per-pixel segmentation labels of hands and objects being interacted with during a diverse array of daily activities. Our dataset is the first to label detailed hand-object contact boundaries. We introduce a context-aware compositional data augmentation technique to adapt to out-of-distribution YouTube egocentric video. We show that our robust hand-object segmentation model and dataset can serve as a foundational tool to boost or enable several downstream vision applications, including hand state classification, video activity recognition, 3D mesh reconstruction of hand-object interactions, and video inpainting of hand-object foregrounds in egocentric videos. Dataset and code are available at: https://github.com/owenzlz/EgoHOS In this supplementary materials, we first describe the following details of this work: 1). details of video frame collection; 2). training details of segmentation network; 3). how we chose the quantity of data augmentation; 4). the performance of using 100-DOH followed by PointRend. Please note that we also provide results of per-frame hand-object segmentations and ”see-through hand” application in egocentric videos. Please check our ”.mp4” file in the supplementary materials for details.