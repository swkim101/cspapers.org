Today, the classiﬁcation of a ﬁle as either benign or malicious is performed by a combination of deterministic indicators (such as antivirus rules), Machine Learning classiﬁers, and, more importantly, the judgment of human experts. However, to compare the difference between human and machine intelligence in malware analysis, it is ﬁrst necessary to understand how human subjects approach malware classiﬁcation. In this direction, our work presents the ﬁrst experimental study designed to capture which ‘features’ of a suspicious program (e.g., static properties or runtime behaviors) are prioritized for malware classiﬁcation according to humans and machines intelligence. For this purpose, we created a malware classiﬁcation game where 110 human players worldwide and with different seniority levels (72 novices and 38 experts) have competed to classify the highest number of unknown samples based on detailed sandbox reports. Surpris-ingly, we discovered that both experts and novices base their decisions on approximately the same features, even if there are clear differences between the two expertise classes. Furthermore, we implemented two state-of-the-art Machine Learning models for malware classiﬁcation and evaluated their performances on the same set of samples. The comparative analysis of the results unveiled a common set of features preferred by both Machine Learning models and helped better understand the difference in the feature extraction. This work reﬂects the difference in the decision-making process of humans and computer algorithms and the different ways they extract information from the same data. Its ﬁndings serve multiple purposes, from training better malware analysts to improving feature encoding.