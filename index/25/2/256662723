To support efficient, balanced news consumption, merging articles from diverse sources into one, potentially through crowdsourcing, could alleviate some hurdles. However, the merging process could also impact annotators’ attitudes towards the content. To test this theory, we propose comparative news annotation; that is, annotating similarities and differences between a pair of articles. By developing and deploying NewsComp—a prototype system—we conducted a between-subjects experiment (N = 109) to examine how users’ annotations compare to experts’, and how comparative annotation affects users’ perceptions of article credibility and quality. We found that comparative annotation can marginally impact users’ credibility perceptions in certain cases; it did not impact perceptions of quality. While users’ annotations were not on par with experts’, they showed greater precision in finding similarities than in identifying disparate important statements. The comparison process also led users to notice differences in information placement and depth, degree of factuality/opinion, and empathetic/inflammatory language use. We discuss implications for the design of future comparative annotation tasks.