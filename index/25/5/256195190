Real-time disaster-induced human fatality information is critical for rapid and accurate disaster impact and loss estimation and effective emergency response. Systems like PAGER incorporate online reported death tolls and loss projection models trained on significant historical earthquake events and ground shaking data to provide projected final seismic loss estimations. However, the input reported death toll data are mainly retrieved from news platforms manually, which is time-consuming and may have a large time bias. In recent years, platforms such as Facebook and Twitter have become hot spots for witness reporting and communication during disaster events, producing large volumes of immediate fatality information without the hindrances of official channels. Though lucrative, social media data is very noisy both in syntax and accuracy, necessitating robust solutions. In this work, we design and deploy a new online system that automatically extracts near-realtime multi-lingual human fatality information including death tolls and injury tolls, from a variety of information sources immediately after an earthquake occurs. Past studies have proposed to use popular machine learning methods such as SVMs, CNNs and Logistic Regression in conjunction with word embeddings to classify the relevancy of each social media message. However, these techniques suffer from impeding requirements of annotated data, which are unavailable at the onset of natural disasters, and cannot directly extract disaster information, instead relying on statistical analysis on their classification results. To address such challenges, we propose a Large Language Model-based approach that leverages its robust language understanding and few-shot learning abilities. In combination with our novel multilingual Hierarchical Event Classifier, another contribution, we achieve effective automatic earthquake casualty information retrieval from social media, which we test by deploying our framework to two recent earthquakes.