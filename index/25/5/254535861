Deep operator network (DeepONet) has demonstrated great
success in various learning tasks, including learning solution
operators of partial differential equations. In particular, it provides
 an efficient approach to predicting the evolution equations
in a finite time horizon. Nevertheless, the vanilla DeepONet
suffers from the issue of stability degradation in the long-
time prediction. This paper proposes a transfer-learning aided
DeepONet to enhance the stability. Our idea is to use transfer
learning to sequentially update the DeepONets as the surro-
gates for propagators learned in different time frames. The
evolving DeepONets can better track the varying complexities
of the evolution equations, while only need to be updated by
efficient training of a tiny fraction of the operator networks.
Through systematic experiments, we show that the proposed
method not only improves the long-time accuracy of Deep-
ONet while maintaining similar computational cost but also
substantially reduces the sample size of the training set.