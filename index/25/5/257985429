Federated learning (FL) enables the collaborative training of machine learning models without sharing training data. Traditional FL heavily relies on a trusted centralized server. Although decentralized FL eliminates the dependence on a centralized server, it faces such issues as poisoning attacks and data representation leakage due to insufficient restrictions on the behavior of participants, and heavy communication costs in fully decentralized scenarios, i.e., peer-to-peer (P2P) settings. This work proposes a blockchainbased fully decentralized P2P framework for FL, called BlockDFL. It takes blockchain as the foundation, leveraging the proposed voting mechanism and a two-layer scoring mechanism to coordinate FL among participants without mutual trust, while effectively defending against poisoning attacks. Gradient compression is introduced to lower communication cost and to prevent data from being reconstructed from transmitted model updates. The results of extensive experiments conducted on two real-world datasets exhibit that BlockDFL obtains competitive accuracy compared to centralized FL and can defend against poisoning attacks while achieving efficiency and scalability. Especially when the proportion of malicious participants is as high as 40%, BlockDFL can still preserve the accuracy of FL, outperforming existing fully decentralized P2P FL frameworks based on blockchain.