We consider the problem of rank-1 low-rank approximation (LRA) in the matrix-vector product model under various Schatten norms:\begin{equation*}\min _{\|u\|_{2}=1}\left\|A\left(I-u u^{\top}\right)\right\|_{\mathcal{S}_{p}}\end{equation*}where $\|M\|_{\mathcal{S}_{p}}$ denotes the $\ell_{p}$ norm of the singular values of M. Given $\varepsilon\gt 0$, our goal is to output a unit vector v such that \begin{equation*}\left\|A\left(I-v v^{\top}\right)\right\|_{\mathcal{S}_{p}} \leqslant\left(1+\varepsilon\right) \min _{\|u\|_{2}=1}\left\|A\left(I-u u^{\top}\right)\right\|_{\mathcal{S}_{p}}\end{equation*}Our main result shows that Krylov methods (nearly) achieve the information-theoretically optimal1 number of matrix-vector products for Spectral $(p=\infty)$, Frobenius $(p=2)$ and Nuclear $(p=1)$ LRA. In particular, for Spectral LRA, we show that any algorithm requires $\Omega\left(\log (n) / \varepsilon^{1 / 2}\right)$ matrix-vector products, exactly matching the upper bound obtained by Krylov methods [40]. Our lower bound addresses Open Question 1 in [59], providing evidence for the lack of progress on algorithms for Spectral LRA and resolves Open Question 1.2 in [5]. Next, we show that for any fixed constant p, i.e. $1 \leqslant p=O(1)$, there is an upper bound of $O\left(\log (1 / \varepsilon) / \varepsilon^{1 / 3}\right)$ matrix-vector products, implying that the complexity does not grow as a function of input size. This improves the $O\left(\log (n / \varepsilon) / \varepsilon^{1 / 3}\right)$ bound recently obtained in [5], and matches their $\Omega\left(1 / \varepsilon^{1 / 3}\right)$ lower bound, to a $\log (1 / \varepsilon)$ factor.1For Spectral LRA, the upper and lower bounds match up to a fixed universal constant. For Frobenius and Nuclear LRA, they match up to a $\log (1 / \varepsilon)$ factor.