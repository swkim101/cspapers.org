This paper addresses the problem of sensor fusion in the context of visual localization, namely the combined usage of multiple vision sensors and visual odometry sources to estimate the position and orientation of an autonomous agent moving into an unknown environment. Focusing on the case of a redundant sensor configuration consisting of many data sources that may also inform about their functioning state, the essential characteristics of visual sensors are recalled, and an effective fusion system is developed. Its two-fold goal is to improve the overall estimate by combining the strengths of potentially different sensors, also adding fault resilience, while allowing for a simple implementation that relies on little information. A test setup that represents a realistic configuration of an autonomous robot is described, and the experimental results are presented and discussed.