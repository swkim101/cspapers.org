Tensor decomposition is a dominant framework for multiway data analysis and prediction. Although practical data often contains timestamps for the observed entries, existing tensor decomposition approaches overlook or under-use this valuable temporal information. They either drop the timestamps or bin them into crude steps and hence ignore the temporal dynamics within each step or use simple parametric time coefﬁcients. To overcome these limitations, we propose Bayesian Continuous-Time Tucker Decomposition (BCTT). We model the tensor-core of the classical Tucker decomposition as a time-varying function, and place a Gaussian process prior to ﬂexibly estimate all kinds of temporal dynamics. In this way, our model maintains the interpretability while is ﬂexi-ble enough to capture various complex temporal relationships between the tensor nodes. For ef-ﬁcient and high-quality posterior inference, we use the stochastic differential equation (SDE) representation of temporal GPs to build an equivalent state-space prior, which avoids huge kernel matrix computation and sparse/low-rank approximations. We then use Kalman ﬁltering, RTS smoothing, and conditional moment matching to develop a scalable message-passing inference al-gorithm. We show the advantage of our method in simulation and several real-world applications.