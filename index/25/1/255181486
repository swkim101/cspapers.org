Human-robot collaboration (HRC) has been considered as a promising paradigm towards futuristic human-centric smart manufacturing, to meet the thriving needs of mass personalization. In this context, existing robotic systems normally adopt a single-granularity semantic segmentation scheme for environment perception, which lacks the flexibility to be implemented to various HRC situations. To fill the gap, this study proposes a multi-granularity scene segmentation network. Inspired by some recent network designs, we construct an encoder network with two ConvNext-T backbones for RGB and depth respectively, and an decoder network consisting of multi-scale supervision and multi-granularity segmentation branches. The proposed model is demonstrated in a human-robot collaborative battery disassembly scenario and further evaluated in comparison with state-of-the-art RGB-D semantic segmentation methods on the NYU-Depth V2 dataset.