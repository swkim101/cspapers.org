While standardization is a well-established activity in other scientiﬁc ﬁelds such as telecommunications, networks or multimedia, in the ﬁeld of AI and more speciﬁcally NLP it is still at its dawn. In this paper, we explore how various aspects of NLP (evaluation, data, tasks...) lack standards and how that can impact science, but also the society, the industry, and regulations. We argue that the numerous initiatives to rationalize the ﬁeld and establish good practices are only the ﬁrst step, and developing formal standards remains needed to bring further clarity to NLP research and industry, at a time where this community faces various crises regarding ethics or reproducibility. We thus encourage NLP researchers to contribute to existing and upcoming standardization projects, so that they can express their needs and concerns, while sharing their expertise.