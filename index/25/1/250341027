Spiking Neural Networks (SNNs) are considered a promising alternative to Artiﬁcial Neural Networks (ANNs) for their event-driven computing paradigm when deployed on energy-efﬁcient neuromorphic hardware. Recently, deep SNNs have shown breathtaking performance improvement through cutting-edge training strategy and ﬂexi-ble structure, which also scales up the number of parameters and computational burdens in a single network. Inspired by the state transition of dendritic spines in the ﬁlopodial model of spinogen-esis, we model different states of SNN weights, facilitating weight optimization for pruning. Furthermore, the pruning speed can be regulated by using different functions describing the growing threshold of state transition. We organize these techniques as a dynamic pruning algorithm based on nonlinear reparameterization mapping from spine size to SNN weights. Our approach yields sparse deep networks on the large-scale dataset (SEW ResNet18 on ImageNet) while maintaining state-of-the-art low performance loss ( ∼ 3% at 88.8% sparsity) compared to existing pruning methods on directly trained SNNs. Moreover, we ﬁnd out pruning speed regulation while learning is crucial to avoiding disastrous performance degradation at the ﬁnal stages of training, which may shed light on future work on SNN pruning.