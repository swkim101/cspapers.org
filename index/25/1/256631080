We introduce question answering with a context in focus , a task that simulates a free interaction with a QA system. The user reads on a screen some information about a topic and they can follow-up with questions that can be either related or not to the topic; and the answer can be found in the document containing the screen content or from other pages. We call such information context . To study the task, we construct F OCUS QA, a dataset for answer sentence selection (AS2) with 12,165 unique (cid:104) question, context (cid:105) pairs and a total of 109,940 answers. To build the dataset, we developed a novel methodology that takes existing questions and pairs them with relevant contexts. To show the beneÔ¨Åts of this approach, we present a comparative analysis with a set of questions written by humans after reading the context , showing that our approach greatly helps in eliciting more realistic (cid:104) question, context (cid:105) pairs. Finally, we show that the task poses several challenges for incorporating contextual information. In this respect, we introduce strong baselines for answer sentence selection that outperform the precision of state-of-the-art models for AS2 up to 21.3% absolute points.