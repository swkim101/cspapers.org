We introduce MedicalSum , a Transformer-001 based sequence-to-sequence architecture for 002 summarizing medical conversations by integrat-003 ing medical domain knowledge from the Uni-004 fied Medical Language System (UMLS). The 005 novel knowledge augmentation is performed 006 in three ways: (i) introducing a guidance sig-007 nal that consists of the medical words in the 008 input sequence, (ii) leveraging semantic type 009 knowledge in UMLS to create clinically mean-010 ingful input embeddings, and (iii) making use 011 of a novel weighted loss function that provides 012 a stronger incentive for the model to correctly 013 predict words with a medical meaning. 014 By applying these three strategies, Medical-015 Sum takes clinical domain knowledge into con-016 sideration during the summarization process 017 and achieves state-of-the-art ROUGE score im-018 provements of 0.8-2 points (including 6.2% er-019 ror reduction in PE section ROUGE-1) when 020 producing medical summaries of patient-doctor 021 conversations. Furthermore, a qualitative analy-022 sis shows that medical summaries produced by 023 the knowledge augmented model contain more 024 relevant clinical facts from the patient-doctor 025 conversation. 026