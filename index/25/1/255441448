In the context of user interface-oriented software development, the task of translating a GUI into code requires sufficient knowledge to identify visual elements and how to code it for one or more platforms. In addition, other issues are important, such as reuse, componentization and understanding of the behavior of trivial visual elements. This is a repetitive and tedious task that could be automated. To perform automation this task many challenges depend on the starting point (hand-draw or hi-fidelity images), the detection and recognition of visual elements from images, their data representation and the code generation itself. This work aims to build a model that makes it possible to automate the process of code generation from images so that it is possible to infer which visual elements are reusable across a range of GUIs and in such a way that you can navigate between GUIs in the same application. This study is being conducted through a DSR (Design Science Research) and so far some classes of problems and artifacts have been found that help to understand how to extract and represent data from GUI and generate web, android and ios application code. The open questions about this problem concern how to identify individual elements and in a group, how to represent these visual elements in a way that can be used in code generators and what is the most efficient code generator for this type of problem. The answers to these questions are part of the next steps of this work.