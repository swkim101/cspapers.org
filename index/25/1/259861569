This paper presents a new Convolutional Neural Network, named Contextual Convolutional Network, that capably serves as a general-purpose backbone for visual recognition. Most existing convolutional backbones follow the representation-to-classiﬁcation paradigm, where representations of the input are ﬁrstly generated by category-agnostic convolutional operations, and then fed into classiﬁers for speciﬁc perceptual tasks (e.g., classiﬁcation and segmentation). In this paper, we deviate from this classic paradigm and propose to augment potential category mem-berships as contextual priors in the convolution for contextualized representation learning. Speciﬁcally, top-k likely classes from the preceding stage are encoded as a contextual prior vector. Based on this vector and the preceding features, off-sets for spatial sampling locations and kernel weights are generated to modulate the convolution operations. The new convolutions can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation without additional supervision. The qualities of Contextual Convolutional Networks make it compatible with a broad range of vision tasks and boost the state-of-the-art architecture ConvNeXt-Tiny by 1 . 8% on top-1 accuracy of ImageNet classiﬁcation. The superiority of the proposed model reveals the potential of contextualized representation learning for vision tasks. Code is available at: https://github.com/liang4sx/contextual_cnn .