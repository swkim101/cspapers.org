Although the incorporation of pre-trained lan- 001 guage models (PLMs) significantly pushes the 002 research frontier of multi-turn response se- 003 lection, it brings a new issue of heavy com- 004 putation costs. To alleviate this problem 005 and make the PLM-based response selection 006 model both effective and efficient, we pro- 007 pose an inference framework together with 008 a post-training strategy that builds upon any 009 pre-trained transformer-based response selec- 010 tion models to accelerate inference by progres- 011 sively selecting and eliminating unimportant 012 content under the guidance of context-response 013 dual-attention. Specifically, at each trans- 014 former layer, we first identify the importance 015 of each word based on context-to-response 016 and response-to-context attention, then select 017 a number of unimportant words to be elimi- 018 nated following a retention configuration de- 019 rived from evolutionary search while passing 020 the rest of the representations into deeper layers. 021 To mitigate the training-inference gap posed 022 by content elimination, we introduce a post- 023 training strategy where we use knowledge dis- 024 tillation to force the model with progressively 025 eliminated content to mimic the predictions of 026 the original model with no content elimination. 027 Experiments on three benchmarks indicate that 028 our method can effectively speeds-up SOTA 029 models without much performance degradation 030 and shows a better trade-off between speed and 031 performance than previous methods. 032