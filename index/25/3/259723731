Automatically detecting emotions from text has countless
applications, ranging from large scale opinion mining to
social robots in healthcare and education. However, emotions
are subjective in nature and are often expressed in ambiguous
ways. At the same time, detecting emotions can also require
implicit reasoning, which may not be available as surface-
level, lexical information. In this work, we conjecture that
the overconfidence of pre-trained language models such as
BERT is a critical problem in emotion detection and show
that alleviating this problem can considerably improve the
generalization performance. We carry out comprehensive
experiments on four emotion detection benchmark datasets
and show that calibrating our model predictions leads to an
average improvement of 1.35% in weighted F1 score.