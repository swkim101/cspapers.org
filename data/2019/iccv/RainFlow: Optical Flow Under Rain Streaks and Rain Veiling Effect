In our experiments, we also compare the effects of using different data scheduling to train our network. We train our network using the dataset scheduling schemes suggested by [1, 3] (FlyingChair → FlyingThings) with long learning rate schedule Slong . Our network presented in the main paper is trained on a mix of the FlyingChair and FlyingThings datasets (ChairThings dataset). As shown in Table 1, we find that sequentially training on FlyingChair+FlyingThings3D (rendered with rain) does not perform as good as mixing the two datasets randomly at the beginning of every training epoch. For training our method on mixed FlyingChair and FlyingThings, our cropped image size is set to 256× 448 instead of 320× 448 described in [3]. As one can see from the table, training our method on the mixed data has significant improvement on the performance on Sintel dataset.