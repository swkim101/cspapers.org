Machine learning (ML) has become increasingly important and performance-critical in modern data centers. This has led to interest inmodel serving systems, which performML inference and serve predictions to end-user applications. However, most existing model serving systems approach ML inference as an extension of conventional data serving workloads and miss critical opportunities for performance. In thispaper,wepresentWillump,a statistically-awareoptimizer forML inference that takes advantage of key properties ofML inference not shared by traditionalworkloads. First,ML models can often be approximated efficiently on many “easy” inputs by judiciously using a less expensive model for these inputs (e.g., not computing all the input features). Willump automatically generates such approximations from anML inference pipeline, providing up to 4.1× speedup without statistically significant accuracy loss. Second, MLmodels are often used in higher-level end-to-end queries in an ML application, such as computing the top K predictions for a recommendation model. Willump optimizes inference based on these higher-level queries by up to 5.7× over naïve batch inference. Willump combines these novel optimizations with standard compiler optimizations and a computation graph-aware feature caching scheme to automatically generate fast inference code for ML pipelines. We show that Willump improves performance of real-world ML inference pipelines by up to 23×, with its novel optimizations giving 3.6–5.7× speedups over compilation. We also show thatWillump integrates easily with existing model serving systems, such as Clipper.