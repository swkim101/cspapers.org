The surge of misinformation poses a serious problem for fact-checkers. Several initiatives for manual fact-checking have stepped up to combat this ordeal. However, computational methods are needed to make the veriﬁcation faster and keep up with the increasing abundance of false information. Machine Learning (ML) approaches have been proposed as a tool to ease the work of manual fact-checking. Speciﬁcally, the act of checking textual claims by using relational datasets has recently gained a lot of traction. However, despite the abundance of proposed solutions, there has not been any formal deﬁnition of the problem, nor a comparison across the different assumptions and results. In this work, we make a ﬁrst attempt at solving these ambiguities. First, we formalize the problem by providing a general deﬁnition that is applicable to all systems and that is agnostic to their assumptions. Second, we deﬁne general dimensions to characterize different prominent systems in terms of assumptions and features. Finally, we report experimental results over three scenarios with corpora of real-world textual claims.