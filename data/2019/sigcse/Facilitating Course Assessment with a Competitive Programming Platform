We present an initiative that introduced the use of a competitive programming platform as a mechanism for auto-grading assignments for an introductory course on algorithm design and analysis. The specific objective of the intervention was to increase the number of assessed programming exercises to an average of 1 per week. A traditionally large enrolment with only a few graduate assistants available meant that prior to the intervention, few assignments were given, and the duration that students waited for feedback was long. Fresh problems were developed for deployment on the platform, each one targeting the specific learning objectives of the week in which they were given. The assignments were given in the format of a contest, and students were permitted to submit multiple attempts without penalty. There was a public leaderboard that showed real-time standings, but a student's grades depended only on the number of test cases his submissions passed and not on its ranking on the leaderboard. Anecdotally, we observed an increased degree of engagement with the course content. However a statistical analysis shows that the impact of the intervention on student performance, relative to previous instances of the course, was mixed. We discuss these and other findings.