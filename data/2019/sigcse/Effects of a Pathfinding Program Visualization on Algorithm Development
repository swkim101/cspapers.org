Program Visualizations (PVs) have been used as educational tools to allow students to visually inspect the runtime behavior of their code. However, many of these systems act as low-level visual debuggers not high-level abstractions of program behavior. Additionally, evaluations of these systems tend to focus more on student engagement or opinion in using the system and not on artifacts produced using the system. This paper discusses the effectiveness of a PV developed to aide students in an undergraduate Artificial Intelligence class on a pathfinding homework assignment. Students in 4 semesters of the course were tasked to develop pathfinding algorithms for an agent to navigate worlds in cases of both certain and uncertain world information. Students in 2 semesters of the course were given access to a PV that allowed them to see a visual representation of their agent navigating the world in either information condition. The final agents developed by these students were compared with those developed by students who never received the PV. Comparisons were made on the performance of these agents in both cases of uncertain and certain world information on several test worlds. Student written reports for the Experimental condition were also analyzed. The results showed significant differences in the performance of the algorithms developed in both certain and uncertain world information. Student reflections on using the PV within the written reports provide insight into how the PV informed the design and development of their submission.