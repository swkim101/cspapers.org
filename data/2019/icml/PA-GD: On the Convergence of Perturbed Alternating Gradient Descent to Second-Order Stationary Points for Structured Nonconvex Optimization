Alternating gradient descent (A-GD) is a simple but popular algorithm in machine learning, which updates two blocks of variables in an alternating manner using gradient descent steps. In this paper, we consider a smooth unconstrained nonconvex optimization problem, and propose a p erturbed A - GD (PA-GD) which is able to converge (with high probability) to the second-order stationary points (SOSPs) with a global sublinear rate. Existing analysis on A-GD type algorithm either only guarantees convergence to ﬁrst-order solutions, or converges to second-order solutions asymptotically (without rates). To the best of our knowledge, this is the ﬁrst alternating type algorithm that takes O ( polylog ( d ) /ϵ 2 ) iterations to achieve an ( ϵ, √ ϵ )-SOSP with high probability, where polylog ( d ) denotes the polynomial of the logarithm with respect to problem dimension d .