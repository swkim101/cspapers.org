In this paper we define two feature representations for grasping. These representations capture hand-object geometric relationships at the near-contact stage â€” before the fingers close around the object. Their benefits are: 1) They are stable under noise in both joint and pose variation. 2) They are largely hand and object agnostic, enabling direct comparison across different hand morphologies. 3) Their format makes them suitable for direct application of machine learning techniques developed for images.We validate the representations by: 1) Demonstrating that they can accurately predict the distribution of $\epsilon$-metric values generated by kinematic noise. I.e., they capture much of the information inherent in contact points and force vectors without the corresponding instabilities. 2) Training a binary grasp success classifier on a real-world data set consisting of 588 grasps.