Visual attention is guided by the integration of two streams: the global, that rapidly processes the scene, and the local, that processes details. For people with autism, the integration of these two streams can be disrupted by the tendency to privilege details (local processing) instead of seeing the big picture (global processing). Consequently, people with autism struggle with typical visual attention, evidenced by their verbal description of local features when asked to describe overall scenes, disrupting their social understanding. This work aims to explore an augmentation for global processing by digitally filtering visual stimuli. This work contributes initial prototypes to improve global processing and leverages an eye tracking dataset to compare results as a validation technique.