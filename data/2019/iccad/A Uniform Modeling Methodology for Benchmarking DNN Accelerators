Deep Neural Networks (DNNs) have achieved tremendous success in many application domains. Inspired by its success, specialized accelerators have been and continue to be developed to process DNN workloads in an energy-efficient manner. The design space for DNN accelerators can be extremely large since they can employ different datapaths, data mapping strategies, circuits, and device technologies. To explore the design space for developing DNN accelerators, it is important to quickly estimate the energy cost associated with an accelerator. This paper introduces a uniform modeling framework, Eva-DNN, to estimate the dynamic energy (a major component of total energy) consumed by a DNN accelerator. Specifically, we model the number of accesses and associated energy cost at different levels of memory and functional units. We derive a uniform expression that estimates the number of accesses as a function of the number of basic operations normalized by data reuse and activity factor of corresponding units. To model the energy cost of an individual functional unit operation, we employ a device-level benchmarking approach. Eva-DNN can accurately model energy contributions from device technology, circuits, architecture, data mapping strategy, and network. We applied our model on three accelerator architectures from the literature, namely: Eyeriss, ShiDianNao, and TrueNorth. Results suggest that Eva-DNN can accurately estimate energy contributions from different architectural units, achieving 4.5% to 8.0% of deviation from energy costs obtained from hardware measurements for different DNN workloads.