The threshold logic (TL) function, parameterized by a vector of weights and a threshold value, is an important class of Boolean functions that imitate neural information processing. When multiple TL functions are to be implemented in circuits or to be valuated through hardware acceleration, weight sharing among them may provide an effective way for circuit minimization or data compression. We study the condition for a set of TL functions to be implementable with a common weight vector, i.e., representable with parallel separating hyperplanes, and devise a new parameter compression technique. Experimental results demonstrate a 7-fold compression ratio for libraries of TL functions with up to 6 inputs and a data storage reduction to about 45% of the original parameter size for the depthwise convolution layers of an activation-binarized neural network aiming at CIFAR10 dataset classification.