Short text clustering poses substantial chal-001 lenges due to the limited amount of informa-002 tion provided by each sample. Previous efforts 003 based on dense representations are still inad-004 equate since texts from different clusters are 005 not sufficiently segregated in the embedding 006 space prior to the clustering step. Even though 007 the state-of-the-art technique integrated con-008 trastive learning with a soft clustering objective 009 to address this issue, the step in which all local 010 tokens are summarized to form a sequence rep-011 resentation for the whole text may include noise 012 that obscures the key information. We propose 013 a framework called MIST: M utual I nformation 014 Maximization for S hort T ext Clustering, which 015 overcomes the information limitation by max-016 imizing the mutual information between text 017 samples on both sequence and token levels. We 018 assess the performance of our proposed method 019 on eight standard short text datasets. Experi-020 mental results show that MIST outperforms 021 the state-of-the-art methods in terms of Accu-022 racy or Normalized Mutual Information in most 023 cases. 024