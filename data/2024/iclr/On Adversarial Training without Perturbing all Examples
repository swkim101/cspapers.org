Adversarial Training (AT) is the de-facto standard for improving robustness against 1 adversarial examples. This usually involves a multi-step adversarial attack applied 2 on each example during training. In this paper, we explore only constructing 3 Adversarial Examples (AEs) on a subset of the training examples. That is, we 4 split the training set in two subsets A and B , train models on both ( A âˆª B ) but 5 construct AEs only for examples in A . Starting with A containing only a single 6 class, we systematically increase the size of A and consider splitting by class and by 7 examples. We observe that: (i) adv. robustness transfers by difficulty and to classes 8 in B that have never been adv. attacked during training, (ii) we observe a tendency 9 for hard examples to provide better robustness transfer than easy examples, yet find 10 this tendency to diminish with increasing complexity of datasets (iii) generating 11 AEs on only 50% of training data is sufficient to recover most of the baseline AT 12 performance even on ImageNet. We observe similar transfer properties across tasks, 13 where generating AEs on only 30% of data can recover baseline robustness on the 14 target task. We evaluate our subset analysis on a wide variety of image datasets 15 like CIFAR-10, CIFAR-100, ImageNet-200 and show transfer to SVHN, Oxford-16 Flowers-102 and Caltech-256. In contrast to conventional practice, our experiments 17 indicate that the utility of computing AEs varies by class and examples and that 18 weighting examples from A higher than B provides high transfer performance. 19