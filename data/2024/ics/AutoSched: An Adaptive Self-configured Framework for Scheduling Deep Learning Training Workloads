Modern Deep Learning Training (DLT) schedulers in GPU datacenters are designed to be very sophisticated with many configurations. These configurations need to be adjusted delicately as they can significantly affect the scheduling performance. Existing schedulers require the datacenter operator to tune the configurations only once before they are deployed, based on the historical workload traces. Unfortunately, workloads in a datacenter would experience dynamic changes and deviate a lot from the historical ones over time, making the pre-determined configurations less effective. To address this dilemma, we design AutoSched, a framework that can automatically, efficiently, and dynamically adjust the configuration parameters of DLT schedulers. Motivated by our characterization analysis of real-world DLT workloads and existing schedulers, we introduce two innovative system designs. (1) We develop a Generation Engine to produce workloads that can reveal the future trace pattern, which facilitates accurate configuration tuning. (2) We design a Search Engine to reduce the exorbitant overhead of configuration tuning. AutoSched is general and can be integrated with off-the-shelf schedulers. We showcase how AutoSched strengthens three representative DLT schedulers and evaluate them on varying DLT traces. Extensive experiments demonstrate that AutoSched improves the performance of state-of-the-art schedulers by up to 46% with 132 Ã— configuration tuning latency reduction.