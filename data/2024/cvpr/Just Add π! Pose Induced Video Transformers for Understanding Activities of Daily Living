Video transformers have become the de facto standard for human action recognition, yet their exclusive reliance on the RGB modality still limits their adoption in certain do-mains. One such domain is Activities of Daily Living (ADL), where RGB alone is not sufficient to distinguish between visually similar actions, or actions observed from multiple viewpoints. To facilitate the adoption of video transform-ers for ADL, we hypothesize that the augmentation of RGB with human pose information, known for its sensitivity to fine-grained motion and multiple viewpoints, is essential. Consequently, we introduce the first Pose Induced Video Transformer: PI- ViT (or 1T - ViT), a novel approach that augments the RGB representations learned by video trans-formers with 2D and 3D pose information. The key elements of 1T - ViT are two plug-in modules, 2D Skeleton Induction Module and 3D Skeleton Induction Module, that are re-sponsible for inducing 2D and 3D pose information into the RGB representations. These modules operate by performing pose-aware auxiliary tasks, a design choice that allows 1T - ViT to discard the modules during inference. Notably, 1T - ViT achieves the state-of-the-art performance on three prominent ADL datasets, encompassing both real-world and large-scale RGB-D datasets, without requiring poses or additional computational overhead at inference. We release code and models at https://github.com/dominickreilpi-vitl.