Generating human motions from textual descriptions has gained growing research interest due to its wide range of applications. However, only a few works consider human- scene interactions together with text conditions, which is crucial for visual and physical realism. This paper fo-cuses on the task of generating human motions in 3D in-door scenes given text descriptions of the human-scene in-teractions. This task presents challenges due to the multi-modality nature of text, scene, and motion, as well as the need for spatial reasoning. To address these challenges, we propose a new approach that decomposes the complex prob-lem into two more manageable sub-problems: (1) language grounding of the target object and (2) object-centric motion generation. For language grounding of the target ob-ject, we leverage the power of large language models. For motion generation, we design an object-centric scene rep-resentation for the generative model to focus on the target object, thereby reducing the scene complexity and facilitating the modeling of the relationship between human motions and the object. Experiments demonstrate the better motion quality of our approach compared to baselines and validate our design choices. Code will be available at link.