Learning scene semantics that can be well generalized to foggy conditions is important for safety-crucial applications such as autonomous driving. 
Existing methods need both annotated clear images and foggy images to train a curriculum domain adaptation model.
Unfortunately, these methods can only generalize to the target foggy domain that has seen in the training stage, but the foggy domains vary a lot in both urban-scene styles and fog styles.
In this paper, we propose to learn scene segmentation well generalized to foggy-scenes under the domain generalization setting, which does not involve any foggy images in the training stage and can generalize to any arbitrary unseen foggy scenes. 
We argue that an ideal segmentation model that can be well generalized to foggy-scenes need to simultaneously enhance the content, de-correlate the urban-scene style and de-correlate the fog style. 
As the content (e.g., scene semantic) rests more in low-frequency features while the style of urban-scene and fog rests more in high-frequency features, we propose a novel bi-directional wavelet guidance (BWG) mechanism to realize the above three objectives in a divide-and-conquer manner. 
With the aid of Haar wavelet transformation,
the low frequency component is concentrated on the content enhancement self-attention, while the high frequency component is shifted to the style and fog self-attention for de-correlation purpose.
It is integrated into existing mask-level Transformer segmentation pipelines in a learnable fashion.
Large-scale experiments are conducted on four foggy-scene segmentation datasets under a variety of interesting settings.
The proposed method significantly outperforms existing directly-supervised, curriculum domain adaptation and domain generalization segmentation methods. 
Source code is available at https://github.com/BiQiWHU/BWG.