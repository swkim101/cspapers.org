Discovering causal relationships from observational data is a challenging topic in artificial intelligence. Recent works formulate causal discovery as a continuous optimization problem with a differentiable acyclic constraint. Although these methods have achieved considerable performance improvement, they have two drawbacks: 1) they require a relatively large number of training samples; and 2) their performance will substantially deteriorate when facing heterogeneous noise. To address these problems, we first propose a low-order conditional independence (CI) constraint for the continuous optimization problem, and then design a soft version of the constraint by transforming it to a regularization term in the loss function of the continuous optimization problem. We show the convergence of continuous optimization with our constraint under some mild conditions, and the consistency of causal structure learning with the CI regularization. Extensive experiments on both synthetic and real-world datasets show that with our CI constraint or regularization, existing continuous optimization methods can achieve considerable performance improvement of causal discovery, especially when sample size is small.