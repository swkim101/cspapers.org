Multi-Label Classification (MLC), which aims to assign multiple labels to each sample simultaneously, has achieved great success in a wide range of applications. MLC saves global label correlation by building a single model shared by all samples but ignores sample-specific local structures, while Personalized Learning (PL) is able to preserve sample-specific information by learning local models but ignores the global structure. Integrating PL with MLC is a straightforward way to overcome the limitations, but it still faces three key challenges. 1) capture both local and global structures in a unified model; 2) efficiently preserve high-order interactions among labels, features and samples; 3) learn a concise and interpretable model where only a fraction of interactions are associated with multiple labels. In this paper, we propose a novel Multi-Label Personalized Classification (MLPC) method to handle these challenges. For 1), it integrates local and global components to preserve sample-specific information and global structure shared across samples, respectively. For 2), a multilinear model is developed to capture high-order interactions, and over-parameterization is avoided by tensor factorization. For 3), exclusive sparsity regularization penalizes factorization by promoting intra-group competition, thereby eliminating irrelevant and redundant interactions during Exclusive Sparse Tensor Factorization (ESTF). Moreover, theoretical analysis reveals the equivalence between MLPC with a family of jointly regularized counterparts. We develop an alternating algorithm to solve the optimization problem, and extensive experiments on various datasets demonstrate its effectiveness.