A series of recent publications by Awasthi, Mao, Mohri, and Zhong [2022b] have introduced the key notion of H -consistency bounds for surrogate loss functions. These are upper bounds on the zero-one estimation error of any predictor in a hypothesis set, expressed in terms of its surrogate loss estimation error. They are both non-asymptotic and hypothesis set-speciﬁc and thus stronger and more informative than Bayes-consistency. However, determining if they hold and deriving these bounds have required a speciﬁc proof and analysis for each surrogate loss. Can we derive more general tools and characterizations? This paper provides both a general characterization and an extension of H -consistency bounds for multi-class classiﬁcation. We present new and tight H -consistency bounds for both the family of constrained losses and that of comp-sum losses, which covers the familiar cross-entropy, or logistic loss applied to the outputs of a neural network. We further extend our analysis beyond the completeness assumptions adopted in previous studies and cover more realistic bounded hypothesis sets. Our characterizations are based on error transformations, which are explicitly deﬁned for each formulation. We illustrate the application of our general results through several special examples. A by-product of our analysis is the observation that a recently derived multi-class H -consistency bound for cross-entropy reduces to an excess bound and is not signiﬁcant. Instead, we prove a much stronger and more signiﬁcant guarantee.