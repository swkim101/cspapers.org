We study ( ε, δ ) -differentially private (DP) stochastic convex optimization under an r -th quantile loss function taking the form c ( u ) = ru + + (1 − r )( − u ) + . The function is non-smooth, and we propose to approximate it with a smooth function obtained by convolution smoothing, which enjoys both structure and bandwidth flexibility and can address outliers. This leads to a better approximation than those obtained from existing methods such as Moreau Enve-lope. We then design private algorithms based on DP stochastic gradient descent and objec-tive perturbation, and show that both algorithms achieve (near) optimal excess generalization risk O (max { 1 √ n , √ d ln(1 /δ ) nε } ) . Through objective perturbation, we further derive an upper bound O (max { (cid:113) dn , (cid:113) d ln(1 /δ ) nε } ) on the parameter estimation error under mild assumptions on data generating processes. Some applications in private quantile regression and private inventory control will be discussed.