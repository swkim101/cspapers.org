A landmark negative result of Long and Serve-dio has had a considerable impact on research and development in boosting algorithms, around the now famous tagline that ”noise defeats all convex boosters”. In this paper, we appeal to the half-century+ founding theory of losses for class probability estimation, an extension of Long and Servedio’s results and a new general convex booster to demonstrate that the source of their negative result is in fact the model class , linear separators. Losses or algorithms are neither to blame. This leads us to a discussion on an otherwise praised aspect of ML, parameterisation .