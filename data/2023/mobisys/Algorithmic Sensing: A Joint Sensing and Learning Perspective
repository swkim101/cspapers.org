Sensing hardware technologies and algorithms to interpret sensing data are two main pillars of high-level perception. Existing works either handle these two pillars independently, hence resulting in a loss of efficiency, or plainly adopt deep learning as a one-size-fits-all solution, attempting to directly fit sensor data to desired outputs (e.g., decisions or predictions). Consequently, there are urgent needs for improving sensing efficiency while improving its practicality and robustness in the face of diversified application scenarios that may severely interfere with the sensing data. To this end, we propose algorithmic sensing to integrate these two pillars, and to avoid blindly applying deep-learning algorithms to sensing. In particular, our joint sensing and learning scheme involves designing adaptable sensing platforms for various learning algorithms, as well as adapting algorithms to fit existing sensing infrastructure. We illustrate the two sides of algorithmic sensing by using past work as examples, thus providing a comprehensive framework for designing future algorithmic sensing systems and suggesting open research directions.