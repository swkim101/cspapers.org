Implementation details. Following HAQ [8], we quantize all the layers, in which the first and the last layers are quantized to 8-bit. Following [4, 2], we introduce weight normalization during training. We use SGD with nesterov [6] for optimization, with a momentum of 0.9. For all models on ImageNet, we first train the full-precision models and then use the pre-trained weights to initialize the quantized models. We then fine-tune for 150 epochs. The learning rate starts at 0.01 and decays with cosine annealing [5]. Main Results. We apply the proposed method to quantize MobileNetV1 [3] and MobileNetV2 [7] to 4-bit. We compare the performance of different methods in Table S1. From the results, our proposed method outperforms other methods by a large margin. For example, compared with HAQ, our proposed method achieve 2.7% and 3.5% higher Top-1 accuracy for 4-bit MobileNetV1 and MobileNetV2.