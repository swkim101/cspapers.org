In this paper, we propose a conceptually simple but very effective attention module for Convolu-tional Neural Networks (ConvNets). In contrast to existing channel-wise and spatial-wise attention modules, our module instead infers 3-D attention weights for the feature map in a layer without adding parameters to the original networks. Speciﬁcally, we base on some well-known neuro-science theories and propose to optimize an energy function to ﬁnd the importance of each neuron. We further derive a fast closed-form solution for the energy function, and show that the solution can be implemented in less than ten lines of code. Another advantage of the module is that most of the operators are selected based on the solution to the deﬁned energy function, avoiding too many efforts for structure tuning. Quantitative evaluations on various visual tasks demonstrate that the proposed module is ﬂexible and effective to improve the representation ability of many ConvNets. Our code is available at Pytorch-SimAM.