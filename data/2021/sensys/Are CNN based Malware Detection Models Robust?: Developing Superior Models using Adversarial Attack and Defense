The tremendous increase of malicious applications in the android ecosystem has prompted researchers to explore deep learning based malware detection models. However, research in other domains suggests that deep learning models are adversarially vulnerable, and thus we aim to investigate the robustness of deep learning based malware detection models. We first developed two image-based E-CNN malware detection models based on android permission and intent. We then acted as an adversary and designed the ECO-FGSM evasion attack against the above models, which achieved more than 50% fooling rate with limited perturbations. The evasion attack converts maximum malware samples into adversarial samples while minimizing the perturbations and maintaining the sample's syntactical, functional, and behavioral integrity. Later, we used adversarial retraining to counter the evasion attack and develop adversarially superior malware detection models, which should be an essential step before any real-world deployment.