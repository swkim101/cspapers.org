We propose an autonomous grasping pipeline that relies on geometric information extracted from segmented point cloud data. This is in contrast to many recent approaches leveraging deep learning and thus relying on a rather large amount of training samples. We argue that the proposed geometric approach facilitates task-level planning as the shape, size, and symmetry of objects can be directly taken into account during the planning process that utilizes the new MoveIt! Task Constructor (MTC) framework to define and plan action sequences composed of several inter-related sub-tasks. The efficiency of the proposed grasping pipeline is illustrated in pick-and-place scenarios, including a long-distance pick-and-place requiring a hand-over between two hands.