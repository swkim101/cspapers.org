ReRAM-based Computing-in-Memory (CiM) architecture has been considered an ideal solution to neural networks, by conducting in-situ matrix multiplications without moving the neural parameters from memory cells. However, we found that keeping the parameters static in ReRAM cells, i.e. weight-static processing, is not the sole choice to implement emerging graph neural networks (GNNs) that operate on the input of ultra large graphs. Therefore, we propose TARe, a Task-Adaptive CiM architecture that supports multiple different in-situ computing modes for Graph Learning. With the proposed novel hybrid in-situ computing architecture, TARe achieves 451.98Ã— speedup on average over the baseline in SOTA GNN workloads.