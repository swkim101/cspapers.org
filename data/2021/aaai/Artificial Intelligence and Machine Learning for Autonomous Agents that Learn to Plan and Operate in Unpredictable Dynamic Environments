My research activity focuses on the integration of acting, learning and planning. The main objective is to build a sys-tem that is capable to learn how to plan and act in an unknown, dynamic and complex environment. The only knowledge the agent has about the environment is provided by a set of sensor observations which returns continuous measures on the environment. On the learning side, I’m interested in developing algorithms that allow an artiﬁcial agent to learn an abstract model of the dynamics of the environment (either an explicit model like a deterministic ﬁnite state machine or a model described in a language to express planning domains). The type of abstract model is speciﬁed by means of discrete state variables rather than continuous variables representing agent observations. In addition to learning the abstract model, I’m interested in learning probabilistic (generative) models that connects the abstract model with the perceptions of the agents. On the acting and planning side, the artiﬁcial agent does not rely on a prior set of execution traces, it rather decides online how to act by means of state-of-art planners. With its own experience, it enriches the planner knowledge, as well as the learned model of the environment. On the learning part, the agent applies techniques for dynamic probabilistic clustering of perceptions in a set of abstract states, neural network for learning transition models, and inductive reasoning for learning action model descriptions. Notice that this is different from Reinforcement Learning where the focus is to learn a policy for achieving a goal, we are interested in learning an abstract model of the environment. We do not have a reward function that encodes the goal to be reached. Indeed in this work an agent does not necessarily need to reach a goal.