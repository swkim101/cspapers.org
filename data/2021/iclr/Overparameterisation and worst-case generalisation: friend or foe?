Overparameterised neural networks have demonstrated the remarkable ability to perfectly ﬁt training samples, while still generalising to unseen test samples. However, several recent works have revealed that such models’ good average performance does not always translate to good worst-case performance: in particular, they may perform poorly on subgroups that are under-represented in the training set. In this paper, we show that in certain settings, overparameterised models’ performance on under-represented subgroups may be improved via post-hoc processing. Speciﬁcally, such models’ bias can be restricted to their classiﬁcation layers