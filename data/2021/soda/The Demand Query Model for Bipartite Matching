We introduce a `concrete complexity' model for studying algorithms for matching in bipartite graphs. The model is based on the "demand query" model used for combinatorial auctions. Most (but not all) known algorithms for bipartite matching seem to be translatable into this model including exact, approximate, sequential, parallel, and online ones. A perfect matching in a bipartite graph can be found in this model with O(n^{3/2}) demand queries (in a bipartite graph with n vertices on each side) and our main open problem is to either improve the upper bound or prove a lower bound. An improved upper bound could yield "normal" algorithms whose running time is better than the fastest ones known, while a lower bound would rule out a faster algorithm for bipartite matching from within a large class of algorithms. Our main result is a lower bound for finding an approximately maximum size matching in parallel: A deterministic algorithm that runs in n^{o(1)} rounds, where each round can make at most n^{1.99} demand queries cannot find a matching whose size is within n^{o(1)} factor of the maximum. This is in contrast to randomized algorithms that can find a matching whose size is $99\%$ of the maximum in O(\log n) rounds, each making n demand queries.