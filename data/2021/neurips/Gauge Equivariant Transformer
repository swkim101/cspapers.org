Attention mechanism has shown great performance and efﬁciency in a lot of deep learning models, in which relative position encoding plays a crucial role. However, when introducing attention to manifolds, there is no canonical local coordinate system to parameterize neighborhoods. To address this issue, we propose an equivariant transformer to make our model agnostic to the orientation of local coordinate systems ( i.e. , gauge equivariant), which employs multi-head self-attention to jointly incorporate both position-based and content-based information. To enhance expressive ability, we adopt regular ﬁeld of cyclic groups as feature ﬁelds in intermediate layers, and propose a novel method to parallel transport the feature vectors in these ﬁelds. In addition, we project the position vector of each point onto its local coordinate system to disentangle the orientation of the coordinate system in ambient space ( i.e. , global coordinate system), achieving rotation invariance. To the best of our knowledge, we are the ﬁrst to introduce gauge equivariance to self-attention, thus name our model Gauge Equivariant Transformer (GET), which can be efﬁciently implemented on triangle meshes. Extensive experiments show that GET achieves state-of-the-art performance on two common recognition tasks.