During experiments, the model configurations can be divided into two different settings according to the complexity of the dataset. For MNIST [6], as it is simple, and the intrinsic structures are distinct by pixels among classes, the structure of SITE degenerates from G ◦ F1 to G by setting F1 to be the identical operator. That is, z = F1(x) = x. Correspondingly, the generatorG instead maps input x to its prototypes Gi(x), i = 1, · · · , c. Hence the structure of SITE is built to be an autoencoder-based structure, where there are c parallel decoders. As for CIFAR-10 [5], due to the need for upsampling in visualization, the image data are resized to 128× 128. The feature extractor F1 is built based on ResNet-18 [3]. Here F1 : R3×128×128 → R10×16×16. And for the generator G, it consists of c = 10 (number of categories) parallel autoencoders, such that Gi : R10×16×16 → R10×16×16. Both MNIST and CIFAR-10 datasets are split into the training and validation sets by default. And all presented examples are from the validation sets. We also test on more complex datasets like Food-101 [1] to demonstrate the scalability of SITE. Please refer to the Appendix 5 for details. Besides, in order to balance the classification loss and the transformation loss we set the scalar factor to be λ = 5 throughout the training phase.