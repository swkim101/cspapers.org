We propose a novel Frank-Wolfe (FW) procedure for the optimization of inﬁnite-dimensional functionals of probability measures - a task which arises naturally in a wide range of areas including statistical learning (e.g. variational inference) and artiﬁcial intelligence (e.g. generative adversarial networks). Our FW procedure takes advantage of Wasserstein gradient ﬂows and strong duality results recently developed in Distributionally Robust Optimization so that gradient steps (in the Wasserstein space) can be efﬁciently computed using ﬁnite-dimensional, convex optimization methods. We show how to choose the step sizes in order to guarantee exponentially fast iteration convergence, under mild assumptions on the functional to optimize. We apply our algorithm to a range of functionals arising from applications in nonparametric estimation.