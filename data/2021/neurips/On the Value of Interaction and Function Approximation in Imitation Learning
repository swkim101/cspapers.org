We study the statistical guarantees for the Imitation Learning (IL) problem in episodic MDPs. Rajaraman et al. [22] show an information theoretic lower bound that in the worst case, a learner which can even actively query the expert policy suffers from a suboptimality growing quadratically in the length of the horizon, H . We study imitation learning under the µ -recoverability assumption of [27] which assumes that the difference in the Q -value under the expert policy across different actions in a state do not deviate beyond µ from the maximum. We show that the reduction proposed by [25] is statistically optimal: the resulting algorithm upon interacting with the MDP for N episodes results in a suboptimality bound of (cid:101) O ( µ |S| H/N ) which we show is optimal up to log-factors. In contrast, we show that any algorithm which does not interact with the MDP and uses an ofﬂine dataset of N expert trajectories must incur suboptimality growing as (cid:38) |S| H 2 /N even under the µ -recoverability assumption. This establishes a clear and provable separation of the minimax rates between the active setting and the no-interaction setting. We also study IL with linear function approximation . When the expert plays actions according to a linear classiﬁer of known state-action features, we use the reduction to multi-class classiﬁcation to show that with high probability, the suboptimality of behavior cloning is (cid:101) O ( dH 2 /N ) given N rollouts from the optimal