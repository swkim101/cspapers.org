In constrained multi-objective RL, the goal is to learn a policy that achieves the best performance speciﬁed by a multi-objective preference function under a constraint. We focus on the ofﬂine setting where the RL agent aims to learn the optimal policy from a given dataset. This scenario is common in real-world applications where interactions with the environment are expensive and the constraint violation is dangerous. For such a setting, we transform the original constrained problem into a primal-dual formulation, which is solved via dual gradient ascent. Moreover, we propose to combine such an approach with pessimism to overcome the uncertainty in ofﬂine data, which leads to our Pessimistic Dual Iteration (PEDI). We establish upper bounds on both the suboptimality and constraint violation for the policy learned by PEDI based on an arbitrary dataset, which proves that PEDI is provably sample efﬁcient. We also specialize PEDI to the setting with linear function approximation. To the best of our knowledge, we propose the ﬁrst provably efﬁcient constrained multi-objective RL algorithm with ofﬂine data without any assumption on the coverage of the dataset.