Cooperative multi-agent reinforcement learning (MARL) has received increasing attention in recent years and has found many scientiﬁc and engineering applications. However, a key challenge arising from many cooperative MARL algorithm designs (e.g., the actor-critic framework) is the policy evaluation problem, which can only be conducted in a decentralized fashion. In this paper, we focus on decentralized MARL policy evaluation with nonlinear function approximation, which is often seen in deep MARL. We ﬁrst show that the empirical decentralized MARL policy evaluation problem can be reformulated as a decentralized nonconvex-strongly-concave minimax saddle point problem. We then develop a decentralized gradient-based descent ascent algorithm called GT-GDA that enjoys a convergence rate of O (1 /T ) . To further reduce the sample complexity, we pro-pose two decentralized stochastic optimization algorithms called GT-SRVR and GT-SRVR I , which enhance GT-GDA by variance reduction techniques. We show that all algorithms all enjoy an O (1 /T ) convergence rate to a stationary point of the reformulated minimax problem. Moreover, the fast convergence rates of GT-SRVR and GT-SRVR I imply O ( (cid:15) − 2 ) communication complexity and O ( m √ n(cid:15) − 2 ) sample complexity, where m is the number of agents and n is