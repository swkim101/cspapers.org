Dropout regularization methods prune a neural network’s pre-determined backbone structure to avoid overﬁtting. However, a deep model still tends to be poorly calibrated with high conﬁdence on incorrect predictions. We propose a uniﬁed Bayesian model selection method to jointly infer the most plausible network depth warranted by data, and perform dropout regularization simultaneously. In particular, to infer network depth we deﬁne a beta process over the number of hidden layers which allows it to go to inﬁnity. Layer-wise activation probabilities induced by the beta process modulate neuron activation via binary vectors of a conjugate Bernoulli process. Experiments across domains show that by adapting network depth and dropout regularization to data, our method achieves superior performance comparing to state-of-the-art methods with well-calibrated uncertainty estimates. In continual learning, our method enables neural networks to dynamically evolve their depths to accommodate incrementally available data beyond their initial structures, and alleviate catastrophic forgetting.