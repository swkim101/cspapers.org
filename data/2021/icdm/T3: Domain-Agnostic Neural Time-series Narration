The task of generating rich and fluent narratives that aptly describe the characteristics, trends, and anomalies of time-series data is invaluable to the sciences (geology, meteorology, epidemiology) or finance (trades, stocks). The efforts for time-series narration hitherto are domain-specific and use predefined templates that offer consistency but lead to mechanical narratives. We present $\mathrm{T}^{3}$ (Time-series-To-Text), a domain-agnostic neural framework for time-series narration, that couples the representation of essential time-series elements in the form of a dense knowledge graph and the translation of said knowledge graph into rich and fluent narratives through the transfer-learning capabilities of PLMs (Pre-trained Language Models). To the best of our knowledge, $\mathrm{T}^{3}$ is the first investigation of the use of neural strategies for time-series narration. We showcase that $\mathrm{T}^{3}$ can improve the lexical diversity of the generated narratives by up to 65.38% while still maintaining grammatical integrity. The performance and practicality of $\mathrm{T}^{3}$ is further validated through an expert review $(n=21)$ where 76.2% of participating experts wary of auto-generated narratives favored $\mathrm{T}^{3}$ as a deployable system for time-series narration due to its rich and diverse narratives. Our code-base and the datasets used with detailed instructions for reproducibility is publicly hosted 1.1https://github.com/Mandar-Sharma/TCube