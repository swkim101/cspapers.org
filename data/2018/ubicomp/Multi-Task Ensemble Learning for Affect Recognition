Emotions are paramount in the human communication. They serve as a medium to enrich the communication, to express preferences, to communicate subjective cues, and even to manipulate others. The scientific research on emotions has been introduced back in 1868 when Charles Darwin undertook a study to prove that humans have an innate and universal set of emotional expressions [1]. In 1997, Picard published her book “Affective Computing” [2], which many consider the start of this scientific field. Two decades afterwards, when Affective Computing is a well-established research field, modeling emotional states still remains a challenging task. Among the main reasons are human subjectivity and inability of artificial intelligent systems to generalize – the lack of general intelligence, where humans excel and AI fails. Affective states are complex states that results in psychological and physiological changes that influence behaving and thinking [5]. A wearable device equipped with galvanic skin response (GSR – measures sweating rate), Electrocardiography (ECG – measures heart electrical activity) or blood volume pulse (BVP – measures cardiovascular dynamics) sensors can capture these psycho-physiological changes. Based on the data from the physiological sensors, machine learning (ML) models for affect recognition can be built. The two specific problems that our work addresses are method quality (affect recognition accuracy) and method generality (the ability to work on diverse datasets). Method quality: The straightforward approach for improving the method quality is to collect larger datasets and use ML algorithms with large learning capacity, such as the deep learning architectures. However, collecting a large dataset in affective computing is time consuming and expensive. Instead of collecting one single dataset, one can combine datasets from several affect recognition domains that are collected with similar physiological sensors, and then learn a unified deep multimodal affect recognition model. To successfully combine several datasets, we are proposing a robust preprocessing method that removes the hardware and person-specific issues. After preprocessing, the data that comes from same type of sensors (e.g., two different ECG sensors) should be similar regardless of the hardware. In addition, not all databases are collected using the same number of sensors. For that reason, we are proposing learning ensembles of ML models, where each model targets a specific sensor combination. 3 Method generality: There are dozens of computer science studies in which affect recognition has been addressed, and in most of the studies, only one domain (dataset) is targeted, e.g., emotion recognition while watching videos. However, a model that is built and tuned only on one dataset, one environment and one type of hardware, would be destined to failure on another domain. Added to that is the specific noise that each hardware produces, making it is nearly impossible to use a computer model trained in one environment on another environment. Our approach for improving the method generality is based on learning from semantically similar, yet technically quite heterogeneous data from physiological sensors. The ML subfield capable of learning models for several tasks in parallel while using a shared representation is Multi-task learning (MTL) [3]. By utilizing MTL approach we are developing ensembles of MTL models on several affective datasets from varying environments, recorded with varying hardware and varying sensor placements. If the method developed will be able to successfully generalize the semantical concepts of emotions from heterogeneous data, it will also enable another insight into creating general intelligence.