It is well known that it is possible to construct “adversarial examples” for neu1 ral networks: inputs which are misclassified by the network yet indistinguishable 2 from true data. We propose a simple modification to standard neural network ar3 chitectures, thermometer encoding, which significantly increases the robustness 4 of the network to adversarial examples. We demonstrate this robustness with ex5 periments on the MNIST, CIFAR-10, CIFAR-100, and SVHN datasets, and show 6 that models with thermometer-encoded inputs consistently have higher accuracy 7 on adversarial examples, without decreasing generalization. State-of-the-art accu8 racy under the strongest known white-box attack was increased from 93.20% to 9 94.30% on MNIST and 50.00% to 79.16% on CIFAR-10. We explore the proper10 ties of these networks, providing evidence that thermometer encodings help neural 11 networks to find more-non-linear decision boundaries. 12