Data-intensive clusters increasingly employ inmemory solutions to improve I/O performance. However, the routinely observed file popularity skew and load imbalance create hot spots, which significantly degrade the benefits of in-memory caching. Common approaches to tame load imbalance include copying multiple replicas of hot files and creating parity chunks using storage codes. Yet, these techniques either suffer from high memory overhead due to cache redundancy or incur nontrivial encoding/decoding complexity. In this paper, we propose an effective approach to achieve load balancing without cache redundancy or encoding/decoding overhead. Our solution, termed SP-Cache, selectively partitions files based on their popularity and evenly caches those partitions across the cluster. We develop an efficient algorithm to determine the optimal number of partitions for a hot fileâ€”too few partitions are incapable of mitigating hot spots, while too many are susceptible to stragglers. We implemented SP-Cache in Alluxio, a popular in-memory distributed storage for data-intensive clusters. EC2 deployment and trace-driven simulations show that, compared to the state-of-the-art solution called EC-Cache [1], SP-Cache reduces the file access latency by up to 40% in both the mean and the tail, using 40% less memory.