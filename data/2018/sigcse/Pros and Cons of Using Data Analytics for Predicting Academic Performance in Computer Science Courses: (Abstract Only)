In the last decade, data analytics has been successfully applied in the field of education to predict student performance. There exists an obvious opportunity for this educational data to make a positive impact on computer science instruction. Machine learning models can use historical data containing behavioral and education-related attributes, such as previous course work, grades and time spent in class discussions, to make predictions about academic performance for prospective students. Even with proven predictive success, many questions related to the application of performance prediction remain unanswered, particularly in the context of larger debates about risk identification, grouping, and bias. This BoF will provide a platform for exploring the following questions: (a) How should computer science instructors use prediction data? Could results be used to group students by predicted academic performance levels? Could predictions help in the identification of students with low performance predictions for additional mentoring? (b) Should predictions be shared with students/instructors? (c) If so, how could instructor bias resulting from these predictions be minimized to ensure fair evaluation of students' actual performance? (d) Do computer science instructors attending this BoF currently implement any predictive tools or risk grouping? Would they consider doing either? (e) How much importance would instructors place on the results of performance predictions? To what degree would the accuracy of a model affect adoption?