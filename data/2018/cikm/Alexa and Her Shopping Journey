Voice-enabled intelligent assistants, such as Amazon Alexa, Google Assistant, Microsoft Cortana or Apple Siri, are on their way to revolutionize the way humans interact with machines. Their ubiquitous presence in our homes, offices, cars, etc. and their ease of use have the potential to fully democratize access to information and services, making them available to all, from young children to senior citizens. To this effect, Alexa is offering an open service available on tens of millions of devices that enables developers to build voice-enabled applications in a multitude of domains from home automation to entertainment. One domain that Alexa is pioneering in particular is the shopping domain. Customers can ask Alexa to order garlic from their kitchen while they are crushing their last clove, and they can as easily ask her about the best surveillance camera. We see then that in the shopping domain, Alexa addresses not only transactional needs but also informational needs, thus covering two of the Web search users' needs defined by Broder in [1]. Yet the usual Web search techniques cannot be applied "as is" in voice-driven product discovery, since as demonstrated by Ingber et al. in [2], users' behavior differs significantly between Web and voice. Consequently, for Alexa to naturally interact with users, and act as the ultimate virtual shopping assistant, new methods need to be invented and a number of open research challenges across various domains need to be addressed. These domains include automatic speech recognition, natural language understanding, search and question answering, and most importantly, user experience, which is critical in such a new and still evolving interaction paradigm. In this talk, we will share with the audience our vision of an intelligent shopping assistant escorting customers in their holistic shopping journey. We will also discuss the involved technical challenges that establish voice shopping as a new area of research in the AI and search communities at large.