We consider the problem of <italic>global optimization</italic> of an unknown non-convex smooth function with noisy zeroth-order feedback. We propose a <italic>local minimax</italic> framework to study the fundamental difficulty of optimizing smooth functions with adaptive function evaluations. We show that for functions with fast growth around their global minima, carefully designed optimization algorithms can identify a near global minimizer with many fewer queries than worst-case global minimax theory predicts. For the special case of strongly convex and smooth functions, our implied convergence rates match the ones developed for zeroth-order <italic>convex</italic> optimization problems. On the other hand, we show that in the worst case no algorithm can converge faster than the minimax rate of estimating an unknown function in the <inline-formula> <tex-math notation="LaTeX">$\ell _\infty $ </tex-math></inline-formula>-norm. Finally, we show that non-adaptive algorithms, though optimal in a global minimax sense, do not attain the optimal local minimax rate.