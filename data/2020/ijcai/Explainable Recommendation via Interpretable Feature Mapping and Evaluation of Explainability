Green's (1998) criticism that connectionist models are devoid of theoretical substance rests on a simplistic view of the nature of connectionist models and a failure to acknowledge the division of labor between the model and the modeller in the enterprise of connectionist modelling. The "theoretical terms" of connectionist theory are not to be found in processing units or in connections but in more abstract characterizations of the functional properties of networks. Moreover, these properties are -and at present should be -only loosely tied to the known (and largely unknown) properties of neural networks in the brain. 1. Green (1998) attempts to find a mapping between connectionist models of cognitive phenomena and traditional scientific theories. Treating nodes and connections as the postulated theoretical terms, he finds these terms to be exceedingly numerous, far removed from the observable phenomena, and to a great extent optional. Moreover, he claims, the ways in which the units interact to produce the desired behavior are generally unfathomable. This leads him to ask "what, exactly, is learned about a cognitive domain modelled by a connectionist network?" His conclusion: not much, if anything at all. 2. Green does, however, hold out hope for current and would-be connectionist modellers: In his words, "at present the only way of interpreting connectionist networks as serious candidates for theories of cognition would be as literal models of the brain activity that underpins cognition." Hence, "connectionists should start restricting themselves to units, connections, and rules that use all and only principles that are known to be true of neurons." This recommendation seems to have found favor in some of the commentaries as well (e.g., O'Brien, 1998). 3. In this commentary I take issue with both the premises and the conclusion of Green's argument: (a) Nodes and connections are not the theoretical terms of connectionist models; rather, that role is filled by more abstract characterizations of the functional properties of the networks. (b) More is known about these functional properties than Green acknowledges, but in any case, there is no principled limit on our ability to understand these properties. (c) Exploring the capacities and functional properties of artificial neural networks can be of value in the study of cognition regardless of whether and how those capacities and properties are realized in the brain. 4. When Green looks at a connectionist network, he sees "dozens, sometimes hundreds, of simple units, bound together by hundreds, sometimes thousands, of connections... Each of the units, connections, and rules in a connectionist network is a theoretical entity..., [yet] neither the units nor the connections correspond to anything in the way that variables and rules did in traditional computational models of cognition." So, Green asks, where is the BEEF? He seems to expect a transparent isomorphism between individual ELEMENTS of the model and the cognitive phenomena that are being modeled. He argues that such is the usual case in scientific theorizing and in traditional cognitive modelling. 5. There are indeed connectionist models -localist models -that are relatively transparent in the mapping between the elements of the model and the actual theoretical claims about the cognitive phenomena being modeled (Feldman & Ballard, 1982; and see Grainger & Jacobs, 1998). But this is not a feature of the parallel distributed processing models that are apparently the target of Green's attack. Are the latter models truly devoid of substantive theoretical content, as Green would have us believe? Should they be abandoned in favor of more transparent localist versions (Grainger & Jacobs, 1998), or at least grounded in substantive claims about BRAIN processing (see parag. 12, below)? 6. The place of connectionism in cognitive theory has been debated extensively since its renaissance began a little over a decade ago (e.g., Broadbent, 1985; Fodor & Pylyshyn, 1988, Massaro, 1988; McCloskey, 1991; Rumelhart & McClelland, 1986; Smolensky, 1988), and many of the points that Green raises have been argued before. Seidenberg (1993) has perhaps been most explicit in fending off the central thrust of Green's argument in the distinction he draws (borrowing from Chomsky, 1965) between "descriptive" and "explanatory" theorizing. The type of theory that Green is looking for would be "descriptive" in Seidenberg's terminology: "Experiments yield data around which descriptive theories are developed... providing systematic descriptions of phenomena and generating novel predictions" (p. 230). Explanatory theories, in contrast, "appeal to a small set of concepts that are independently motivated rather than taskor phenomenon-specific' (p. 230). In Seidenberg's brand of "explanatory connectionism," theories are derived from "general connectionist principles in conjunction with domain-specific boundary conditions" (p. 231). 7. Without getting embroiled in the "more explanatory than thou" aspects of Seidenberg's argument, let us simply note that for him, connectionist theories, like other theories, are embodied in concepts and principles rather than in units and connections. Moreover, the theoretical claims are formulated at various levels of generality and abstraction. Thus, in discussing Seidenberg and McClelland's (1989) model of word reading, Seidenberg (1993, p. 232) identified "broad theoretical claims," such as those concerning the representational status of words (i.e., no explicit lexical representation) and the postulation of a single-process mechanism (as opposed to the traditional dual-process account) to handle rule-governed words, irregular words and nonwords. He also pointed to more specific claims concerning the factors that influence the generation of pronunciations from print (e.g., the importance of sublexical units, such as word bodies), and a "novel link" between frequency effects and the effects of spelling-sound consistency. Some of these claims, those directly tied to general principles, were specified in advance of the modelling, whereas others fell out of the modelling process itself. 8. The division of labor between the model and the modeller in carving out the cognitive theory deserves some amplification. Indeed, this aspect seems to have been completely overlooked in Green's analysis. Green's basic premise, that the theory is (or is not) to be found IN THE MODEL, is misguided. Theories are put forward by scientists, not by models. Simulation models are powerful tools that help researchers develop, test, present and demonstrate the plausibility of their theoretical ideas. They do not, however, "discover" the theory for the researcher, nor do they embody it. Clearly, connectionist modelling is a complex task. Because the principles of computation in connectionist (parallel distributed processing) computational architectures are not yet well understood, a large part of the discovery process comes from working with the models themselves -trying out various architectures, input representations, learning rules and parameters, and so forth. The end product of the synergistic interaction between modeller and model, however, is not just the model (or models), but a scientific article, in which the researcher's theoretical ideas, and their justification, are articulated (viz., as an interrelated set of linguistic propositions). 9. Often in such articles, the models are treated as experimental "subjects," whose essential computational properties are inferred by the modeller. A good example is Plaut and Shallice (1993), who systematically investigated various "design issues" concerning an earlier simulation model (Hinton & Shallice, 1991) used to explain the co-occurrence of visual and semantic errors found in deep dyslexia. As summarized by Plaut (1995), "The design issues included the definition of the task of reading via meaning, the network architecture (i.e., the numbers of units, their organization into layers, and how these groups are connected), the training procedure, used for adjusting connection strengths, and the procedure for evaluating the behavior of the trained network in its normal state and after damage. The major finding was that the occurrence of the qualitative error pattern was surprisingly insensitive to these detailed aspects of the simulation. Rather, what appeared critical was a more general property that all of the implementations shared: that units learned to interact in such a way that familiar patterns of activity over semantic features -corresponding to word meanings -formed STABLE ATTRACTORS in the space of all possible semantic representations" (Plaut, 1995, pp. 299-300, emphasis in original). 10. Plaut and many other connectionists propose that concepts such as stable attractors, basins of attraction, clean-up processes, collateral support, superposition, gradient descent, trajectories in weight space, trajectories in state space, and so forth, offer new and valuable ways of understanding how networks -and arguably how people -perform cognitive tasks. Whether or not they are right (cf. Koriat & Goldsmith, 1996b), it should be clear that the building blocks of connectionist explanations of cognitive performance are not "units" or "weights," but higher order descriptions of the functional properties and dynamics (interactions) of networks, during learning, during processing, or both. 11. There are various general techniques for probing the inner workings of connectionist networks (see, for example, Berkeley et al., 1995; Elman, 1990a, 1990b; Hanson & Burr, 1990; Hinton, 1989; Hinton, McClelland and Rumelhart, 1986; Hinton & Shallice, 1991; Meddler & Dawson, 1998; Sejnowski & Rosenberg, 1988; Rumelhart & Todd, 1993), as well as many clever methods that have been tailored for specific models in particular studies. Some of the techniques are useful in attempting to map approximate, "macro-lev