Proof. For discounted rewards, a straightforward adaptation of the proof of Theorem 3.1 in (Altman, 1999) shows that the discounted occupation distribution of any policy can be obtained with that of a stationary stochastic Markov policy. As the evaluation of policies with GGF is completely determined by their discounted occupation distributions, the GGF of any policy can be obtained with that of a stationary stochastic Markov policy.