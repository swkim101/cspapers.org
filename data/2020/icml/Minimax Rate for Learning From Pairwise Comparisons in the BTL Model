We consider the problem of learning the qualities w 1 , . . . , w n of a collection of items by performing noisy comparisons among them. A standard assumption is that there is a ﬁxed “compar-ison graph” and every neighboring pair of items is compared k times. We will study the popular Bradley-Terry-Luce model, where the probability that item i wins a comparison against j equals w i / ( w i + w j ) . The goal is to understand how the expected error in estimating the vector w = ( w 1 , . . . , w n ) behaves in the regime when the number of comparisons k is large. Our contribution is the determination of the minimax rate up to a constant factor. We show that this rate is achieved by a simple algorithm based on weighted least squares, with weights determined from the empirical outcomes of the comparisons. This al-gorithm can be implemented in nearly linear time in the total number of comparisons.