Frank-Wolfe algorithm is an efﬁcient method for optimizing non-convex constrained problems. However, most of existing methods focus on the ﬁrst-order case. In real-world applications, the gradient is not always available. To address the problem of lacking gradient in many applications, we propose two new stochastic zeroth-order Frank-Wolfe algorithms and theoretically proved that they have a faster convergence rate than existing methods for non-convex problems. Speciﬁcally, the function queries oracle of the proposed faster zeroth-order Frank-Wolfe (FZFW) method is O ( n 1 / 2 d ✏ 2 ) which can match the iteration complexity of the ﬁrst-order counterpart approximately. As for the proposed faster zeroth-order conditional gradient sliding (FZCGS) method, its function queries oracle is improved to O ( n 1 / 2 d ✏ ) , indicating that its iteration complexity is even better than that of its ﬁrst-order counterpart NCGS-VR. In other words, the iteration complelxity of the accelerated ﬁrst-order Frank-Wolfe method NCGS-VR is suboptimal. Then, we proposed a new algorithm to improve its IFO (incremental ﬁrst-order oracle) to O ( n 1 / 2 ✏ ) . At last, the empirical studies on benchmark datasets validate our theoretical results.