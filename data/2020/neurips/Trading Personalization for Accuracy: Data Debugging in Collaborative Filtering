Collaborative ﬁltering has been widely used in recommender systems. Existing work has primarily focused on improving the prediction accuracy mainly via either building reﬁned models or incorporating additional side information, yet has largely ignored the inherent distribution of the input rating data. In this paper, we propose a data debugging framework to identify overly personalized ratings whose existence degrades the performance of a given collaborative ﬁltering model. The key idea of the proposed approach is to search for a small set of ratings whose editing (e.g., modiﬁcation or deletion) would near-optimally improve the recommendation accuracy of a validation set. Experimental results demonstrate that the proposed approach can signiﬁcantly improve the recommendation accuracy. Furthermore, we observe that the identiﬁed ratings signiﬁcantly deviate from the average ratings of the corresponding items, and the proposed approach tends to modify them towards the average. This result sheds light on the design of future recommender systems in terms of balancing between the overall accuracy and personalization.