Metric learning aims to learn a distance measure that can beneﬁt distance-based 1 methods such as the nearest neighbor (NN) classiﬁer. While considerable efforts 2 have been made to improve its empirical performance and analyze its generalization 3 ability by focusing on the data structure and model complexity, an unresolved ques-4 tion is how choices of algorithmic parameters such as training time affect metric 5 learning as it is typically formulated as an optimization problem and nowadays 6 more often as a non-convex problem. In this paper, we theoretically address this 7 question and prove the agnostic Probably Approximately Correct (PAC) learnabil-8 ity for metric learning algorithms with non-convex objective functions optimized 9 via gradient descent (GD); in particular, our theoretical guarantee takes training 10 time into account. We ﬁrst show that the generalization PAC bound is a sufﬁcient 11 condition for agnostic PAC learnability and this bound can be obtained by ensuring 12 the uniform convergence on a densely concentrated subset of the parameter space. 13 We then show that, for classiﬁers optimized via GD, their generalizability can 14 be guaranteed if the classiﬁer and loss function are both Lipschitz smooth, and 15 further improved by using fewer iterations. To illustrate and exploit the theoretical 16 ﬁndings, we ﬁnally propose a novel metric learning method called S mooth M etric 17 and representative I nstance LE arning (SMILE), designed to satisfy the Lipschitz 18 smoothness property and learned via GD with an early stopping mechanism for 19 better discriminability and less computational cost of NN. 20