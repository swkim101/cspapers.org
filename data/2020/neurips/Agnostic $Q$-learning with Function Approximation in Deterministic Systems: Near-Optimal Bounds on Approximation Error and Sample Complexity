The current paper studies the problem of agnostic Q -learning with function approx-1 imation in deterministic systems where the optimal Q -function is approximable 2 by a function in the class F with approximation error δ ≥ 0 . We propose a novel 3 recursion-based algorithm and show that if δ = O (cid:0) ρ/ √ dim E (cid:1) , then one can ﬁnd 4 the optimal policy using O (dim E ) trajectories, where ρ is the gap between the 5 optimal Q -value of the best actions and that of the second-best actions and dim E 6 is the Eluder dimension of F . Our result has two implications: 7 1. In conjunction with the lower bound in [Du et al., 2020], our upper bound 8 suggests that the condition δ = (cid:101) Θ (cid:0) ρ/ √ dim E (cid:1) is necessary and sufﬁcient for 9 algorithms with polynomial sample complexity. 10 2. In conjunction with the obvious lower bound in the tabular case, our upper 11 bound suggests that the sample complexity (cid:101) Θ (dim E ) is tight in the agnostic 12 setting. 13 Therefore, we help address the open problem on agnostic Q -learning proposed 14 in [Wen and Van Roy, 2013]. We further extend our algorithm to the stochastic 15 reward setting and obtain similar results. 16