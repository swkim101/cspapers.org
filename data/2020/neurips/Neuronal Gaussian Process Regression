N (y;m,K) or y ∼ N (m,K) y is normally distributed with mean m and covariance K, p(y) = e − 1 2 (y−m)>K−1(y−m) √ |2πK| I identity matrix 1 = (1, ..., 1)> vector of ones n number of training data points d dimensionality of inputs m number of inducing points / neurons in first layer f latent function k(x,x′) covariance function σ2 noise variance s2 signal variance {lc}c=1 length-scales / width of tuning curves along each dimension X = {xi}i=1 (d-dimensional) training inputs y = {yi}i=1 (real, scalar) training outputs f = {f(xi)}i=1 latent function values at input points x∗ test point Z = {zj}j=1 inducing point locations / tuning curve centers Kff covariance matrix at input locations X Kuu covariance matrix at inducing point locations Z Kfu = K > uf covariance matrix between input locations X and inducing point locations Z kf∗ covariance vector between input locations X and test point x∗ ku∗ covariance vector between inducing point locations Z and test point x∗ Qff = KfuK −1 uuKuf Nyström approximation of Kff μ∗,Σ∗ predictive mean and variance for test point x∗ / activity of the two output neurons φ(·) = {φj(·)}j=1 = {k(zj , ·)}j=1 tuning curves / activations of 1st layer neurons ψ(·) = {ψj(·)}j=1 activations of 2nd layer neurons w,U, wΣ synaptic weights bΣ bias η learning rate δ = μ− y prediction error χ = δ2 squared prediction error ρ(x∗) non-normalized variance of f∗ / activity of 3rd layer neuron in Fig. S2 ξ perturbation of inducing point location / tuning curve center B baseline, control variate to reduce variance of the gradient estimate