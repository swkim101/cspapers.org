The Internet-of-things (IoT) devices and technologies led to a revolutionary breakthrough over the data collection procedure and Machine Learning (ML) approaches of a distributed network. It is preferable to store sensitive data on-device without sharing with a centralized computation agent and carry-out computation at the edge devices to ensure security and privacy. A recently invented distributed ML technique, Federated Learning (FL) holds the same theme that allows the edge devices to perform training on their edges and obtains a final model by learning from the model information of all the distributed edge clients. As the clients' raw data remain at local and models are generated on clients' edge, so it enhances security, privacy, and reduces computation cost in large-scale ML problems. The FL technique deals with various distributed clients that may have statistical heterogeneity and systems heterogeneity. This paper aims at dealing with such heterogeneity within an FL environment by monitoring each client's activities and leveraging resources based on the required computation during the model training phase. For each training round, we consider the proficient and trustworthy client by inspecting their resource-availability and previous history. We assign a trust score to each client based on their performance and update that score after each training period. To bring systems heterogeneity within our FL environment, we consider distributed mobile robots as FL clients with heterogeneous system configurations in terms of memory, processor, bandwidth, or battery life to understand their performance and resource-constraint behavior in a real-world setting. We filter-out the weak clients who cannot perform computation based on their available resources and exclude the untrustworthy clients who has previous record of repeatedly infusing incorrect or diverge model information, or, become stragglers during FL training. After eliminating the stragglers and untrustworthy FL clients, we conduct local training on each selected FL clients. To further mitigate the straggler issue, we enable asynchronous FL technique that can handle the clients' variant response time and continue FL training without waiting for a particular client for a long period.