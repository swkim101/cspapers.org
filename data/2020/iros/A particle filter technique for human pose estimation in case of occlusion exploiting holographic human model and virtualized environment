In a collaborative scenario, robots working side by side with humans might rely on vision sensors to monitor the activity of the other agent. When occlusions of the human body occur, both the safety of the cooperation and the performance of the team can be penalized, since the robot could receive incorrect information about the ongoing cooperation. In this work, we propose a novel particle filter algorithm that, by merging the data acquired through a RGB-D camera and a MR headset, estimates online the human wrist position. This algorithm allows to significantly reduce the uncertainty of the human pose estimation, in case of both static and dynamic occlusions. To this purpose, the proposed particle filter is integrated with a detailed virtual model of the real workspace. Moreover, additional constraints describing the boundaries of the motion of the human upper body are included in a virtualized framework. The results showed that the proposed technique entails significant improvements, determining a relevant reduction of the estimation error and of the uncertainty of the estimate.