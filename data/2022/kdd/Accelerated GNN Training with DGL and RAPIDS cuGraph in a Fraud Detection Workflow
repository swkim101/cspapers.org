Graph Neural Networks (GNNs) have gained the interest of industry with Relational Graph Convolutional Networks (R-GCNs) showing promise for fraud detection. Taking existing workflows that leverage graph features to train a gradient boosted decision tree (GBDT) and replacing the graph features with GNN produced embedding achieves an increase in accuracy. However, recent work has shown that the combination of graph attributes with GNN embeddings provides the biggest lift in accuracy. Whether to use a GNN is half of the picture. Data loading, data cleaning and prep (ETL), and graph processing are critical first steps before graph features or GNN training can be performed. Moreover, the entire process is interactive, optimizing training and validation, for shorter model delivery cycles. Quicker model updates are the key to staying ahead of evolving fraud techniques. McDonald and Deotte [1] published a BLOG on the importance of being able to iterate quickly in finding a solution. The RAPIDS [2] suite of open-source software libraries gives the data scientist the freedom to execute end-to-end analytics work-flows on GPUs. The ETL and data loading portion is handled by RAPIDS cuDF, which utilizes a familiar DataFrame API. The GBDT process is handled by RAPIDS cuML that has an implementation of XGBoost and RandomForest. The graph analytic portion is handled by RAPIDS cuGraph. Recently cuGraph announced integration into Deep Graph Library (DGL) [3]. For GNN training, graph sampling can consume up to 80% of the training time. RAPIDS cuGraph sampling algorithms execute 10x to 100x faster than similar CPU versions and scale to support massive size graphs. Join us as we dive into GNNs for fraud detection and as we demonstrate how RAPIDS + DGL drastically reduces training time. We will cover everything from accelerating data load and data prep to accelerated GNN training with cuGraph + DGL.