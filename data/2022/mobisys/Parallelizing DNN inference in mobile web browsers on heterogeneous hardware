Mobile Web apps are emerging to leverage DNN models to provide intelligent user experience. But the limited functionalities for heterogeneous hardware provided by mobile Web browsers challenge the Web apps to perform DNN inference efficiently. In this paper, we propose a novel DNN inference engine, named PipeEngine, to parallelize the DNN inference process on CPU and GPU in mobile Web browsers. The design of PipeEngine enables pipeline parallelism between two adjacent DNN inference tasks with heterogeneous hardware. Evaluation results show that PipeEngine can increase the inference throughput by up to 2.77Ã—.