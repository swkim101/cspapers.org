Entity alignment (EA), which aims to discover equivalent entities in knowledge graphs (KGs), bridges heterogeneous sources of information and facilitates the integration of knowledge. Recently, based on translational models, EA has achieved impressive performance in utilizing graph structures or by adopting auxiliary information. However, existing entity alignment methods mainly rely on manually labeled entity alignment seeds, limiting their applicability in real scenarios. In this paper, a simple but effective Uncertainty-aware Pseudo Label Refinery (UPLR) framework is proposed without manually labeling requirement and is capable of learning high-quality entity embeddings from pseudo-labeled data sets containing noisy data. Our proposed model relies on two key factors: First, a non-sampling calibration strategy is provided that does not require artificially designed thresholds to reduce the influence of noise labels. Second, the entity alignment model achieves goal-oriented uncertainty correction through a gradual enhancement strategy. Experimental results on benchmark datasets demonstrate that our proposed model outperforms the existing supervised methods in cross-lingual knowledge graph tasks. Our source code is available at: https://github.com/Jia-Li2/UPLR/.