To avoid the computational cost of Hessian matrix, the first-order DARTS [11] and the first-order approximation of MAML [3] are employed for searching meta-learners. As for the inner-learners of φ and θ, we use the vanilla SGD with inner learning rate αinner = 1× e−2 for optimizing φ, while a inner learning rate βinner = 0.1 for training θ. In the meta-learner of φ, an Adam [6] optimizer is employed for updating, with an initial learning rate αmeta = 1× e−3 and a weight decay of 3 × 10−4. A similar Adam without weight decay is applied to training the meta-learner of θ. We choose M = 5 as the inner update step. The searching is executed on both Omniglot and Mini-Imagenet under the setting of 5-way, 5-shot. For each dataset, we sample 1200 tasks from Dmeta−train for training and 600 tasks from Dmeta−test for evaluation. On Omniglot, we prune the architecture every three epochs from the fifth epoch, while we do it every five epoch from ninth epoch in Mini-Imagenet. All search and adaptation experiments are carried out on NVIDIA RTX 2080TI GPUs. The whole search process requires about 0.6 GPU days on Mini-Imagenet. The searched architectures is visualized in Fig.1 and Fig.2.