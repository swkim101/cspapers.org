Graph meta-learning which is used to deal with graph few-shot learning attracts more and more research interests. Existing graph meta-learning methods mainly focus on capturing node-level relations, but they ignore task-level relations which are beneficial for improving the performance of few-shot node classification. Furthermore, contrastive learning which can learn knowledge without labeled data is suitable for few-shot scenario, but existing graph few-shot learning methods have never exploited it. To tackle above problems, in this paper, we combine conventional graph meta-learning framework with graph contrastive learning and propose a novel joint model named -${\underline T}$asklevel -${\underline R}$elations Modelling for -${\underline G}$raph ${\underline M}$eta-learning (TRGM). By constructing auxiliary contrastive pretext tasks, TRGM can fully capture the inter-task relations (task correlation and task discrepancy) and promote the primary few-shot learning. Finally, we conduct extensive experiments on six benchmark datasets to validate the effectiveness and efficiency of TRGM. Experimental results show that our model outperforms several strong baselines and achieves the new state-of-the-art.