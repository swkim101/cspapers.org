Pretraining multimodal models on Electronic 001 Health Records (EHRs) provides a means to 002 learn rich representations that might transfer 003 to downstream tasks with minimal supervision. 004 Recent multimodal models induce soft local 005 alignments between modalities (image regions 006 and sentences). This is of particular interest in 007 the medical domain, where alignments could 008 serve to highlight regions in an image relevant 009 to specific phenomena described in free-text. 010 Past work has presented example “heatmaps” 011 as qualitative evidence that cross-modal soft 012 alignments can be interpreted in this manner. 013 However, there has been little quantitative eval014 uation of such alignments. Here we compare 015 alignments from a state-of-the-art multimodal 016 (image and text) model for EHR with human 017 annotations that associate image regions with 018 sentences. Our main finding is that the text has 019 surprisingly little influence on the attention; 020 alignments do not consistently reflect basic 021 anatomical information. Moreover, synthetic 022 modifications, such as substituting “left” for 023 “right,” do not substantially influence attention. 024 We find that simple techniques such as masking 025 out entity names during training show promise 026 in terms of their ability to improve alignments 027 without additional supervision. 028