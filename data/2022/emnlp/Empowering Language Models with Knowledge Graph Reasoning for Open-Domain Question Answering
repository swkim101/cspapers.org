Answering open-domain questions requires world knowledge about in-context entities. As pre-trained Language Models ( LM s) lack the power to store all required knowledge, exter-nal knowledge sources, such as knowledge graphs, are often used to augment LM s. In this work, we propose knOwledge REasOning em-powered Language Model (O REO LM), which consists of a novel Knowledge Interaction Layer that can be flexibly plugged into existing Transformer-based LM s to interact with a differentiable Knowledge Graph Reasoning module collaboratively. In this way, LM guides KG to walk towards the desired answer, while the retrieved knowledge improves LM . By adopting O REO LM to RoBERTa and T5, we show significant performance gain, achieving state-of-art results in the Closed-Book setting. The performance enhancement is mainly from the KG reasoning’s capacity to infer missing relational facts. In addition, O REO LM provides reasoning paths as rationales to interpret the model’s decision.