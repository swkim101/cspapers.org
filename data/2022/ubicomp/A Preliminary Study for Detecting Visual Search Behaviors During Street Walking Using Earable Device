Map applications on smartphones are powerful navigation tools for walking among places to visit for the first time and are widely used. On the other hand, checking the map applications tend to cause accidents on the road such as collisions with people, cars, and objects. To prevent this, we need to detect a walker’s context regarding visual search behaviors and provide appropriate navigational information to the walker. In this paper, we propose a method to detect a walker’s context regarding visual search behaviors by using motion sensors on an earable device. We collected and investigated motion and gaze data from an earable device and a gaze tracker respectively during street walking from five participants. Based on the investigation, we created a machine learning model for detecting looking around, smartphone, or normal during walking and stopping conditions. Our evaluations showed that our models can detect more than 95% walking and stopping conditions, and 71% of three detail conditions during walking, respectively.