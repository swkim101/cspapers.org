Continual learning has been widely studied in recent years to resolve the catastrophic forgetting of deep neural networks. In this paper, we ﬁrst enforce a low-rank ﬁlter subspace by decomposing convolutional ﬁlters within each network layer over a small set of ﬁlter atoms. Then, we perform continual learning with ﬁlter atom swapping. In other words, we learn for each task a new ﬁlter subspace for each convolutional layer, i.e., hundreds of parameters as ﬁlter atoms, but keep subspace coefﬁcients shared across tasks. By maintaining a small footprint memory of ﬁlter atoms, we can easily archive models for past tasks to avoid forgetting. The effectiveness of this simple scheme for continual learning is illustrated both empirically and theoretically. The proposed atom swapping framework further enables ﬂexible and efﬁcient model ensemble with members selected within task or across tasks to improve the performance in different continual learning settings. Being validated on multiple benchmark datasets with different convolutional network structures, the proposed method outperforms the state-of-the-art methods in both accuracy and scalability.