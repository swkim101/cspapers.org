In Internet of Things (IoT) scenarios such as smart homes, autonomous vehicles, and wearable devices, data pattern changes over time due to changing environments and user requirements, known as domain shifts. When encountering domain shifts, deep neural network models in IoT suffers from performance degradation and need to retrain from scratch to adapt to domain shifts incrementally. Therefore, incremental learning is needed to adapt a model to domain shifts without retraining. Existing methods using the parameter isolation technique perform well in incremental learning of new domains without performance degradation. However, they cannot be directly adopted in IoT applications as they store masks and require users to label the task to indicate task-specific parameters during inference, which is memory inefficient and cumbersome. In this paper, we propose a memory-efficient method for IoT to incrementally adapt to domain shifts in a fixed neural network, named E-DomainIL. Our method freezes learned parameters and allows reusing them later in training to avoid interference between different domains. E-DomainIL does not require task labels or storing masks as it uses all parameters during inference. We use data-driven pruning to adjust the parameter ratio according to the dataset, thus maintaining the balance between accuracy and parameter efficiency. Experimental results on image classification benchmarks demonstrate our method's efficiency and accuracy.