Quantum-assisted sampling is a promising technique to enable training probabilistic ML models, which otherwise depend on slow-mixing classical sampling methods; such as, the use of Quantum Annealing Processors (QAP) to train Boltzmann Machines (BMs). Previous work has shown that QAPs can sample from a Boltzmann distribution, although, at an unknown instance-dependent temperature. Due to this distribution divergence, existing training algorithms have resorted to negative-phase temperature scaling. This method, although effective under arduous tuning, introduces unwanted noise to the sampleset due to the quantization errors caused by the underutilization of the QAP bias ranges; and is prone to bias overflow. We introduce a change in the training algorithm to allow positive-phase temperature scaling; an approach that reduces the impact of quantization noise, while still incorporating temperature scaling. As a result, we see an overall improvement in the convergence rate and testing accuracy, when compared to the state-of-the-art approach.