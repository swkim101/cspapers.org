Semi-supervised learning (SSL) has proven to be successful in overcoming labeling difﬁculties by leveraging unlabeled data. Previous SSL algorithms typically assume a balanced class distribution. However, real-world datasets are usually class-imbalanced, causing the performance of existing SSL algorithms to be seriously decreased. One essential reason is that pseudo-labels for unlabeled data are selected based on a ﬁxed conﬁ-dence threshold, resulting in low performance on minority classes. In this paper, we develop a simple yet effective framework, which only involves adaptive thresholding for different classes in SSL algorithms, and achieves remarkable performance improvement on more than twenty imbalance ra-tios. Speciﬁcally, we explicitly optimize the number of pseudo-labels for each class in the SSL objective, so as to simultaneously obtain adaptive thresholds and minimize empirical risk. More-over, the determination of the adaptive threshold can be efﬁciently obtained by a closed-form solu-tion. Extensive experimental results demonstrate the effectiveness of our proposed algorithms.