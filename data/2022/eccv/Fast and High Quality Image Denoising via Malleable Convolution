The proposed MalleNet-families include MalleNet-S, -M, -L, and -Xl. We start from constructing MalleNet-S. MalleNet-S consists of four levels, the top two levels use 16-channel intermediate features and bottom two levels use 32-channel intermediate features. We adopt Inverse Bottleneck Block [3] as the basic operator for each level, whose detailed structure is shown in Fig. 2 right and we use a fixed expansion ratio of 3 in the depth-wise convolution. We use two blocks for each of the top two levels and six blocks for each of the bottom two levels, the and inject one 3Ã— 3 MalleConv layer in the middle of each level. To construct MalleNet-M, -L, -XL, we simply modify MalleNet-S by growing the number of channels of each stage from {16, 16, 32, 32} to {32, 32, 64, 64}, {64, 64, 128, 128}, {144, 144, 288, 288} respectively. For MalleNet-XL, we further increase the expansion ratio from 3 to 5 to increase its capability. For real-world noise benchmark SIDD dataset, we construct MalleNet-R by adjusting the base channel number to 64 and adopt ResNet Block as basic operator (as shown in Fig. 2 left). To better illustrate how to adopt MalleConv on existing popular backbones, we describe the detailed injecting position of MalleConv in the ablation study on Sec. 5.2 of our main manuscripts. Concretely speaking, we inject MalleConv in the middle layer of DnCNN [4] and UNet, and the last layer of Residual Dense Block (RDN). Furthermore, we present the details of the proposed efficient predictor network, as shown in Fig. 3. The predictor network is constructed with several stacked ResNet Blocks and MaxPooling layer. We apply the standard ResNet blocks for all variants of MalleNet-families. To mitigate training instability, we will optimize the static convolutional kernel only for the first 10 iterations on each backbone, and then include the Malleable Convolution (dynamic kernel) in the forward and backward process. This is observed to avoid NaN loss in most cases.