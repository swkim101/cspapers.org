In this section, we show the effect that loss designs have on ensembles in terms of bringing knowledge closer and separating knowledge. We trained four networks and compared the ensemble accuracy using them. We used ResNet-18 [2] as the network, the probability distribution and attention map as the knowledge, and Stanford Dogs [3] as the dataset. The attention map was created from the output of ResBlock4 using attention transfer [8]. The loss design for the probability distribution used KL-divergence (KL) and cosine similarity (cos). The loss design for the attention map used mean square error (MSE) and cosine similarity (cos). Tables 1 and 2 show the results of the loss design for the probability distribution and the attention map. “+” means the loss design for bringing knowledge closer and “−” means the loss design for separating knowledge. With the exception of several loss designs, the ensemble accuracy did not change significantly with the loss design. Loss designs that use division have the possibility of dividing by zero. Therefore, to train the network as a minimization problem, we selected different loss designs for bringing knowledge closer and separating knowledge.